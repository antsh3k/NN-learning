{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TODO: Sentiment classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antsh3k/NN-learning/blob/master/Sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdjVtLYL6sXJ",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_QSfQu7arNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SWITCH TO GPU\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import torch \n",
        "\n",
        "SEED = 15\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4lXNDq6njd",
        "colab_type": "text"
      },
      "source": [
        "# Get the data from github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0g6YMvNcFUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/w-is-h/tmp/master/dataset.csv\", encoding='cp1252')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YALrViGcIQ7",
        "colab_type": "code",
        "outputId": "0574a9af-dd6c-49e0-c2b1-db7df8fad4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>first think another Disney movie, might good, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>big fan Stephen King's work, film made even gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment\n",
              "0  first think another Disney movie, might good, ...          1\n",
              "1  Put aside Dr. House repeat missed, Desperate H...          0\n",
              "2  big fan Stephen King's work, film made even gr...          1\n",
              "3  watched horrid thing TV. Needless say one movi...          0\n",
              "4  truly enjoyed film. acting terrific plot. Jeff...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO8wGAHN6uMq",
        "colab_type": "text"
      },
      "source": [
        "# Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqoppNTg6xzu",
        "colab_type": "code",
        "outputId": "95b450b2-81d5-40f1-8111-d8393844df50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# TODO: Print the shape, columns and the number of positive values in our dataset\n",
        "shape = df.shape\n",
        "cols = df.columns\n",
        "num_pos = np.sum(df[\"Sentiment\"] > 0)\n",
        "print(\"The shape of the dataset is: \" + str(shape))\n",
        "print(\"The columns are: \" + str(cols))\n",
        "print(\"Number of positive values: \" + str(num_pos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is: (25000, 2)\n",
            "The columns are: Index(['SentimentText', 'Sentiment'], dtype='object')\n",
            "Number of positive values: 12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YadlC2_cMzc",
        "colab_type": "code",
        "outputId": "d72d3ccd-f40f-48b2-b930-27cccd4c90f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# TODO: using 'plt' plot a histogram of the column Sentiment\n",
        "plt.hist(df[\"Sentiment\"])\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVcTC417YI8",
        "colab_type": "text"
      },
      "source": [
        "###Get the $x,y$ values from the datset\n",
        "\n",
        "#### (x is the. input and y is the output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa66fNKWd2lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df['SentimentText'].values\n",
        "y = df['Sentiment'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErAVeYuduCx",
        "colab_type": "text"
      },
      "source": [
        "###Look at x and check do we need to clean anything\n",
        "\n",
        "Cleaning is not always necessary, but the more garbage we leave the more the Neural Network has to learn. So, if the cleaning process is easy - better to do it. Or get a gigantic dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO4wpcINdWMz",
        "colab_type": "code",
        "outputId": "698c8b3f-ffcc-4cbb-b3b4-4709e465f9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(x[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. <br /><br />The characters composed one lame stereo-type another, obvious attempt creating another \"Bad News Bears\" embarrassing say least.<br /><br />I seen prized turkeys time, reason list since \"Numero Uno\".<br /><br />Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I9rNmcu6fpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove mails and https links\n",
        "pat_1 = r\"(?:\\@|https?\\://)\\S+\"\n",
        "# Remove tags\n",
        "pat_2 = r'#\\w+ ?'\n",
        "# Combine into one regex\n",
        "combined_pat = r'|'.join((pat_1, pat_2))\n",
        "# Remove websites\n",
        "www_pat = r'www.[^ ]+'\n",
        "# Remove HTML tags\n",
        "html_tag = r'<[^>]+>'\n",
        "def data_cleaner(text):\n",
        "  cleantags = \"\"\n",
        "  try:\n",
        "    stripped = re.sub(combined_pat, '', text)\n",
        "    stripped = re.sub(www_pat, '', stripped)\n",
        "    cleantags = re.sub(html_tag, '', stripped)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    cleantags = \"None\"\n",
        "  return cleantags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98OEBCta7Os0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Run the cleaning function for each sentence in 'x' and save the results into a\n",
        "#new variable x2\n",
        "x2 = []\n",
        "for doc in x:\n",
        "  cleaned_doc = data_cleaner(doc)\n",
        "  x2.append(cleaned_doc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoQ0ndefe_IF",
        "colab_type": "code",
        "outputId": "0ff92485-a876-4a68-89b6-306406c052f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x_original = x\n",
        "x = x2\n",
        "print(x[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. The characters composed one lame stereo-type another, obvious attempt creating another \"Bad News Bears\" embarrassing say least.I seen prized turkeys time, reason list since \"Numero Uno\".Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z75yCgk-7wLa",
        "colab_type": "text"
      },
      "source": [
        "# SpaCy\n",
        "\n",
        "We can use spacy to tokenize the text and further clean it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmE8oH7KbonG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.attrs import LOWER\n",
        "# Load the english model for spacy, the disable part is used to make it faster\n",
        "nlp = spacy.load('en', disable=['ner', 'parser']) #ner = named entity recognition. disable these two for speed reasons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Jz7Ql5jCKr",
        "colab_type": "text"
      },
      "source": [
        "###The nlp variable is an instance of a class that has the method \\_\\_call__ defined, which means it is a class that is callable\n",
        "\n",
        "It returns an object that is a spacy representation of the input sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccllc0l7zHG",
        "colab_type": "code",
        "outputId": "72e8a24c-edef-43f2-e543-1b697b3ad09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Type of the nlp variable is: \" + str(type(nlp)))\n",
        "doc = nlp(\"I was running yesterday.\")\n",
        "print(\"\\nThe 'nlp' function returns: \" + str(type(doc)))\n",
        "print(\"\\nIf we print the 'doc' variable we get the original input sentence: \" + str(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the nlp variable is: <class 'spacy.lang.en.English'>\n",
            "\n",
            "The 'nlp' function returns: <class 'spacy.tokens.doc.Doc'>\n",
            "\n",
            "If we print the 'doc' variable we get the original input sentence: I was running yesterday.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2gPwleAkTXb",
        "colab_type": "text"
      },
      "source": [
        "###To access tokens we can loop over the document - this means the 'doc' class has an \\_\\_iter__ method defined\n",
        "\n",
        "Look [here](https://github.com/explosion/spaCy/blob/9003fd25e5e966bd8d1b67a18f3ebd6010d6f718/spacy/tokens/doc.pyx) for the class definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2shYPjFX75Gq",
        "colab_type": "code",
        "outputId": "f40c95e7-ae9e-4544-f82b-ccf655d6df7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "was\n",
            "running\n",
            "yesterday\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-m0CAgoddy",
        "colab_type": "text"
      },
      "source": [
        "###Note that each token is a spacy object - not a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xicuAf3qoj_K",
        "colab_type": "code",
        "outputId": "f06154ef-73fe-44a9-82c4-5d7e021061b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(doc[0])) # we can also index the doc object and get tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ja7vJ3omkQ",
        "colab_type": "text"
      },
      "source": [
        "###To get strings we use the .text property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9y7IEeoq6W",
        "colab_type": "code",
        "outputId": "e71e7573-ba83-4dde-c55d-7c99fc534c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(doc[0].text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzY4iHv18CVt",
        "colab_type": "code",
        "outputId": "22c3abb0-602b-4f57-9d45-a43d7f724350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Each token has multiple properties, e.g. lemma_ - note the '_', that means print text\n",
        "for token in doc:\n",
        "  print(token.lemma_)\n",
        "  print(token.is_stop) # stop words are very frequent words\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PRON-\n",
            "True\n",
            "\n",
            "be\n",
            "True\n",
            "\n",
            "run\n",
            "False\n",
            "\n",
            "yesterday\n",
            "False\n",
            "\n",
            ".\n",
            "False\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXbv-Df64G50",
        "colab_type": "code",
        "outputId": "726f3652-35a5-4060-dee3-3f2753167a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "snt = \"That was a very good movie, John\"\n",
        "# TODO: Tokenize the sentence above and save the lemmatized strings into the \n",
        "#tmp variable\n",
        "\n",
        "tmp = []\n",
        "doc = nlp(snt)\n",
        "for token in doc:\n",
        "  tmp.append(token.lemma_)#if you dont put un underscore youll get a hash (numerical representation of the. word)\n",
        "print(tmp)\n",
        "# The output should be ['that', 'be', 'a', 'very', 'good', 'movie', ',', 'John']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['that', 'be', 'a', 'very', 'good', 'movie', ',', 'John']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUpxGw6vo2su",
        "colab_type": "code",
        "outputId": "c3e4eae2-c496-43f7-e718-28ecc851af33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: Tokenize the sentence again and save the lemmatized strings into the \n",
        "#tmp variable, but skip stopwords and punctuation plus lowercase the lemmas, \n",
        "#print the tmp variable\n",
        "tmp = []\n",
        "for token in doc:\n",
        "  if not token.is_punct and not token.is_stop:\n",
        "    tmp.append(token.lemma_.lower())\n",
        "print(tmp)\n",
        "# The output should be ['good', 'movie', 'john']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good', 'movie', 'john']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ykAaAne8KVb",
        "colab_type": "text"
      },
      "source": [
        "# Split sentences into tokens and lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81ry5hpe8XGp",
        "colab_type": "code",
        "outputId": "94f258af-d55a-474e-f042-0f21e2085dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYR6ueKeWz6",
        "colab_type": "code",
        "outputId": "2753d6f7-d597-486b-ccb8-e9ae7edb41cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# TODO: Lemmatize and split each sentence in our dataset 'x', save the new\n",
        "#lemmatized strings into the tok_snts variable. Skip punctuation and stopwords plus lowercase lemmas\n",
        "tok_snts = []\n",
        "for snt in x:\n",
        "  tkns = [token.lemma_.lower() for token in nlp(snt) if not token.is_punct and not token.is_stop]\n",
        "  tok_snts.append(tkns)\n",
        "  \n",
        "# Save back\n",
        "x = tok_snts\n",
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['think', 'disney', 'movie', 'good', 'kid', 'movie', 'watch', 'help', 'enjoy', 'age', 'love', 'movie', 'see', 'movie', '10', '8', 'year', 'later', 'love', 'danny', 'glover', 'superb', 'play', 'better', 'christopher', 'lloyd', 'hilarious', 'perfect', 'tony', 'danza', 'believable', 'mel', 'clark', 'help', 'enjoy', 'movie', '10/10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQc1FRv48n9J",
        "colab_type": "text"
      },
      "source": [
        "# Train word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVSxGpV4cOx8",
        "colab_type": "code",
        "outputId": "2c748c42-3368-472d-961c-f29c4f76adea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Basic configuration for logging - needed for gensim to print some output\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(x, size=300, window=6, min_count=4, workers=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-25 20:18:55,958 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2019-06-25 20:18:55,967 : INFO : collecting all words and their counts\n",
            "2019-06-25 20:18:55,967 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-06-25 20:18:56,183 : INFO : PROGRESS: at sentence #10000, processed 1068953 words, keeping 52119 word types\n",
            "2019-06-25 20:18:56,398 : INFO : PROGRESS: at sentence #20000, processed 2117054 words, keeping 74728 word types\n",
            "2019-06-25 20:18:56,505 : INFO : collected 83742 word types from a corpus of 2633298 raw words and 25000 sentences\n",
            "2019-06-25 20:18:56,506 : INFO : Loading a fresh vocabulary\n",
            "2019-06-25 20:18:56,587 : INFO : effective_min_count=4 retains 27093 unique words (32% of original 83742, drops 56649)\n",
            "2019-06-25 20:18:56,588 : INFO : effective_min_count=4 leaves 2558477 word corpus (97% of original 2633298, drops 74821)\n",
            "2019-06-25 20:18:56,675 : INFO : deleting the raw counts dictionary of 83742 items\n",
            "2019-06-25 20:18:56,680 : INFO : sample=0.001 downsamples 31 most-common words\n",
            "2019-06-25 20:18:56,681 : INFO : downsampling leaves estimated 2406272 word corpus (94.1% of prior 2558477)\n",
            "2019-06-25 20:18:56,755 : INFO : estimated required memory for 27093 words and 300 dimensions: 78569700 bytes\n",
            "2019-06-25 20:18:56,755 : INFO : resetting layer weights\n",
            "2019-06-25 20:18:57,072 : INFO : training model with 4 workers on 27093 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
            "2019-06-25 20:18:58,082 : INFO : EPOCH 1 - PROGRESS: at 16.97% examples, 414932 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:18:59,118 : INFO : EPOCH 1 - PROGRESS: at 36.46% examples, 435319 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-25 20:19:00,133 : INFO : EPOCH 1 - PROGRESS: at 55.52% examples, 441509 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:01,134 : INFO : EPOCH 1 - PROGRESS: at 73.76% examples, 439933 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:02,192 : INFO : EPOCH 1 - PROGRESS: at 93.54% examples, 441054 words/s, in_qsize 6, out_qsize 1\n",
            "2019-06-25 20:19:02,448 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-25 20:19:02,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-25 20:19:02,497 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-25 20:19:02,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-25 20:19:02,512 : INFO : EPOCH - 1 : training on 2633298 raw words (2406189 effective words) took 5.4s, 442889 effective words/s\n",
            "2019-06-25 20:19:03,568 : INFO : EPOCH 2 - PROGRESS: at 17.69% examples, 417300 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:04,620 : INFO : EPOCH 2 - PROGRESS: at 36.11% examples, 419876 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:05,629 : INFO : EPOCH 2 - PROGRESS: at 51.66% examples, 405664 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:06,667 : INFO : EPOCH 2 - PROGRESS: at 70.85% examples, 413307 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:07,673 : INFO : EPOCH 2 - PROGRESS: at 89.30% examples, 418846 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:08,257 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-25 20:19:08,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-25 20:19:08,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-25 20:19:08,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-25 20:19:08,291 : INFO : EPOCH - 2 : training on 2633298 raw words (2405768 effective words) took 5.8s, 417440 effective words/s\n",
            "2019-06-25 20:19:09,342 : INFO : EPOCH 3 - PROGRESS: at 18.04% examples, 424517 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:10,356 : INFO : EPOCH 3 - PROGRESS: at 37.16% examples, 440161 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:11,364 : INFO : EPOCH 3 - PROGRESS: at 55.18% examples, 436803 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:12,385 : INFO : EPOCH 3 - PROGRESS: at 74.50% examples, 440901 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:13,403 : INFO : EPOCH 3 - PROGRESS: at 93.54% examples, 441706 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:13,672 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-25 20:19:13,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-25 20:19:13,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-25 20:19:13,709 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-25 20:19:13,710 : INFO : EPOCH - 3 : training on 2633298 raw words (2406190 effective words) took 5.4s, 444577 effective words/s\n",
            "2019-06-25 20:19:14,761 : INFO : EPOCH 4 - PROGRESS: at 17.69% examples, 415572 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:15,770 : INFO : EPOCH 4 - PROGRESS: at 36.11% examples, 427831 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:16,785 : INFO : EPOCH 4 - PROGRESS: at 53.54% examples, 424719 words/s, in_qsize 6, out_qsize 1\n",
            "2019-06-25 20:19:17,788 : INFO : EPOCH 4 - PROGRESS: at 73.06% examples, 433644 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:18,794 : INFO : EPOCH 4 - PROGRESS: at 91.52% examples, 435152 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:19,167 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-25 20:19:19,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-25 20:19:19,198 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-25 20:19:19,208 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-25 20:19:19,209 : INFO : EPOCH - 4 : training on 2633298 raw words (2406244 effective words) took 5.5s, 438061 effective words/s\n",
            "2019-06-25 20:19:20,241 : INFO : EPOCH 5 - PROGRESS: at 18.04% examples, 433563 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:21,249 : INFO : EPOCH 5 - PROGRESS: at 36.80% examples, 441825 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:22,260 : INFO : EPOCH 5 - PROGRESS: at 55.52% examples, 443424 words/s, in_qsize 8, out_qsize 0\n",
            "2019-06-25 20:19:23,271 : INFO : EPOCH 5 - PROGRESS: at 74.50% examples, 444800 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:24,273 : INFO : EPOCH 5 - PROGRESS: at 94.33% examples, 449756 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-25 20:19:24,552 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-25 20:19:24,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-25 20:19:24,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-25 20:19:24,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-25 20:19:24,576 : INFO : EPOCH - 5 : training on 2633298 raw words (2406347 effective words) took 5.4s, 449214 effective words/s\n",
            "2019-06-25 20:19:24,577 : INFO : training on a 13166490 raw words (12030738 effective words) took 27.5s, 437426 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpMdj6WIUvS",
        "colab_type": "text"
      },
      "source": [
        "#Similarity\n",
        "\n",
        "We can now easily calculate word similarity using the `w2v.wv.most_similar()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KyOvtimcsdh",
        "colab_type": "code",
        "outputId": "63ca2105-0fe9-467f-b0f3-091631f997e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "w2v.wv.most_similar(\"bad\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-25 20:19:30,426 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('worst', 0.8141058087348938),\n",
              " ('terrible', 0.786808431148529),\n",
              " ('horrible', 0.7836024165153503),\n",
              " ('awful', 0.7810854911804199),\n",
              " ('suck', 0.7549688816070557),\n",
              " ('worse', 0.7426633834838867),\n",
              " ('lousy', 0.7179365158081055),\n",
              " ('possibly', 0.6970950365066528),\n",
              " ('lame', 0.6915003061294556),\n",
              " ('crappy', 0.6906173825263977)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzOsY_HAfGuv",
        "colab_type": "code",
        "outputId": "d5271afb-7e3b-4a81-dfc6-814b198ea420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# TODO: For anyword you want print the most similar words\n",
        "w2v.wv.most_similar(\"man\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.7491421699523926),\n",
              " ('lady', 0.6092380285263062),\n",
              " ('innocent', 0.4909660518169403),\n",
              " ('sheep', 0.4724947214126587),\n",
              " ('male', 0.46628299355506897),\n",
              " ('gland', 0.46614259481430054),\n",
              " ('melting', 0.46410226821899414),\n",
              " ('path', 0.463747501373291),\n",
              " ('businesswoman', 0.4604407548904419),\n",
              " ('hyland', 0.45868462324142456)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMdm2yTIHmM",
        "colab_type": "text"
      },
      "source": [
        "#Get vectors\n",
        "\n",
        "To get a vector for a word 'w' we can use the `w2v.wv.get_vector(w)` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU_X_Wb48tGW",
        "colab_type": "code",
        "outputId": "f2026ea9-bf73-40ed-dce2-1ea516918de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#To check is a word in our vocab we use:\n",
        "if 'bad' in w2v.wv:\n",
        "  print(w2v.wv.get_vector(\"bad\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-8.86090100e-01  1.42315984e-01 -5.49913347e-02 -4.20135736e-01\n",
            "  9.57655072e-01  3.03101391e-01 -1.64010286e-01  4.90033686e-01\n",
            "  1.99888870e-01 -8.09188783e-01  1.52442858e-01  1.12511349e+00\n",
            "  1.22338617e+00 -4.01357830e-01  3.56517851e-01  8.21341693e-01\n",
            " -3.43892664e-01 -1.33052385e+00  6.38899863e-01  6.14235327e-02\n",
            "  1.19194233e+00  1.89173505e-01  1.31052792e+00  3.74916308e-02\n",
            " -3.15570623e-01 -2.46257499e-01  3.39526206e-01 -6.81301892e-01\n",
            " -1.91243649e-01  6.90278351e-01  6.73216343e-01 -2.86559314e-02\n",
            " -2.97286153e-01 -7.16380894e-01 -9.77633297e-01 -4.55711067e-01\n",
            " -6.04641616e-01 -5.60926497e-01 -5.19115804e-03  7.70555973e-01\n",
            " -5.88088572e-01  1.92406625e-01  4.34591323e-02 -1.14593089e+00\n",
            "  7.26597369e-01  2.59172022e-01  2.12699473e-01  2.47087747e-01\n",
            " -6.45115197e-01  1.54743388e-01 -9.16770697e-01  5.92647910e-01\n",
            "  1.19437182e+00  7.78690875e-01 -6.73841417e-01  8.43201578e-01\n",
            " -2.36079141e-01  8.02887827e-02 -3.18201329e-03  4.23600107e-01\n",
            " -6.48602903e-01 -5.58054805e-01  2.69964248e-01  7.77294874e-01\n",
            " -4.85163003e-01 -2.80233324e-01  3.61030787e-01  4.21462879e-02\n",
            "  1.16303883e-01  7.40170181e-01  2.11498722e-01 -2.43326336e-01\n",
            "  1.28496313e+00  1.31356847e+00 -6.57773167e-02  5.97455561e-01\n",
            " -6.34552538e-01 -8.86166096e-01  5.29472053e-01 -9.00340378e-01\n",
            " -8.63424420e-01 -5.65311909e-01 -8.74950409e-01 -1.90523952e-01\n",
            " -3.09701025e-01 -5.10533154e-01  3.29977334e-01  4.91863102e-01\n",
            " -2.20748827e-01 -8.42188418e-01 -9.88049358e-02 -1.10107034e-01\n",
            " -1.25294244e+00  1.22658968e+00 -5.76197684e-01 -8.07817936e-01\n",
            "  2.58179158e-01  9.25794899e-01 -4.88188654e-01 -3.71701807e-01\n",
            "  1.48392320e-01 -5.94345808e-01 -7.03122437e-01 -1.15886986e+00\n",
            " -8.67349625e-01 -9.87772644e-01 -5.56670390e-02 -6.26723886e-01\n",
            " -9.87299979e-01 -1.30645943e+00 -2.06965134e-01  8.10556889e-01\n",
            "  1.16000354e+00  2.61811092e-02  1.19719732e+00  9.83513176e-01\n",
            "  4.40220714e-01 -1.54766068e-02  4.73191440e-01  3.65592867e-01\n",
            "  5.68722606e-01  1.25080213e-01  2.02326274e+00  6.60521150e-01\n",
            " -3.12997252e-02  4.48848367e-01 -6.54781699e-01 -8.52478087e-01\n",
            " -7.12199152e-01 -5.53634465e-01  2.02814087e-01 -8.25994730e-01\n",
            " -3.79051864e-01 -1.22312653e+00 -9.61320460e-01 -3.45385790e-01\n",
            "  1.54148880e-02  2.20803395e-01  3.11844647e-01 -4.10078615e-01\n",
            "  3.70247751e-01  1.08678246e+00  2.15418175e-01  8.48462805e-02\n",
            " -8.11443090e-01 -5.78321576e-01 -6.57937288e-01  5.36813080e-01\n",
            "  1.11249959e+00 -9.24239993e-01 -4.70552981e-01 -8.29067886e-01\n",
            " -7.49838591e-01 -4.00813639e-01 -7.08382428e-01 -9.32523310e-01\n",
            "  4.41224962e-01 -8.99131894e-01 -1.22074466e-02 -5.32393754e-01\n",
            "  6.89019561e-01 -3.41348499e-01  1.84811115e-01 -8.75620008e-01\n",
            " -4.73667592e-01 -5.72678223e-02 -2.19806910e-01 -4.53803509e-01\n",
            "  8.29411391e-03  3.02053842e-04  4.10157025e-01 -5.56045771e-01\n",
            "  8.09676349e-01 -6.39568031e-01  2.87614256e-01 -1.06628336e-01\n",
            " -1.04363465e+00  2.36346558e-01  2.61016250e-01  1.00884247e+00\n",
            " -5.80658726e-02  1.10718167e+00 -3.76438320e-01  6.23853743e-01\n",
            " -8.76004875e-01  9.51262474e-01 -3.51060241e-01  8.90782475e-01\n",
            "  4.20653790e-01 -1.01710320e+00 -5.00947297e-01  1.54177773e+00\n",
            " -3.13219190e-01 -1.56617373e-01  1.01225162e+00  1.89921319e-01\n",
            " -7.27264881e-01  2.29543950e-02 -1.02431226e+00  7.82539666e-01\n",
            "  1.96129546e-01  5.22027075e-01 -5.98345339e-01 -1.01430714e-01\n",
            " -1.19886687e-02 -1.82761580e-01  1.12013412e+00  1.13220692e+00\n",
            "  3.33706200e-01  2.70664662e-01  1.34556675e+00 -2.20755368e-01\n",
            " -2.46994659e-01  7.25977242e-01 -9.98764634e-02  9.25447524e-01\n",
            " -1.93568483e-01 -8.28410506e-01  1.27975607e+00  6.87558115e-01\n",
            " -5.26221335e-01 -1.09503496e+00 -3.08654308e-01 -5.23480713e-01\n",
            " -4.28101122e-01  7.35242069e-01  8.40780020e-01  1.83567051e-02\n",
            " -1.56971991e-01  7.35507488e-01 -1.33538708e-01 -8.01317036e-01\n",
            "  9.59445059e-01 -1.07565725e+00  4.95356172e-01  3.04526269e-01\n",
            " -3.62335801e-01 -6.40801117e-02  9.47733298e-02  1.26844227e-01\n",
            "  6.09288514e-01  5.45017362e-01 -7.59008825e-01 -7.59787917e-01\n",
            " -9.36966062e-01 -8.61149967e-01 -6.86588883e-01  4.97826412e-02\n",
            "  3.49823684e-01  2.41757378e-01  1.05677024e-01  2.57418662e-01\n",
            " -6.45320833e-01 -4.33033891e-02  1.52608776e+00 -2.05089614e-01\n",
            " -1.93594009e-01  1.84038043e+00  4.05244291e-01  6.61771119e-01\n",
            "  4.39198434e-01 -3.00347388e-01  3.52786720e-01 -1.08232096e-01\n",
            "  3.89117096e-03  2.30544209e-01  3.64205129e-02  1.42215431e+00\n",
            "  7.54575729e-01 -6.27508879e-01 -1.08906090e+00 -1.00139701e+00\n",
            " -1.83792323e-01  7.40743637e-01 -2.54463583e-01 -5.61558664e-01\n",
            "  1.38578570e+00  7.72554576e-01 -3.59986037e-01 -3.20415258e-01\n",
            " -7.26700783e-01 -3.29929024e-01 -6.92571029e-02 -1.10223436e+00\n",
            " -1.17158687e+00  2.97792077e-01  5.06238878e-01 -8.29354405e-01\n",
            " -1.56549263e+00  5.96529126e-01 -5.95088780e-01 -7.86157787e-01\n",
            "  1.03892815e+00  5.64668477e-01 -5.52073777e-01  5.48184037e-01\n",
            " -1.98850751e-01 -2.15948790e-01 -1.48609138e+00  1.08283556e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYL3rZn8gvJL",
        "colab_type": "code",
        "outputId": "42ddd126-ade1-4963-fafe-be6067201bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First convert the 'snt' below into an average of the vector representations of \n",
        "#its tokens\n",
        "snt = x[0] # The first sentence in our dataset\n",
        "\n",
        "sum_of_vectors = np.zeros(300)\n",
        "cnt = 0\n",
        "for tkn in snt:\n",
        "  # Check is token in our vocab first\n",
        "  if tkn in w2v.wv:\n",
        "    tkn_vec = w2v.wv.get_vector(tkn) #? Get the vector representation of the token, but only if it exists in our vocab\n",
        "    sum_of_vectors += tkn_vec #? Add the new vector to the sum for this sentence\n",
        "    cnt += 1#? Increase counter so that we can average\n",
        "  \n",
        "avg_vector = sum_of_vectors / cnt #? Calculate the average of the sum_of_vectors\n",
        "print(avg_vector.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZsis4V88i-",
        "colab_type": "text"
      },
      "source": [
        "# Convert each sentence into the average of the vector representations of its tokens\n",
        "\n",
        "Save the results into a new variable x_emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTQktTzBdaYS",
        "colab_type": "code",
        "outputId": "6ee0fa5d-418b-406e-89cb-4de6351f34f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# TODO\n",
        "\n",
        "# x_emb - embedded sentences\n",
        "x_emb = np.zeros((len(x), 300))\n",
        "\n",
        "# Loop over sentences\n",
        "for i_snt, snt in enumerate(x):\n",
        "  cnt = 0\n",
        "  sum_of_vectors = np.zeros(300)\n",
        "  for tkn in snt:\n",
        "  # Check is token in our vocab first\n",
        "    if tkn in w2v.wv:\n",
        "      tkn_vec = w2v.wv.get_vector(tkn) #? Get the vector representation of the token, but only if it exists in our vocab\n",
        "      sum_of_vectors += tkn_vec #? Add the new vector to the sum for this sentence\n",
        "      cnt += 1 #? Increase counter so that we can average\n",
        "  if cnt > 0:\n",
        "    avg_vector = sum_of_vectors / cnt\n",
        "    x_emb[i_snt] = avg_vector\n",
        " \n",
        "print(x_emb[i_snt])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.09287069 -0.24536227  0.14207411 -0.03796183  0.00661839  0.29292349\n",
            " -0.46096396  0.46242709  0.13307727 -0.2462389  -0.25988647  0.07615793\n",
            "  0.51380004  0.06841991 -0.15085909 -0.00195103  0.08264603  0.0597985\n",
            "  0.33614502  0.2763594   0.23872204 -0.03425322  0.15382106 -0.19238496\n",
            "  0.27763515 -0.14441383  0.09772283 -0.29958617 -0.2778395  -0.02571391\n",
            "  0.18419496  0.18460253  0.10853763 -0.03922015 -0.17264048 -0.08447553\n",
            " -0.06543151  0.10224859  0.03560635  0.08126945  0.01655208  0.00729838\n",
            " -0.1991234  -0.01215077 -0.06273152  0.00180257  0.44071803 -0.11943759\n",
            " -0.09657203  0.2422451   0.01898603 -0.00415104 -0.0500241   0.01995836\n",
            " -0.33875786  0.26056826 -0.07102983 -0.27119083  0.00443615 -0.20041377\n",
            "  0.02579126  0.24783248 -0.15850724  0.42030956 -0.36746406  0.02797173\n",
            "  0.02935774  0.07433241 -0.24542138  0.35232959 -0.1252517   0.0340203\n",
            " -0.16333116  0.26660759 -0.03366092  0.11501555 -0.29459976  0.12161115\n",
            "  0.05923075 -0.25727315 -0.01697325 -0.03146195 -0.33754152  0.09058926\n",
            " -0.34066545  0.01423242  0.09310092 -0.11114574  0.09495199 -0.13599759\n",
            "  0.32475675  0.33596434 -0.36916965 -0.06896457 -0.14807499 -0.26107937\n",
            " -0.19246124  0.31967745 -0.096121   -0.19922566  0.12438996 -0.1262494\n",
            " -0.0742701  -0.11231842 -0.23402495 -0.01728063 -0.11319937 -0.16318785\n",
            " -0.21532682 -0.34634788 -0.13094841  0.24231551 -0.03432696  0.22764596\n",
            "  0.33485106 -0.01230447  0.05415497  0.03129978 -0.01825506  0.05974445\n",
            "  0.33529208 -0.05740616  0.22167659 -0.27427241  0.12805749  0.14764283\n",
            " -0.27651474 -0.10906317 -0.08802032  0.09475727 -0.05887102 -0.12710843\n",
            " -0.03488996 -0.39852645 -0.09393431 -0.45619953  0.23347919 -0.07209556\n",
            "  0.00933724  0.26720603  0.63702874  0.16133577  0.02361944  0.316311\n",
            " -0.01841681 -0.21621133 -0.44762807  0.15451369  0.41106361 -0.07241594\n",
            "  0.28018387  0.023103   -0.10313459  0.40604244 -0.34955276 -0.10270223\n",
            "  0.14541189 -0.1504336   0.03085406  0.02385589  0.14002979  0.09759868\n",
            "  0.1217227  -0.28585648 -0.17492752 -0.0653419   0.16130178 -0.12932996\n",
            " -0.22440126  0.03696898 -0.27564119 -0.24210415  0.46175514  0.11303722\n",
            "  0.30653848  0.43438786 -0.04525863 -0.34986447 -0.05258463 -0.09798811\n",
            "  0.30312337  0.21211711 -0.32820544  0.01308493 -0.0708161   0.18280595\n",
            "  0.18612343 -0.0756224  -0.18214561  0.5958597  -0.12530279 -0.19571045\n",
            "  0.07910489 -0.21982617  0.13282352  0.00477224 -0.014695   -0.13613473\n",
            " -0.34482261 -0.03362477  0.36969355 -0.05720061 -0.05661215  0.11379048\n",
            " -0.21021717 -0.28459522  0.05102618  0.17254193 -0.11625792  0.31821326\n",
            "  0.2894129  -0.33117549 -0.16945725 -0.14747769 -0.13329408  0.092295\n",
            "  0.00946734 -0.21765593  0.44948019  0.08127251 -0.09431929 -0.09819266\n",
            "  0.08809272  0.28113389 -0.24505778  0.01435857 -0.05719584  0.27264248\n",
            " -0.01803334 -0.01481575 -0.12429872 -0.11546769  0.0900436  -0.1131879\n",
            "  0.24200995  0.01279832 -0.01851375 -0.17948675  0.18126897  0.06050128\n",
            "  0.18608197 -0.12151632 -0.17263928 -0.1720232  -0.434224    0.151561\n",
            "  0.27175773 -0.20822749 -0.13201152  0.12387792 -0.06856183  0.07293322\n",
            " -0.13441973 -0.19168673  0.36330055  0.07740168 -0.22987724  0.25684412\n",
            "  0.31299102 -0.20954189 -0.04767578 -0.20230619 -0.15242759  0.11126253\n",
            "  0.16344528 -0.14175946  0.35017681 -0.14281539 -0.09132483  0.01520987\n",
            "  0.25994142 -0.33049848  0.21088936 -0.20764353  0.03408268  0.01773659\n",
            "  0.10355521  0.06974778  0.08740136  0.26860973 -0.28660258 -0.07636851\n",
            " -0.01313117  0.01433741 -0.24628539  0.14064134 -0.04685953 -0.08153316\n",
            " -0.38021327 -0.12222601 -0.03504819 -0.40746483  0.12125272 -0.09323196\n",
            "  0.17682471 -0.11966945 -0.33476106 -0.34174309 -0.05816701 -0.0314816 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQJQFjdkUo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get torch stuff\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sklearn.metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATUWE2b9VKd",
        "colab_type": "text"
      },
      "source": [
        "# Split the dataset into train/test/dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kpy4rEGnghM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "inds = np.random.permutation(len(x))\n",
        "inds_train = inds[0:int(0.8*len(x))]\n",
        "inds_test = inds[int(0.8*len(x)):int(0.9*len(x))]\n",
        "inds_dev = inds[int(0.9*len(x)):]\n",
        "\n",
        "# 80% of the dataset\n",
        "x_train = x_emb[inds_train]\n",
        "y_train = y[inds_train]\n",
        "x_train_w = np.array(x)[inds_train]\n",
        "x_train_o = np.array(x_original)[inds_train]\n",
        "\n",
        "# 10% of the dataset\n",
        "x_test = x_emb[inds_test]\n",
        "y_test = y[inds_test]\n",
        "\n",
        "# 10% of the dataset\n",
        "x_dev = x_emb[inds_dev]\n",
        "y_dev = y[inds_dev]\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "x_dev = torch.tensor(x_dev, dtype=torch.float32)\n",
        "y_dev = torch.tensor(y_dev.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH3ssL6_dK4",
        "colab_type": "text"
      },
      "source": [
        "#Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjDb8Wz0m2PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# TODO: Set the device to 'cuda'\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# TODO: Build a network with 4 layers:\n",
        "#1: 150 neurons and ReLU activation (300 inputs)\n",
        "#2: 70 neurons and ReLU activation\n",
        "#3: 25 neurons and ReLU activation\n",
        "#4: 1 neuron and sigmoid activation\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 150)\n",
        "      self.fc2 = nn.Linear(150, 70)\n",
        "      self.fc3 = nn.Linear(70, 25)\n",
        "      self.fc4 = nn.Linear(25, 1)\n",
        "      \n",
        "      self.d1 = nn.Dropout(0.5)\n",
        "    def forward(self, x):\n",
        "      x = self.d1(torch.relu(self.fc1(x)))\n",
        "      x = self.d1(torch.relu(self.fc2(x)))\n",
        "      x = self.d1(torch.relu(self.fc3(x)))\n",
        "      x = torch.sigmoid(self.fc4(x))\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMz8tBisAaGA",
        "colab_type": "code",
        "outputId": "63e1b294-d455-421c-8820-7e035874af8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Create the network and get BCE loss\n",
        "net = Net()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# TODO: Make a SGD optimizer with lr=0.002 and momentum=0.99 - use 'optim'\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.99)\n",
        "# TODO: Move the net to the device\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (fc2): Linear(in_features=150, out_features=70, bias=True)\n",
              "  (fc3): Linear(in_features=70, out_features=25, bias=True)\n",
              "  (fc4): Linear(in_features=25, out_features=1, bias=True)\n",
              "  (d1): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7VRuEb9bj1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtU1lLR_cfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move data to the right device\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "x_dev = x_dev.to(device)\n",
        "y_dev = y_dev.to(device)\n",
        "\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXYUTXUoS-t",
        "colab_type": "code",
        "outputId": "06173d19-0143-4417-ff77-1c0e754d9de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "net.train()\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "for epoch in range(3000):  # do 10,000 epochs \n",
        "  # TODO: zero the gradients on the optimizer\n",
        "  optimizer.zero_grad() #?\n",
        "\n",
        "  # TODO: Calculate the forward pass \n",
        "  outputs = net(x_train)#?\n",
        "  # TODO: Calculate the loss\n",
        "  loss = criterion(outputs, y_train)#?\n",
        "  # TODO: Backward pass\n",
        "  loss.backward()#?\n",
        "  # TODO: Optimize/Update parameters\n",
        "  optimizer.step() #?\n",
        "  \n",
        "  # Track the changes - This is normally done using tensorboard or similar\n",
        "  losses.append(loss.item())\n",
        "  accs.append(sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy()))\n",
        "\n",
        "  # print statistics\n",
        "  if epoch % 500 == 0:\n",
        "      net.eval()\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_dev)\n",
        "      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "      accs_dev.append(acc_dev)\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "      net.train()\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.69605 Acc: 0.502 Acc Dev: 0.484\n",
            "Epoch:  500 Loss: 0.48247 Acc: 0.791 Acc Dev: 0.821\n",
            "Epoch: 1000 Loss: 0.40594 Acc: 0.829 Acc Dev: 0.844\n",
            "Epoch: 1500 Loss: 0.38950 Acc: 0.841 Acc Dev: 0.842\n",
            "Epoch: 2000 Loss: 0.38047 Acc: 0.845 Acc Dev: 0.842\n",
            "Epoch: 2500 Loss: 0.36921 Acc: 0.852 Acc Dev: 0.842\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhPQXS_uopWq",
        "colab_type": "code",
        "outputId": "e275b363-155c-4416-964d-8a7ed77a3065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Switch to eval mode \n",
        "net.eval()\n",
        "net.to(\"cpu\")\n",
        "print(\"Predicted value: \" + str(net(torch.tensor(x_emb[4], dtype=torch.float32))))\n",
        "print(\"Real value: \" + str(y[4]))\n",
        "print(\"Input sentence: \" + df['SentimentText'].iloc[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted value: tensor([0.7846], grad_fn=<SigmoidBackward>)\n",
            "Real value: 1\n",
            "Input sentence: truly enjoyed film. acting terrific plot. Jeff Combs talent recognized for. part flick would change ending. death creature far gruesome Sci Fi Channel.<br /><br />There interesting religious messages film. Jeff Combs obviously played Messiah figure creature (or shark prefer) represented anti-Chirst. particularly frightening scenes 'end world feel'. noticed third viewing classic creature feature. know many people won't get references Christianity, watch close you'll get it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJsArQIGHz3B",
        "colab_type": "text"
      },
      "source": [
        "#Go back and add dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGjzLNow-T9_",
        "colab_type": "text"
      },
      "source": [
        "#Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2vaQdnM-Wyf",
        "colab_type": "code",
        "outputId": "8362225c-ba04-4d99-b78c-992bc9213e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Make a small network with one layer\n",
        "device = torch.device('cuda')\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 1)\n",
        "       \n",
        "    def forward(self, x):\n",
        "      x = torch.sigmoid(self.fc1(x))\n",
        "      return x\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.99)\n",
        "for epoch in range(4000):  # do 10,000 epochs \n",
        "  # zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Forward \n",
        "  outputs = net(x_train)\n",
        "  # Calculate error\n",
        "  loss = criterion(outputs, y_train)\n",
        "  # Backward\n",
        "  loss.backward()\n",
        "  # Optimize/Update parameters\n",
        "  optimizer.step()\n",
        "  \n",
        "  # Track the changes - This is normally done using tensorboard or similar\n",
        "  losses.append(loss.item())\n",
        "\n",
        "  # print statistics\n",
        "  if epoch % 500 == 0:\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_dev)\n",
        "      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.70342 Acc: 0.352 Acc Dev: 0.341\n",
            "Epoch:  500 Loss: 0.44560 Acc: 0.803 Acc Dev: 0.814\n",
            "Epoch: 1000 Loss: 0.41732 Acc: 0.819 Acc Dev: 0.828\n",
            "Epoch: 1500 Loss: 0.40601 Acc: 0.824 Acc Dev: 0.833\n",
            "Epoch: 2000 Loss: 0.39968 Acc: 0.827 Acc Dev: 0.835\n",
            "Epoch: 2500 Loss: 0.39559 Acc: 0.828 Acc Dev: 0.836\n",
            "Epoch: 3000 Loss: 0.39271 Acc: 0.829 Acc Dev: 0.836\n",
            "Epoch: 3500 Loss: 0.39056 Acc: 0.831 Acc Dev: 0.836\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVx2mD7b_YZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the prediction of the first 30 sentences\n",
        "for i in range(30):\n",
        "  out = net(x_train[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCVkmeCE_iKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the 'i_snt' to high prob\n",
        "i_snt = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeAWnVvA-r4h",
        "colab_type": "code",
        "outputId": "d3c65d3e-9024-45b7-f918-eef407e79e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "prod = []\n",
        "# Take the weights of our layer\n",
        "we = net.fc1.weight.detach().cpu().numpy()[0]\n",
        "# For each word in the dataset check how much did it contribute to the \n",
        "#final classification\n",
        "for ind, word in enumerate(x_train_w[i_snt]):\n",
        "  if word in w2v:\n",
        "    prod.append(np.dot(we, w2v.wv.get_vector(word)))\n",
        "  else:\n",
        "    prod.append(0)\n",
        "    \n",
        "# Sort the contributions\n",
        "srt = np.argsort(prod)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mewfla3Q_Gfi",
        "colab_type": "code",
        "outputId": "8709c806-3ac3-4684-8041-5f77529fafbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "print(x_train_o[i_snt].replace(\"<br /><br />\", \"\\n\"))\n",
        "print()\n",
        "# print the top 10 words that contributed the most\n",
        "for i in range(1, 30):\n",
        "  print(x_train_w[i_snt][srt[-i]], prod[srt[-i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1954 Marlon Brando hot actor performances Streetcar Named Desire Waterfront. Frank Sinatra yet re-invent silver screen. Sinatra's portrayal erstwhile Nathan Detroit, helped re-establish Sinatra fans.\n",
            "It great screen version great play choices leads support players terrific. Imagine movie Brando sings? one singing role portrayed Sky Masterson. addition female leads, Jean Simmons Vivian Blaine(replaying stage role Nathan's long suffering girlfriend Adelade), put superlative efforts. Special mention goes great Stubby Kaye(as Nicely Nicely), due respect Eric Clapton, one's version Rockin' Boat even comes close Stubby's. Sheldon Leonard, would go fame TV producer shows Danny Thomas Show Dick Van Dyke Show \"Harry Horse\" wonders, B.S.Pulley excellent harsh mannered rough talking \"Big Julie\", even Regis Toomey offers excellence \"Brother Arvide\".\n",
            "It one fun musicals see, good comedy, get Sinatra Brando. Soooooo \"Luck Lady Tonight\" brother...\"it's dice\"\n",
            "\n",
            "great 32.21438\n",
            "great 32.21438\n",
            "great 32.21438\n",
            "excellent 27.871546\n",
            "fun 17.50863\n",
            "support 15.690714\n",
            "good 14.907042\n",
            "musical 14.544361\n",
            "terrific 14.300327\n",
            "fan 13.5328245\n",
            "comedy 12.824418\n",
            "performance 12.190239\n",
            "frank 11.946346\n",
            "show 10.807332\n",
            "close 9.642669\n",
            "portrayal 9.593542\n",
            "sinatra 9.382718\n",
            "sinatra 9.382718\n",
            "sinatra 9.382718\n",
            "sinatra 9.382718\n",
            "sing 8.146638\n",
            "player 7.751675\n",
            "danny 7.730137\n",
            "nicely 7.6946855\n",
            "nicely 7.6946855\n",
            "offer 7.5410924\n",
            "sky 7.368249\n",
            "harry 7.3064404\n",
            "singing 7.260653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoAXnTuhAenx",
        "colab_type": "code",
        "outputId": "51c7a5a3-6ab0-4c1f-a548-82c7b9e1cd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#TODO: Write a movie review - at least 20 words\n",
        "review = \"This movie was amazing!! However the actors are not well known and there were some scenes where i could not see anything. Additionally, the plot was crap.\" #?\n",
        "print(review)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This movie was amazing!! However the actors are not well known and there were some scenes where i could not see anything. Additionally, the plot was crap.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TemK6Xj4GlbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Tokenize the review using spacy - same as we have done above\n",
        "tkns = [token.lemma_.lower() for token in nlp(review) if not token.is_punct and not token.is_stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhH-GngNCqwz",
        "colab_type": "code",
        "outputId": "944c2945-7289-40ee-f5f4-58ea404ab6aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# TODO: Convert the tokens to vectors and save into an array\n",
        "vecs = [w2v.wv.get_vector(token) for token in tkns if token in w2v.wv]#?\n",
        "print(vecs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.23307408 -0.3971362  -0.11596384  0.09229731 -0.05499481  0.1590928\n",
            " -1.0342268   1.471547   -0.13434096  0.03753657 -0.01452418  0.17316674\n",
            "  1.4115424   0.22116984 -0.32026187  1.0443661   0.47027415  0.1807275\n",
            "  1.2882869   0.8986692   0.5330685  -0.25538912  0.46406978 -0.3962227\n",
            "  0.07543738 -0.06695285  0.6962083  -0.65619725 -1.1657215   0.03593308\n",
            "  0.74974495  0.7860926  -0.31626332 -0.15976623 -0.60397667 -0.9142768\n",
            "  0.4888428  -0.32787043 -0.55616504 -0.08931158  0.44264263  0.4462606\n",
            " -0.74852204 -0.55297947 -0.3885132  -0.2542718  -0.07653806 -0.56970674\n",
            "  0.37848547  0.4839474  -0.5116596   0.6549463   0.9011514   0.09162443\n",
            " -0.8019317   0.7367139   0.11066645 -0.10954394  0.05827421 -0.3526521\n",
            " -0.16299616 -0.2111744  -0.5136807  -0.49893633 -0.48659208 -0.790021\n",
            "  0.1736023   0.21710636 -0.6017499   0.4676928  -0.34681654  0.31822628\n",
            "  0.47012362  0.08905905 -0.02701258  0.29690832 -0.41033524 -0.02711443\n",
            "  0.23891926 -0.7056184  -0.20055011 -0.33935177 -0.01373479  0.23464802\n",
            " -0.64952064  0.3366318   0.86031646 -0.65933836  0.3458946  -0.04302838\n",
            "  0.77117157 -0.23591034 -0.1550263   0.09452722 -0.5485202  -1.1731393\n",
            " -0.00466508  1.1601636  -0.06339879 -0.7485631   0.96470237 -0.27127182\n",
            " -0.45351464  0.41889277  0.69245434  0.13440345 -0.9633925  -0.8972834\n",
            " -0.3943374  -0.8043629   0.5405838  -0.03125945  0.4202029   0.30087993\n",
            "  0.5431484  -0.03619866  0.45756334  0.92751706  0.52821887 -0.20250678\n",
            "  0.46520555  0.25227094  0.5463261  -0.08313231 -0.15484568 -0.06441558\n",
            " -0.8833332  -0.43702725 -0.76888317 -0.40731952  0.29525894  0.16947809\n",
            " -0.4978537  -1.1382569  -0.25432912 -0.92185986  0.34947938  0.48754427\n",
            "  0.50131685  0.29700482  1.5164895   0.7594282   0.42343983  1.1622307\n",
            " -0.60855263  0.2767735  -0.06806646  0.78671855  0.200445   -0.545224\n",
            " -0.32144526 -0.07030977 -0.71048963  0.05743574 -0.26205388 -0.6130973\n",
            "  0.88464946  0.4767329   0.8255835   0.57017124 -0.43255937  0.70751595\n",
            "  0.1637949  -0.8592087   0.09570087 -0.30336538  0.08609382 -0.8616718\n",
            " -0.01823367  0.18278903 -1.2278744  -0.02977156  0.7929767   0.14714569\n",
            "  0.53115803 -0.6062962  -0.21774383  0.08173488 -0.11799829  0.28062513\n",
            " -0.02571613  0.32479608 -1.0061975  -0.24962549 -0.3575132   0.66260016\n",
            "  0.16964844  1.0132877   0.25543603  0.5458654  -0.17647694 -0.22973843\n",
            "  0.21726647 -0.5450304   0.96463776  0.7432346  -1.0726218  -0.32657284\n",
            " -0.5770442   0.34228402 -0.11711638  1.5940422   0.01683745  0.42915595\n",
            " -0.34020433 -0.3135471   0.65650165  0.5833968   0.08817231  0.54014915\n",
            "  0.8475845   0.7078637  -0.16880217 -0.41601628 -0.53973556  0.47070283\n",
            " -0.31993014 -0.62442553  1.0921655   0.61888933 -0.35727692 -0.78000945\n",
            "  0.15665738  0.19584279 -0.23623392  0.10030858 -0.26417783 -0.09989706\n",
            " -0.7481664   0.49817657 -0.05463504 -0.74657536  0.2934827   0.24766524\n",
            "  0.57683045 -0.17316815  0.5153623   0.62872434  0.75306356  0.3497277\n",
            "  0.62423337  0.39802524 -0.54814845  0.19466408 -1.3519369  -0.00822675\n",
            "  0.4150856  -1.1006287  -0.84478015  0.14833581 -0.6113643   0.27049276\n",
            " -0.34237128 -0.2807779   1.1333208  -0.48820576 -0.54832304  0.9462108\n",
            "  0.5748018  -0.1377365   0.45491046 -0.48289877 -0.7676849   0.26009688\n",
            "  0.15788402 -0.24689843  1.111878    0.26880953 -0.5630705   0.09744151\n",
            "  0.3033149  -0.46221894  0.24331036 -0.01455945  0.3267658  -0.4123394\n",
            "  0.6895683  -0.3126089  -0.7368355  -0.5959556  -1.1416135  -0.43742126\n",
            " -0.47739568 -0.48630098 -0.71436125  0.38789043  0.50956964 -0.65296173\n",
            " -0.99431443  0.59680516 -1.0062283  -0.56894016  0.45068276  0.16351257\n",
            "  0.45774904 -0.07954554 -0.5048781  -0.02467592 -0.78875875  0.06616151]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSocSg4SGugV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Average the vectors and convert to torch tensor\n",
        "emb = torch.tensor(np.average(vecs, axis = 0)) #?\n",
        "emb = emb.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWwz_6vqC6ZD",
        "colab_type": "code",
        "outputId": "a6242def-c54a-4731-f5c7-e99d796bb7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: Calculate the output for our review\n",
        "print(net(emb)) #?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0218], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APusGyPaC9V1",
        "colab_type": "code",
        "outputId": "6705a7d5-75a0-4175-bc9f-660e8981fb0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# TODO: Multiply the vector representations of tokens by 'we' and print \n",
        "#contribution of each word\n",
        "for ind, vec in enumerate(vecs):\n",
        "  print(tkns[ind], np.dot(we, vec)) #?  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movie 0.31889373\n",
            "amazing 21.138624\n",
            "actor -4.8174267\n",
            "know 3.0659153\n",
            "scene 6.304481\n",
            "additionally -0.48813602\n",
            "plot -25.550776\n",
            "crap -26.560837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ4bg8QFKJVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}