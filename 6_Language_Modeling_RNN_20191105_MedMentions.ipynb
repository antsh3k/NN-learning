{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TODO: Language Modeling - RNN 20191105- MedMentions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antsh3k/NN-learning/blob/master/6_Language_Modeling_RNN_20191105_MedMentions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o_v0SBzb8Xj",
        "colab_type": "text"
      },
      "source": [
        "# Switch to GPU\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoNTo1TicXwD",
        "colab_type": "text"
      },
      "source": [
        "## Imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DampUSlI0H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTS (try to organize/group your imports)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAHbz8p1JKC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Any global variables\n",
        "SEED = 15\n",
        "DATA_PATH = '/content/' # Used for Colab\n",
        "MAX_SEQ_LEN = 12 # We go with something small as the dataset is small\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "DEVICE = 'cuda'\n",
        "BATCH_SIZE = 64 # Again small as no powerful GPUs\n",
        "\n",
        "# Set SEEDs\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsTIvekTJzPC",
        "colab_type": "text"
      },
      "source": [
        "## Download the data\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Change directory accordingly if working locally or not on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RZ23ln_JOm5",
        "colab_type": "code",
        "outputId": "a112a989-9b99-40e5-e3e8-413b20c4f47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!wget https://github.com/w-is-h/DeepLearningNLP/raw/master/Session_7/data/text_medmentions.txt -P /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-05 13:30:53--  https://github.com/w-is-h/DeepLearningNLP/raw/master/Session_7/data/text_medmentions.txt\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/w-is-h/DeepLearningNLP/master/Session_7/data/text_medmentions.txt [following]\n",
            "--2019-11-05 13:30:55--  https://raw.githubusercontent.com/w-is-h/DeepLearningNLP/master/Session_7/data/text_medmentions.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7157528 (6.8M) [text/plain]\n",
            "Saving to: ‘/content/text_medmentions.txt’\n",
            "\n",
            "text_medmentions.tx 100%[===================>]   6.83M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-11-05 13:30:55 (118 MB/s) - ‘/content/text_medmentions.txt’ saved [7157528/7157528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfqNTBm4x4NH",
        "colab_type": "code",
        "outputId": "5d9b4d9f-db44-43ba-f1a6-de46bb605b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Display a couple of lines from the downloaded file\n",
        "#?TODO\n",
        "!head /content/text_medmentions.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DCTN4\tas\ta\tmodifier\tof\tchronic\tPseudomonas\taeruginosa\tinfection\tin\tcystic\tfibrosis\tPseudomonas\taeruginosa\t(\tPa\t)\tinfection\tin\tcystic\tfibrosis\t(\tCF\t)\tpatients\tis\tassociated\twith\tworse\tlong\t-\tterm\tpulmonary\tdisease\tand\tshorter\tsurvival\t,\tand\tchronic\tPa\tinfection\t(\tCPA\t)\tis\tassociated\twith\treduced\tlung\tfunction\t,\tfaster\trate\tof\tlung\tdecline\t,\tincreased\trates\tof\texacerbations\tand\tshorter\tsurvival\t.\tBy\tusing\texome\tsequencing\tand\textreme\tphenotype\tdesign\t,\tit\twas\trecently\tshown\tthat\tisoforms\tof\tdynactin\t4\t(\tDCTN4\t)\tmay\tinfluence\tPa\tinfection\tin\tCF\t,\tleading\tto\tworse\trespiratory\tdisease\t.\tThe\tpurpose\tof\tthis\tstudy\twas\tto\tinvestigate\tthe\trole\tof\tDCTN4\tmissense\tvariants\ton\tPa\tinfection\tincidence\t,\tage\tat\tfirst\tPa\tinfection\tand\tchronic\tPa\tinfection\tincidence\tin\ta\tcohort\tof\tadult\tCF\tpatients\tfrom\ta\tsingle\tcentre\t.\tPolymerase\tchain\treaction\tand\tdirect\tsequencing\twere\tused\tto\tscreen\tDNA\tsamples\tfor\tDCTN4\tvariants\t.\tA\ttotal\tof\t121\tadult\tCF\tpatients\tfrom\tthe\tCochin\tHospital\tCF\tcentre\thave\tbeen\tincluded\t,\tall\tof\tthem\tcarrying\ttwo\tCFTR\tdefects\t:\t103\tdeveloped\tat\tleast\t1\tpulmonary\tinfection\twith\tPa\t,\tand\t68\tpatients\tof\tthem\thad\tCPA\t.\tDCTN4\tvariants\twere\tidentified\tin\t24\t%\t(\t29/121\t)\tCF\tpatients\twith\tPa\tinfection\tand\tin\tonly\t17\t%\t(\t3/18\t)\tCF\tpatients\twith\tno\tPa\tinfection\t.\tOf\tthe\tpatients\twith\tCPA\t,\t29\t%\t(\t20/68\t)\thad\tDCTN4\tmissense\tvariants\tvs\t23\t%\t(\t8/35\t)\tin\tpatients\twithout\tCPA\t.\tInterestingly\t,\tp\t.\tTyr263Cys\ttend\tto\tbe\tmore\tfrequently\tobserved\tin\tCF\tpatients\twith\tCPA\tthan\tin\tpatients\twithout\tCPA\t(\t4/68\tvs\t0/35\t)\t,\tand\tDCTN4\tmissense\tvariants\ttend\tto\tbe\tmore\tfrequent\tin\tmale\tCF\tpatients\twith\tCPA\tbearing\ttwo\tclass\tII\tmutations\tthan\tin\tmale\tCF\tpatients\twithout\tCPA\tbearing\ttwo\tclass\tII\tmutations\t(\tP\t=\t0.06\t)\t.\tOur\tobservations\treinforce\tthat\tDCTN4\tmissense\tvariants\t,\tespecially\tp\t.\tTyr263Cys\t,\tmay\tbe\tinvolved\tin\tthe\tpathogenesis\tof\tCPA\tin\tmale\tCF\t.\t\n",
            "Nonylphenol\tdiethoxylate\tinhibits\tapoptosis\tinduced\tin\tPC12\tcells\tNonylphenol\tand\tshort\t-\tchain\tnonylphenol\tethoxylates\tsuch\tas\tNP2\tEO\tare\tpresent\tin\taquatic\tenvironment\tas\twastewater\tcontaminants\t,\tand\ttheir\ttoxic\teffects\ton\taquatic\tspecies\thave\tbeen\treported\t.\tApoptosis\thas\tbeen\tshown\tto\tbe\tinduced\tby\tserum\tdeprivation\tor\tcopper\ttreatment\t.\tTo\tunderstand\tthe\ttoxicity\tof\tnonylphenol\tdiethoxylate\t,\twe\tinvestigated\tthe\teffects\tof\tNP2\tEO\ton\tapoptosis\tinduced\tby\tserum\tdeprivation\tand\tcopper\tby\tusing\tPC12\tcell\tsystem\t.\tNonylphenol\tdiethoxylate\titself\tshowed\tno\ttoxicity\tand\trecovered\tcell\tviability\tfrom\tapoptosis\t.\tIn\taddition\t,\tnonylphenol\tdiethoxylate\tdecreased\tDNA\tfragmentation\tcaused\tby\tapoptosis\tin\tPC12\tcells\t.\tThis\tphenomenon\twas\tconfirmed\tafter\ttreating\tapoptotic\tPC12\tcells\twith\tnonylphenol\tdiethoxylate\t,\twhereas\tthe\tcytochrome\tc\trelease\tinto\tthe\tcytosol\tdecreased\tas\tcompared\tto\tthat\tin\tapoptotic\tcells\tnot\ttreated\twith\tnonylphenol\tdiethoxylate\ts.\tFurthermore\t,\tBax\tcontents\tin\tapoptotic\tcells\twere\treduced\tafter\texposure\tto\tnonylphenol\tdiethoxylate\t.\tThus\t,\tnonylphenol\tdiethoxylate\thas\tthe\topposite\teffect\ton\tapoptosis\tin\tPC12\tcells\tcompared\tto\tnonylphenol\t,\twhich\tenhances\tapoptosis\tinduced\tby\tserum\tdeprivation\t.\tThe\tdifference\tin\tstructure\tof\tthe\ttwo\tcompounds\tis\thypothesized\tto\tbe\tresponsible\tfor\tthis\tphenomenon\t.\tThese\tresults\tindicated\tthat\tnonylphenol\tdiethoxylate\thas\tcapability\tto\taffect\tcell\tdifferentiation\tand\tdevelopment\tand\thas\tpotentially\tharmful\teffect\ton\torganisms\tbecause\tof\tits\tunexpected\timpact\ton\tapoptosis\t.\t©\t2015\tWiley\tPeriodicals\t,\tInc.\tEnviron\tToxicol\t31\t:\t1389\t-\t1398\t,\t2016\t.\t\n",
            "Prevascularized\tsilicon\tmembranes\tfor\tthe\tenhancement\tof\ttransport\tto\timplanted\tmedical\tdevices\tRecent\tadvances\tin\tdrug\tdelivery\tand\tsensing\tdevices\tfor\tin\tsitu\tapplications\tare\tlimited\tby\tthe\tdiffusion\t-limiting\tforeign\tbody\tresponse\tof\tfibrous\tencapsulation\t.\tIn\tthis\tstudy\t,\twe\tfabricated\tprevascularized\tsynthetic\tdevice\tports\tto\thelp\tmitigate\tthis\tlimitation\t.\tMembranes\twith\trectilinear\tarrays\tof\tsquare\tpores\twith\twidths\tranging\tfrom\t40\tto\t200\tμm\twere\tcreated\tusing\tmaterials\t(\t50\tμm\tthick\tdouble\t-\tsided\tpolished\tsilicon\t)\tand\tprocesses\t(\tphotolithography\tand\tdirected\treactive\tion\tetching\t)\tcommon\tin\tthe\tmanufacturing\tof\tmicrofabricated\tsensors\t.\tVascular\tendothelial\tcells\tresponded\tto\tmembrane\tgeometry\tby\teither\tforming\tvascular\ttubes\tthat\textended\tthrough\tthe\tpore\tor\tcompletely\tfilling\tmembrane\tpores\tafter\t4\tdays\tin\tculture\t.\tAlthough\ttube\tformation\tbegan\tto\tpredominate\tovergrowth\taround\t75\tμm\tand\tcontinued\tto\tincrease\tat\teven\tlarger\tpore\tsizes\t,\ttubes\tformed\tat\tthese\tlarge\tpore\tsizes\twere\tnot\tcompletely\tround\tand\thad\trelatively\tthin\twalls\t.\tThus\t,\tthe\toptimum\trange\tof\tpore\tsize\tfor\tprevascularization\tof\tthese\tmembranes\twas\testimated\tto\tbe\t75\t-\t100\tμm\t.\tThis\tstudy\tlays\tthe\tfoundation\tfor\tcreating\ta\tprevascularized\tport\tthat\tcan\tbe\tused\tto\treduce\tfibrous\tencapsulation\tand\tthus\tenhance\tdiffusion\tto\timplanted\tmedical\tdevices\tand\tsensors\t.\t©\t2015\tWiley\tPeriodicals\t,\tInc.\tJ\tBiomed\tMater\tRes\tPart\tB\t:\tAppl\tBiomater\t,\t104B\t:\t1602\t-\t1609\t,\t2016\t.\t\n",
            "Seated\tmaximum\tflexion\t:\tAn\talternative\tto\tstanding\tmaximum\tflexion\tfor\tdetermining\tpresence\tof\tflexion\t-\trelaxation\t?\tThe\tflexion\t-\trelaxation\tphenomenon\t(\tFRP\t)\tin\tstanding\tis\ta\tspecific\tand\tsensitive\tdiagnostic\ttool\tfor\tlow\tback\tpain\t.\tSeated\tflexion\tas\tan\talternative\tcould\tbe\tbeneficial\tfor\tcertain\tpopulations\t,\tyet\tthe\tbehavior\tof\tthe\ttrunk\textensors\tduring\tseated\tmaximum\tflexion\tcompared\tto\tstanding\tflexion\tremains\tunclear\t.\tCompare\tFRP\toccurrences\tand\tspine\tangles\tbetween\tseated\tand\tstanding\tflexion\tpostures\tin\tthree\tlevels\tof\tthe\terector\tspinae\tmuscles\t.\tThirty\t-\tone\tparticipants\tfree\tof\tback\tpain\tperformed\tseated\tand\tstanding\tmaximum\ttrunk\tflexion\t.\tElectromyographical\tsignals\twere\trecorded\tfrom\tthe\tbilateral\tlumbar\t(\tL3\t)\t,\tlower\t-\tthoracic\t(\tT9\t)\t,\tand\tupper\t-\tthoracic\t(\tT4\t)\terector\tspinae\tand\tassessed\tfor\tthe\toccurrence\tof\tFRP\t.\tSpine\tangles\tcorresponding\tto\tFRP\tonset\tand\tcessation\twere\tdetermined\t,\tand\tFRP\toccurrences\tand\tangles\twere\tcompared\tbetween\tposture\tand\tmuscle\t.\tFRP\toccurrence\twas\tsimilar\tin\tstanding\tand\tseated\tmaximum\tflexion\tacross\tall\tmuscles\t,\twith\tthe\tlumbar\tmuscles\tshowing\tthe\tgreatest\tconsistency\t.\tStanding\tFRP\tonset\tand\tcessation\tangles\twere\tconsistently\tgreater\tthan\tthe\tcorresponding\tseated\tFRP\tangles\t.\tConsidering\tthe\tsimilar\tnumber\tof\tFRP\toccurrences\t,\tseated\tmaximum\tflexion\tmay\tconstitute\tan\tobjective\tcriterion\tfor\tlow\tback\tpain\tdiagnosis\t.\tFuture\twork\tshould\tseek\tto\tconfirm\tthe\tutility\tof\tthis\ttest\tin\tindividuals\twith\tlow\tback\tpain\t.\t\n",
            "The\tRelationship\tBetween\tDistance\tand\tPost\t-\toperative\tVisit\tAttendance\tFollowing\tMedical\tMale\tCircumcision\tin\tNyanza\tProvince\t,\tKenya\tTo\tdate\t,\tthere\tis\tno\tresearch\ton\tvoluntary\tmedical\tmale\tcircumcision\t(\tVMMC\t)\tcatchment\tareas\tor\tthe\trelationship\tbetween\tdistance\tto\ta\tVMMC\tfacility\tand\tattendance\tat\ta\tpost\t-\toperative\tfollow\t-\tup\tvisit\t.\tWe\tanalyzed\tdata\tfrom\ta\trandomly\tselected\tsubset\tof\tmales\tself\t-\tseeking\tcircumcision\tat\tone\tof\t16\tparticipating\tfacilities\tin\tNyanza\tProvince\t,\tKenya\tbetween\t2008\tand\t2010\t.\tAmong\t1437\tparticipants\t,\t46.7\t%\tattended\tfollow\t-\tup\t.\tThe\tmedian\tdistance\tfrom\tresidence\tto\tutilized\tfacility\twas\t2.98\tkm\t(\tIQR\t1.31\t-\t5.38\t)\t.\tNearly\tall\tparticipants\t(\t98.8\t%\t)\tlived\twithin\t5\tkm\tfrom\ta\tfacility\t,\thowever\t,\t26.3\t%\tvisited\ta\tfacility\tmore\tthan\t5\tkm\taway\t.\tStratified\tresults\tdemonstrated\tthat\tamong\tthose\tutilizing\tfixed\tfacilities\t,\tgreater\tdistance\twas\tassociated\twith\thigher\todds\tof\tfollow\t-\tup\tnon\t-\tattendance\t(\tOR\t5.01\t-\t10\tkm\tvs.\t0\t-\t1\tkm\t=\t1.71\t,\t95\t%\tCI\t1.08\t,\t2.70\t,\tp\t=\t0.02\t;\tOR\t>\t10\tkm\tvs.\t0\t-\t1\tkm\t=\t2.80\t,\t95\t%\tCI\t1.26\t,\t6.21\t,\tp\t=\t0.01\t)\t,\tadjusting\tfor\tage\tand\tdistrict\tof\tresidence\t.\tWe\tfound\t5\tkm\tmarked\tthe\tthreshold\tdistance\tbeyond\twhich\tfollow\t-\tup\tattendance\tsignificantly\tdropped\t.\tThese\tresults\tdemonstrate\tdistance\tis\tan\timportant\tpredictor\tof\tattending\tfollow\t-\tup\t,\tand\tthis\trelationship\tappears\tto\tbe\tmodified\tby\tfacility\ttype\t.\t\n",
            "Promoting\tlifestyle\tbehaviour\tchange\tand\twell\t-\tbeing\tin\thospital\tpatients\t:\ta\tpilot\tstudy\tof\tan\tevidence\t-\tbased\tpsychological\tintervention\tLifestyle\trisk\tbehaviours\tshow\tan\tinverse\tsocial\tgradient\t,\tclustering\tin\tvulnerable\tgroups\t.\tWe\tdesigned\tand\tpiloted\tan\tintervention\tto\taddress\tbarriers\tto\tlifestyle\tbehaviour\tchange\tamong\thospital\tpatients\t.\tWe\tdesigned\tour\tintervention\tusing\teffective\tcomponents\tof\tbehaviour\tchange\tinterventions\tinformed\tby\tpsychological\ttheory\t.\tDelivered\tby\ta\thealth\tpsychologist\tbased\tat\tthe\tRoyal\tFree\tLondon\tNHS\tFoundation\tTrust\t,\tthe\t4-week\tintervention\tincluded\tdetailed\tbaseline\tassessment\t,\tpersonalized\tgoal\tsetting\t,\tpsychological\tskills\tdevelopment\t,\tmotivation\tsupport\tand\treferral\tto\tcommunity\tservices\t.\tPrimary\toutcomes\twere\tfeasibility\tand\tpatient\tacceptability\t.\tWe\talso\tevaluated\tchanges\tto\thealth\tand\twell\t-\tbeing\t.\tFrom\t1\tJuly\t2013\tto\t31\tSeptember\t2014\t,\t686\tpatients\twere\treferred\t,\t338\t(\t49.3\t%\t)\tattended\ta\tfirst\tappointment\tand\t172\t(\t25.1\t%\t)\tcompleted\tfollow\t-\tup\t.\tFurthermore\t,\t72.1\t%\tof\tattenders\twere\tfemale\twith\tthe\tmedian\tage\t55\tyears\tand\tpoor\tself\t-\treported\tbaseline\thealth\t.\tAfter\t4\tweeks\t,\tself\t-\tefficacy\t,\thealth\tand\twell\t-\tbeing\tscores\tsignificantly\timproved\t:\t63\t%\tof\tlifestyle\tgoals\tand\t89\t%\tof\thealth\tmanagement\tgoals\twere\tfully\tachieved\t;\t58\t%\tof\treferrals\tto\tcommunity\tlifestyle\tbehaviour\tchange\tservices\tand\t79\t%\tof\treferrals\tto\tother\tservices\t(\te.g.\tCitizen\t's\tAdvice\tBureau\t)\twere\taccepted\t;\t99\t%\twere\tsatisfied\t/\tvery\tsatisfied\twith\tthe\tservice\t.\tOur\thospital\t-\tbased\tintervention\twas\tfeasible\t,\tacceptable\tand\tshowed\tpreliminary\thealth\tand\twell\t-\tbeing\tgains\t.\t\n",
            "The\tImpact\tof\tDeployment\ton\tParental\t,\tFamily\tand\tChild\tAdjustment\tin\tMilitary\tFamilies\tSince\t9/11\t,\tmilitary\tservice\tin\tthe\tUnited\tStates\thas\tbeen\tcharacterized\tby\twartime\tdeployments\tand\treintegration\tchallenges\tthat\tcontribute\tto\ta\tcontext\tof\tstress\tfor\tmilitary\tfamilies\t.\tResearch\tindicates\tthe\tnegative\timpact\tof\twartime\tdeployment\ton\tthe\twell\tbeing\tof\tservice\tmembers\t,\tmilitary\tspouses\t,\tand\tchildren\t.\tYet\t,\tfew\tstudies\thave\tconsidered\thow\tparental\tdeployments\tmay\taffect\tadjustment\tin\tyoung\tchildren\tand\ttheir\tfamilies\t.\tUsing\tdeployment\trecords\tand\tparent\t-\treported\tmeasures\tfrom\tprimary\tcaregiving\t(\tN\t=\t680\t)\tand\tmilitary\t(\tn\t=\t310\t)\tparents\t,\twe\texamined\tthe\tinfluence\tof\tdeployment\ton\tadjustment\tin\tmilitary\tfamilies\twith\tchildren\tages\t0\t-\t10\tyears\t.\tGreater\tdeployment\texposure\twas\trelated\tto\timpaired\tfamily\tfunctioning\tand\tmarital\tinstability\t.\tParental\tdepressive\tand\tposttraumatic\tstress\tsymptoms\twere\tassociated\twith\timpairments\tin\tsocial\temotional\tadjustment\tin\tyoung\tchildren\t,\tincreased\tanxiety\tin\tearly\tchildhood\t,\tand\tadjustment\tproblems\tin\tschool\t-\tage\tchildren\t.\tConversely\t,\tparental\tsensitivity\twas\tassociated\twith\timproved\tsocial\tand\temotional\toutcomes\tacross\tchildhood\t.\tThese\tfindings\tprovide\tguidance\tto\tdeveloping\tpreventive\tapproaches\tfor\tmilitary\tfamilies\twith\tyoung\tchildren\t.\t\n",
            "Combining\telectrostatic\tpowder\twith\tan\tinsecticide\t:\teffect\ton\tstored\t-\tproduct\tbeetles\tand\ton\tthe\tcommodity\tThe\topportunity\tto\treduce\tthe\tamount\tof\tpirimiphos\t-\tmethyl\tapplied\tto\tgrain\tby\tformulating\tit\tin\tan\telectrostatic\tpowder\twas\tinvestigated\t.\tThe\tinsecticidal\tefficacy\tof\tpirimiphos\t-\tmethyl\tin\tEC\tformulation\tor\tformulated\tusing\telectrostatic\tpowder\t(\tEP\t)\tas\tan\tinert\tcarrier\twas\tinvestigated\tagainst\tSitophilus\toryzae\t(\tL.\t)\t,\tOryzaephilus\tsurinamensis\t(\tL.\t)\t,\tRhyzopertha\tdominica\t(\tF.\t)\tand\tTribolium\tconfusum\tJacquelin\tdu\tVal\t.\tFurthermore\t,\tthe\tadhesive\tproperties\tof\tEP\tto\trice\t,\tcorn\tand\twheat\t,\ttogether\twith\tthe\teffect\ton\tbulk\tdensity\tand\tbread\t-\tand\tpasta\t-\tmaking\tproperties\t,\twere\tinvestigated\t.\tThe\tresults\tshowed\tthat\tpirimiphos\t-\tmethyl\tformulated\twith\tEP\tprovided\tbetter\tefficacy\tagainst\tadults\twhen\tcompared\twith\tEC\tformulation\tfor\tO.\tsurinamensis\tand\tT.\tconfusum\t,\tbut\tthere\twas\tno\tdifference\tfor\tR.\tdominica\t.\tProgeny\tproduction\twas\tconsistently\tlower\tin\tgrain\ttreated\twith\tthe\tEP\tformulation\tthan\tin\tgrain\ttreated\twith\tthe\tEC\t.\tTests\tshowed\tthat\tEP\tadhered\tto\tthe\tkernels\tfor\tlonger\ton\thard\twheat\tthan\ton\tmaize\tor\trice\t.\tIn\tmost\tcommodities\t,\tEP\tdid\tnot\talter\tthe\tbulk\tdensity\t.\tFinally\t,\tthe\taddition\tof\tEP\tdid\tnot\taffect\tflour\t-\tand\tbread\t-\tmaking\tproperties\t,\tnor\tthe\tpasta\t-making\tproperties\t.\tThe\tresults\tof\tthe\tpresent\tstudy\tsuggest\tthat\tan\tEP\tcould\tbe\tused\tto\treduce\tthe\tamount\tof\tpirimiphos\t-\tmethyl\tapplied\tto\tgrain\tfor\teffective\tpest\tcontrol\t,\twith\tno\tdetrimental\teffects\ton\tgrain\tquality\t.\t©\t2016\tSociety\tof\tChemical\tIndustry\t.\t\n",
            "Radiofrequency\tablation\tof\tposteroseptal\taccessory\tpathways\tassociated\twith\tcoronary\tsinus\tdiverticula\tPosteroseptal\taccessory\tpathways\tmay\tbe\tassociated\twith\ta\tcoronary\tsinus\t(\tCS\t)\tdiverticulum\t.\tOur\tpurpose\twas\tto\tdescribe\tthe\tclinical\tcharacteristics\t,\tmapping\tand\tablation\tof\tthese\tpathways\t.\tThis\twas\ta\tretrospective\tstudy\tof\tall\tpatients\twho\tunderwent\tablation\tof\tposteroseptal\taccessory\tpathways\tin\ta\tsingle\tcentre\t.\tPatients\twith\ta\tdiverticulum\tof\tthe\tCS\tor\tone\tof\tits\ttributaries\twere\tincluded\tin\tgroup\tI\t,\twhile\tthe\tother\tpatients\tformed\tgroup\tII\t.\tClinical\tpresentation\t,\tablation\tprocedure\tand\toutcome\twere\tcompared\tbetween\tthe\ttwo\tgroups\t.\tA\ttotal\tof\t51\tpatients\twere\tincluded\t,\t16\tin\tgroup\tI\tand\t35\tin\tgroup\tII\t.\tThere\twere\tno\tsignificant\tdifferences\tin\tage\tor\tsex\tdistribution\t.\tAtrial\tfibrillation\t(\tAF\t)\tand\tprevious\tunsuccessful\tablation\twere\tmore\tcommon\tin\tgroup\tI.\tA\tnegative\tdelta\twave\tin\tlead\tII\twas\tthe\tECG\tfinding\twith\tbest\tsensitivity\tand\tspecificity\tfor\tthe\tpresence\tof\ta\tdiverticulum\t.\tA\tpathway\tpotential\twas\tcommon\tat\tthe\tsuccessful\tsite\tin\tgroup\tI\t,\tand\tthe\tinterval\tbetween\tlocal\tventricular\telectrogram\tand\tdelta\twave\tonset\twas\tshorter\t(\t19.5\t±\t8\tvs\t33.1\t±\t7.6\tms\t,\tp\t<\t0.001\t)\t.\tThere\twas\ta\ttrend\ttoward\tlower\tprocedural\tsuccess\trate\tand\thigher\trecurrence\trate\tin\tgroup\tI\t,\talthough\tthis\twas\tnot\tsignificant\t.\tCS\tdiverticula\tshould\tbe\tsuspected\tin\tpatients\twith\tmanifest\tposteroseptal\taccessory\tpathways\twho\thave\ta\tprevious\tfailed\tablation\t,\tdocumented\tAF\tor\ttypical\telectrocardiographic\tsigns\t.\tA\tdiscrete\tpotential\tis\tfrequently\tseen\tat\tthe\tsuccessful\tsite\t,\tbut\tthe\tlocal\tventricular\telectrogram\tis\tnot\tas\tearly\tas\tin\tother\taccessory\tpathways\t.\t\n",
            "Association\tof\tRBP4\tlevels\twith\tincreased\tarterial\tstiffness\tin\tadolescents\twith\tfamily\thistory\tof\ttype\t2\tdiabetes\tThe\taim\tof\tthis\tstudy\twas\tto\texplore\tthe\timpact\tof\tfamily\thistory\tof\ttype\t2\tdiabetes\t(\tFH2D\t)\ton\tarterial\tstiffness\tin\tyoung\tpeople\tand\tits\trelationship\tto\tadipocytokines\t.\tThis\tcase\t-\tcontrol\tstudy\tincluded\t52\tadolescents\t(\tmale\t/\tfemale\t28/24\t)\twith\tFH2D\t(\tFH2D+\t)\tand\t40\tadolescents\t(\tmale\t/\tfemale\t21/19\t)\twithout\tFH2D\t(\tFH2D-\t)\t.\tAnthropometric\tmeasurements\t,\tincluding\theight\t,\tweight\t,\twaist\tcircumference\t(\tWC\t)\t,\tand\tblood\tpressure\t,\twere\tobtained\t.\tBlood\tsamples\twere\tcollected\t,\tfasting\tplasma\tglucose\t(\tFPG\t)\t,\tserum\tlipids\t,\tRetinol\tBinding\tProtein\t4\t(\tRBP4\t)\t,\tC\treactive\tprotein\t(\tCRP\t)\t,\tadiponectin\tand\tvisfatin\twere\texamined\t.\tBrachial\t-\tankle\tpulse\twave\tvelocity\t(\tbaPWV\t)\twas\tused\tto\tevaluate\tarterial\tstiffness\t.\tVisceral\tfat\tarea\t(\tVFA\t)\twas\tmeasured\tby\tcomputerized\ttomography\t.\tCompared\twith\tFH2D-\tgroup\t,\tFH2D+\tgroup\thad\ta\tsignificantly\thigher\toral\tglucose\ttolerance\ttest\t(\tOGTT\t)\t2-hour\tinsulin\t,\tRBP4\tand\tbaPWV\tlevels\t,\ta\tlower\tadiponectin\tand\tglucose\tinfusing\trate\t(\tGIR\t)\t(\tP<0.05\t)\t.\tBaPWV\twas\tpositively\tcorrelated\twith\tage\t,\tsystolic\tblood\tpressure\t(\tSBP\t)\t,\tdiastolic\tblood\tpressure\t(\tDBP\t)\t,\t2-hour\t(\tOGTT\t)\tinsulin\t,\tRBP4\t,\tand\tVFA\t,\tand\tnegatively\tcorrelated\twith\tGIR\tin\tFH2D+\tgroup\t.\tAfter\tmultivariate\tanalysis\t,\tage\t,\tSBP\t,\tRBP4\tand\tVFA\tmaintained\tan\tindependent\tassociation\twith\tbaPWV\tin\tFH2D+\tgroup\t(\tP<0.05\t)\t,\twhile\tonly\tage\t,\tSBP\t,\tand\tVFA\twere\tindependent\tpredictors\tof\tbaPWV\tin\tFH2D-\tgroup\t(\tP<0.05\t)\t.\tThese\tfindings\tled\tto\tthe\tconclusion\tthat\tRBP4\tlevel\twas\tassociated\twith\tincreased\tarterial\tstiffness\tin\tyoung\tsubjects\twith\tfamily\thistory\tof\ttype\t2\tdiabetes\t.\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vzO7GyUJaJj",
        "colab_type": "text"
      },
      "source": [
        "## We need to put the data into the right format (x, y)\n",
        "\n",
        "The input data is in the following format:\n",
        "```\n",
        "<token>\\t<token>\\t<token>....<all_tokens_for_doc_1>\n",
        "<token>\\t<token>\\t<token>....<all_tokens_for_doc_2>\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "We want to put the data into x so that:\n",
        "```\n",
        "x = [[<token>, <token>, <token>, ..., <all_tokens_for_doc_1], \n",
        "     [<token>, <token>, <token>, ..., <all_tokens_for_doc_2], \n",
        "     ...]\n",
        "```\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Usually we would not load the data into memory, but in this case it is small so who cares.\n",
        "- We also lowercase the text\n",
        "- There is no `y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc7gTmXOJ8pP",
        "colab_type": "code",
        "outputId": "a9d69de2-fdf2-44d1-ec9d-208c80c126bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Load the data into x\n",
        "x = [] #? TODO\n",
        "\"\"\"\n",
        "filename = open(\"/content/text_medmentions.txt\")\n",
        "for token in filename:\n",
        "  token = token.lower()\n",
        "  for tkns in [token]:\n",
        "    x.append(token)\n",
        "\n",
        "\"\"\"\n",
        "x = [[token.lower() for token in sentence.split(\"\\t\")] for sentence in open(DATA_PATH + \"text_medmentions.txt\")]\n",
        "  \n",
        "  \n",
        "\n",
        "# Sanity\n",
        "print(x[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nonylphenol', 'diethoxylate', 'inhibits', 'apoptosis', 'induced', 'in', 'pc12', 'cells', 'nonylphenol', 'and', 'short', '-', 'chain', 'nonylphenol', 'ethoxylates', 'such', 'as', 'np2', 'eo', 'are', 'present', 'in', 'aquatic', 'environment', 'as', 'wastewater', 'contaminants', ',', 'and', 'their', 'toxic', 'effects', 'on', 'aquatic', 'species', 'have', 'been', 'reported', '.', 'apoptosis', 'has', 'been', 'shown', 'to', 'be', 'induced', 'by', 'serum', 'deprivation', 'or', 'copper', 'treatment', '.', 'to', 'understand', 'the', 'toxicity', 'of', 'nonylphenol', 'diethoxylate', ',', 'we', 'investigated', 'the', 'effects', 'of', 'np2', 'eo', 'on', 'apoptosis', 'induced', 'by', 'serum', 'deprivation', 'and', 'copper', 'by', 'using', 'pc12', 'cell', 'system', '.', 'nonylphenol', 'diethoxylate', 'itself', 'showed', 'no', 'toxicity', 'and', 'recovered', 'cell', 'viability', 'from', 'apoptosis', '.', 'in', 'addition', ',', 'nonylphenol', 'diethoxylate', 'decreased', 'dna', 'fragmentation', 'caused', 'by', 'apoptosis', 'in', 'pc12', 'cells', '.', 'this', 'phenomenon', 'was', 'confirmed', 'after', 'treating', 'apoptotic', 'pc12', 'cells', 'with', 'nonylphenol', 'diethoxylate', ',', 'whereas', 'the', 'cytochrome', 'c', 'release', 'into', 'the', 'cytosol', 'decreased', 'as', 'compared', 'to', 'that', 'in', 'apoptotic', 'cells', 'not', 'treated', 'with', 'nonylphenol', 'diethoxylate', 's.', 'furthermore', ',', 'bax', 'contents', 'in', 'apoptotic', 'cells', 'were', 'reduced', 'after', 'exposure', 'to', 'nonylphenol', 'diethoxylate', '.', 'thus', ',', 'nonylphenol', 'diethoxylate', 'has', 'the', 'opposite', 'effect', 'on', 'apoptosis', 'in', 'pc12', 'cells', 'compared', 'to', 'nonylphenol', ',', 'which', 'enhances', 'apoptosis', 'induced', 'by', 'serum', 'deprivation', '.', 'the', 'difference', 'in', 'structure', 'of', 'the', 'two', 'compounds', 'is', 'hypothesized', 'to', 'be', 'responsible', 'for', 'this', 'phenomenon', '.', 'these', 'results', 'indicated', 'that', 'nonylphenol', 'diethoxylate', 'has', 'capability', 'to', 'affect', 'cell', 'differentiation', 'and', 'development', 'and', 'has', 'potentially', 'harmful', 'effect', 'on', 'organisms', 'because', 'of', 'its', 'unexpected', 'impact', 'on', 'apoptosis', '.', '©', '2015', 'wiley', 'periodicals', ',', 'inc.', 'environ', 'toxicol', '31', ':', '1389', '-', '1398', ',', '2016', '.', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcmUm6BKEqs",
        "colab_type": "text"
      },
      "source": [
        "## Download the word embeddings\n",
        "\n",
        "THe links on AWS contain the glove trained model (glove.840B.300d.zip), I've only converted it into keyed_vectors for gensim.\n",
        "\n",
        "\n",
        "---\n",
        "Notes: \n",
        "- Change directory accordingly if working locally or not on Colab\n",
        "- Pretrained word embeddings taken from: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-53gJDJnKNZb",
        "colab_type": "code",
        "outputId": "045026f2-976b-42bf-e38e-cdb419f1fc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat -P /content/\n",
        "!wget https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat.vectors.npy -P /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-05 13:32:01--  https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat\n",
            "Resolving zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)... 52.218.104.19\n",
            "Connecting to zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)|52.218.104.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120497892 (115M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘/content/keyed_vectors_840_300.dat’\n",
            "\n",
            "keyed_vectors_840_3 100%[===================>] 114.92M  93.8MB/s    in 1.2s    \n",
            "\n",
            "2019-11-05 13:32:03 (93.8 MB/s) - ‘/content/keyed_vectors_840_300.dat’ saved [120497892/120497892]\n",
            "\n",
            "--2019-11-05 13:32:04--  https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat.vectors.npy\n",
            "Resolving zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)... 52.218.104.19\n",
            "Connecting to zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)|52.218.104.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2635219328 (2.5G) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘/content/keyed_vectors_840_300.dat.vectors.npy’\n",
            "\n",
            "keyed_vectors_840_3 100%[===================>]   2.45G  95.8MB/s    in 26s     \n",
            "\n",
            "2019-11-05 13:32:31 (95.5 MB/s) - ‘/content/keyed_vectors_840_300.dat.vectors.npy’ saved [2635219328/2635219328]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUmzmxBnKNsi",
        "colab_type": "code",
        "outputId": "e082729e-c665-413a-e2df-8007ad584048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "DATA_PATH = \"/content/\"\n",
        "keyed_vectors = KeyedVectors.load(DATA_PATH + \"keyed_vectors_840_300.dat\") #?\n",
        "\n",
        "# Sanity check\n",
        "#? TODO: Get vectors most similar to the word \"fibrosis\"\n",
        "print(keyed_vectors.most_similar(\"fibrosis\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('cystic', 0.8054730892181396), ('pulmonary', 0.7173466086387634), ('emphysema', 0.699813961982727), ('cirrhosis', 0.6976104974746704), ('lung', 0.6918838620185852), ('Fibrosis', 0.6711395978927612), ('sarcoidosis', 0.6456377506256104), ('bronchiectasis', 0.6421423554420471), ('renal', 0.6387313604354858), ('sclerosis', 0.6375002861022949)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_yj1xAknSD1",
        "colab_type": "text"
      },
      "source": [
        "## Subset the pretrained embeddings to only the ones we need\n",
        "Create a vocabulary based on our current datasete `x`. We use this to subset the full keyed_vectors from glove.\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Usually we would not do this, unles we know that our current dataset contains all words that we are ever going to see in the future. Here we are only doing it to speed things up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meRGSgz1KP7U",
        "colab_type": "code",
        "outputId": "d0bff25c-c992-4353-b25d-45331c489d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO\n",
        "current_vocab = [token for sublist in x for token in sublist]\n",
        "\n",
        "\"\"\"\n",
        "for sublist in x:\n",
        "  for token in sublist:\n",
        "    current_vocab.append(token)\n",
        "    \"\"\"\n",
        "# Convert to set to remove duplicates\n",
        "current_vocab = set(current_vocab)\n",
        "print(len(current_vocab))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyEhqLtmKTYW",
        "colab_type": "text"
      },
      "source": [
        "## From the gensim model take only what we need\n",
        "\n",
        "`embeddings` - a list of vectors, where each row represents the embedding of one word\n",
        "\n",
        "`id2word` - map from index in the embeddings list to words\n",
        "\n",
        "`word2id` - map from word to index in the embeddings list\n",
        "\n",
        "\n",
        "Once done we should be able to get the embedding for word \"house\" like this:\n",
        "`embeddings[word2id['house']]`\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- We only want to have the embeddings for the words in `current_vocab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDUpCsSIKZJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = [] # A list of embeddings for each word in the word2vec vocab\n",
        "\n",
        "# Embeddings is a list, meaning we know that embeddings[1] is a vector for the \n",
        "#word with ID=1, but we don't know what word is that. That is why we need \n",
        "#the id2word and word2id mappings.\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "\n",
        "# Loop over all words in the vocabulary and add the values\n",
        "for word in keyed_vectors.vocab.keys():\n",
        "  # Get only the words that are in our current_vocab\n",
        "  if word in current_vocab:\n",
        "    id2word[len(embeddings)] = word\n",
        "    word2id[word] = len(embeddings)\n",
        "    embeddings.append(keyed_vectors[word])\n",
        "\n",
        "# Add <START> and <END>\n",
        "word = \"<START>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "word = \"<END>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "\n",
        "# Add <UNK> and <PAD>\n",
        "word = \"<UNK>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "word = \"<PAD>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.zeros(len(embeddings[0])))\n",
        "\n",
        "# Convert the embeddings list into a tensor\n",
        "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# Sanity\n",
        "assert len(embeddings) == len(id2word) == len(word2id)\n",
        "assert keyed_vectors['house'][0] == embeddings[word2id['house']][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmrg-g31N6L3",
        "colab_type": "text"
      },
      "source": [
        "## Convert tokens to indices\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- At the beginning of each sample add the index of the `<START>` token\n",
        "- If a word does not exist in our `word2id` map we use the index for `<UNK>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky5V98nOKb0S",
        "colab_type": "code",
        "outputId": "5fea6f73-5429-4461-cb64-13198c4993ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x_ind = []\n",
        "\n",
        "#? TODO\n",
        "i_start = word2id[\"<START>\"]\n",
        "i_unk = word2id[\"<UNK>\"]\n",
        "\n",
        "for sample in x:\n",
        "  new_sample = [i_start]\n",
        "  for token in sample:\n",
        "    # Get index for the token, if not there, get i_unk\n",
        "    i_token = word2id.get(token, i_unk)\n",
        "    # Append to the new sample with indices\n",
        "    new_sample.append(i_token)\n",
        "  x_ind.append(new_sample)\n",
        "\n",
        "\n",
        "print(x_ind[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[34805, 34807, 26, 6, 12185, 5, 3714, 24692, 18750, 2982, 7, 12786, 12058, 24692, 18750, 13, 6305, 12, 2982, 7, 12786, 12058, 13, 11420, 12, 965, 10, 1187, 18, 1646, 197, 15, 794, 8282, 1183, 3, 4376, 3712, 0, 3, 3714, 6305, 2982, 13, 17721, 12, 10, 1187, 18, 2067, 5162, 977, 0, 1803, 618, 5, 5162, 3501, 0, 1241, 945, 5, 19901, 3, 4376, 3712, 1, 22, 210, 27268, 10018, 3, 2532, 12473, 357, 0, 20, 28, 737, 967, 14, 17244, 5, 32679, 120, 13, 34807, 12, 110, 1903, 6305, 2982, 7, 11420, 0, 913, 4, 1646, 6461, 1183, 1, 2, 1172, 5, 25, 582, 28, 4, 4187, 2, 820, 5, 34807, 22578, 7697, 16, 6305, 2982, 6715, 0, 577, 23, 98, 6305, 2982, 3, 3714, 6305, 2982, 6715, 7, 6, 9518, 5, 1423, 11420, 965, 27, 6, 497, 1814, 1, 13125, 2044, 2421, 3, 1080, 10018, 82, 167, 4, 883, 14041, 2873, 11, 34807, 7697, 1, 6, 654, 5, 4099, 1423, 11420, 965, 27, 2, 24312, 1625, 11420, 1814, 29, 79, 745, 0, 41, 5, 86, 2673, 124, 34807, 5962, 9, 3669, 1084, 23, 325, 63, 8282, 2982, 18, 6305, 0, 3, 2777, 965, 5, 86, 76, 17721, 1, 34807, 7697, 82, 2320, 7, 351, 106, 13, 34807, 12, 11420, 965, 18, 6305, 2982, 3, 7, 94, 386, 106, 13, 20991, 12, 11420, 965, 18, 88, 6305, 2982, 1, 5, 2, 965, 18, 17721, 0, 585, 106, 13, 34807, 12, 76, 34807, 22578, 7697, 1229, 464, 106, 13, 33391, 12, 7, 965, 221, 17721, 1, 12439, 0, 2261, 1, 34807, 1978, 4, 24, 48, 2342, 3116, 7, 11420, 965, 18, 17721, 92, 7, 965, 221, 17721, 13, 33997, 1229, 33938, 12, 0, 3, 34807, 22578, 7697, 1978, 4, 24, 48, 3698, 7, 1554, 11420, 965, 18, 17721, 4188, 124, 520, 4403, 8470, 92, 7, 1554, 11420, 965, 221, 17721, 4188, 124, 520, 4403, 8470, 13, 2261, 125, 11814, 12, 1, 67, 4946, 8102, 14, 34807, 22578, 7697, 0, 559, 2261, 1, 34807, 0, 110, 24, 875, 7, 2, 13763, 5, 17721, 7, 1554, 11420, 1, 34807]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2xa7UrKs-E",
        "colab_type": "code",
        "outputId": "b9f61c79-3ce7-4fdb-d7c9-2d87e4d8c86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Sanity: convert the indexes for x_ind[0] back to words\n",
        "#? TODO\n",
        "\" \".join([id2word[id] for id in x_ind[0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<START> <UNK> as a modifier of chronic pseudomonas aeruginosa infection in cystic fibrosis pseudomonas aeruginosa ( pa ) infection in cystic fibrosis ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased rates of exacerbations and shorter survival . by using exome sequencing and extreme phenotype design , it was recently shown that isoforms of dynactin 4 ( <UNK> ) may influence pa infection in cf , leading to worse respiratory disease . the purpose of this study was to investigate the role of <UNK> missense variants on pa infection incidence , age at first pa infection and chronic pa infection incidence in a cohort of adult cf patients from a single centre . polymerase chain reaction and direct sequencing were used to screen dna samples for <UNK> variants . a total of 121 adult cf patients from the cochin hospital cf centre have been included , all of them carrying two <UNK> defects : 103 developed at least 1 pulmonary infection with pa , and 68 patients of them had cpa . <UNK> variants were identified in 24 % ( <UNK> ) cf patients with pa infection and in only 17 % ( 3/18 ) cf patients with no pa infection . of the patients with cpa , 29 % ( <UNK> ) had <UNK> missense variants vs 23 % ( 8/35 ) in patients without cpa . interestingly , p . <UNK> tend to be more frequently observed in cf patients with cpa than in patients without cpa ( 4/68 vs 0/35 ) , and <UNK> missense variants tend to be more frequent in male cf patients with cpa bearing two class ii mutations than in male cf patients without cpa bearing two class ii mutations ( p = 0.06 ) . our observations reinforce that <UNK> missense variants , especially p . <UNK> , may be involved in the pathogenesis of cpa in male cf . <UNK>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-nKnPl_ONZ8",
        "colab_type": "text"
      },
      "source": [
        "## Flatten x_ind\n",
        "\n",
        "Concatentate all samples into one flat list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-mof9EKt_R",
        "colab_type": "code",
        "outputId": "07fa74a6-f9d0-488d-d456-e9d0cef5b70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Flatten x\n",
        "x_ind_flat = [token for sublist in x_ind for token in sublist] #? TODO\n",
        "\n",
        "# sanity \n",
        "print(len(x_ind_flat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1214137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ECXMu5OXQp",
        "colab_type": "text"
      },
      "source": [
        "## Write the batchify function\n",
        "\n",
        "The function takes a flat `x` dataset and batch_size. It should output a matrix where `batch_size` is number of culumns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DowOpusNKzVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchify(x, batch_size):\n",
        "    nbatch = len(x) // batch_size\n",
        "    \n",
        "    # Trim off any extra elements that wouldn't cleanly fit.\n",
        "    x = x[:nbatch * batch_size]\n",
        "    \n",
        "    # Convert x to tensor of type long\n",
        "    x = torch.tensor(x, dtype=torch.long)\n",
        "    \n",
        "    # Evenly divide the data across the batches. Transpose is needed to keep \n",
        "    #the data sequential over rows. \n",
        "    x = x.view(batch_size, -1).t() #? TODO\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxb7G-gVO3EC",
        "colab_type": "text"
      },
      "source": [
        "## Split the dataset into train/test \n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Usually we would have train/dev/test but as this is mainly for demonstration we skip the `dev` set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1r7IFieK1qZ",
        "colab_type": "code",
        "outputId": "52ed4c0d-409b-47d3-a202-02edd887f531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get the proportion for train/test\n",
        "end_train = int(len(x_ind_flat) * 0.9)\n",
        "\n",
        "x_train = batchify(x_ind_flat[:end_train], BATCH_SIZE)\n",
        "x_test = batchify(x_ind_flat[end_train:], BATCH_SIZE)\n",
        "\n",
        "# Sanity\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([17073, 64]) torch.Size([1897, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdOfJprmPHdL",
        "colab_type": "text"
      },
      "source": [
        "## Write the get_batch function\n",
        "\n",
        "It takes:\n",
        "- `source` - that is the training/test set  \n",
        "- `i` - that denotes where we are in the source dataset, which is in fact the batch number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y05hb_EfLIue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(source, i, device):\n",
        "  # Sequence length is either the MAX_SEQ_LEN or smaller if the dataset/current_batch\n",
        "  #does not have sequences of length MAX_SEQ_LEN\n",
        "  seq_len = min(MAX_SEQ_LEN, len(source) - 1 - i)\n",
        "  \n",
        "  # Get the data and targets\n",
        "  data = source[i:i+seq_len]\n",
        "  # Targets are flat\n",
        "  target =  source[i+1:i+seq_len+1] #? TODO\n",
        "  \n",
        "  # Move to device and return\n",
        "  return data.to(device).contiguous(), target.to(device).contiguous().view(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJImhN-IHDrx",
        "colab_type": "code",
        "outputId": "f6f2f2eb-8c6c-4ddb-b599-e81d0125e7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([34809, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGIShlJbStvR",
        "colab_type": "text"
      },
      "source": [
        "## Create the network\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- We need to add the init_hidden function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4zYLiHkLXKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, embeddings, padding_idx):\n",
        "    super(RNN, self).__init__()\n",
        "    # Get the required sizes\n",
        "    self.vocab_size = len(embeddings)\n",
        "    self.embedding_size = len(embeddings[0])\n",
        "    \n",
        "    # Initialize embeddings\n",
        "    self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=padding_idx)\n",
        "    self.embeddings.load_state_dict({'weight': embeddings})\n",
        "\n",
        "    # Disable training for the embeddings - IMPORTANT\n",
        "    self.embeddings.weight.requires_grad = False\n",
        "    \n",
        "    self.num_layers = 2\n",
        "    self.hidden_size = 300\n",
        "    self.dropout = 0.5\n",
        "    \n",
        "\n",
        "\n",
        "    # Create the RNN cell\n",
        "    self.rnn = nn.LSTM(input_size=self.embedding_size, \n",
        "                       hidden_size=self.hidden_size, \n",
        "                       num_layers=self.num_layers, \n",
        "                       dropout=self.dropout)\n",
        "    \n",
        "    # Create the FC layer which is in fact the decoder in this case\n",
        "    self.fc1 = nn.Linear(self.hidden_size, self.vocab_size) #? TODO\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    # Initialize the hidden for the case when we are at batch 0\n",
        "    weight = next(self.parameters())\n",
        "    \n",
        "    return (weight.new_zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "            weight.new_zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # Embed the input: from id -> vec\n",
        "    x = self.embeddings(x) # x.shape = batch_size x sequence_length x emb_size\n",
        "    \n",
        "    # Run 'x' through the RNN, but also provide hidden init\n",
        "    x, hidden = self.rnn(x, hidden) #? TODO\n",
        "\n",
        "    # Push x through the fc network\n",
        "    x = self.fc1(x)\n",
        "    return x, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s713ML7gTdiQ",
        "colab_type": "text"
      },
      "source": [
        "## We need one additional function, detach hidden states\n",
        "\n",
        "As for each batch we provide the hidden states from the previous batch, if we do not detach them pytorch will try to backpropagate the error to the beginning of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km2soTwIQvmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h) # Recursive calls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGOz0kPMUEYf",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate the device, network, criterion and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u758urPuQz1p",
        "colab_type": "code",
        "outputId": "0698bba4-f533-470f-9658-ce60fca33b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "device = torch.device(DEVICE) # Create a torch device\n",
        "net = RNN(embeddings, padding_idx=word2id['<PAD>']) # Create an instance of the RNN, take care what input parameters does it require\n",
        "criterion = nn.CrossEntropyLoss() # Set the criterion to Cross Entropy Loss\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters()) # Get only the parameters that require training\n",
        "optimizer = optim.Adam(parameters, lr=0.001) # Set the optimizer to Adam with lr = 0.001\n",
        "net.to(device) # Move the network to device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embeddings): Embedding(34809, 300, padding_idx=34808)\n",
              "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.5)\n",
              "  (fc1): Linear(in_features=300, out_features=34809, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzCGAGdrQ1sk",
        "colab_type": "code",
        "outputId": "7c9bcc50-38a3-4eb0-d392-bc9865e17939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        }
      },
      "source": [
        "for epoch in range(20):\n",
        "  # Switch network to train mode\n",
        "  net.train()\n",
        "\n",
        "  # Create the running loss array\n",
        "  running_loss = []\n",
        "\n",
        "  # Get the initial values for the hidden layer of RNNs\n",
        "  hidden = net.init_hidden(BATCH_SIZE) #?\n",
        "  for i in range(0, x_train.size(0) - 1, MAX_SEQ_LEN):\n",
        "    x_train_batch, y_train_batch = get_batch(x_train, i, device) #? TODO: get batch\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Get outputs for our batch\n",
        "    outputs, hidden = net(x_train_batch, hidden) #? TODO\n",
        "\n",
        "    # Repackage hidden\n",
        "    hidden = repackage_hidden(hidden)\n",
        "\n",
        "    # Get loss\n",
        "    loss = criterion(outputs.view(-1, len(embeddings)), y_train_batch)\n",
        "    # Do the backward step\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip grads, so we dont have exploding gradients\n",
        "    #? TODO\n",
        "    parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "    torch.nn.utils.clip_grad_norm_(parameters, 0.25)\n",
        "    \n",
        "    # Do the optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the loss to the running_loss\n",
        "    running_loss.append(loss.item())\n",
        "  # For now we only print the train loss - to speed things up, but you would\n",
        "  #usually have the test/dev loss also here.\n",
        "  print(\"LOSS: {}\\n\\n\".format(np.average(running_loss)))\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOSS: 6.531473404072743\n",
            "\n",
            "\n",
            "LOSS: 5.584591087118807\n",
            "\n",
            "\n",
            "LOSS: 5.182388572397989\n",
            "\n",
            "\n",
            "LOSS: 4.9051427402214856\n",
            "\n",
            "\n",
            "LOSS: 4.6919456382746105\n",
            "\n",
            "\n",
            "LOSS: 4.516093103297563\n",
            "\n",
            "\n",
            "LOSS: 4.376480452878662\n",
            "\n",
            "\n",
            "LOSS: 4.260773102665818\n",
            "\n",
            "\n",
            "LOSS: 4.156516642949151\n",
            "\n",
            "\n",
            "LOSS: 4.06878418748176\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-e58e8d4d7e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#? TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Do the optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGREeByUeUR",
        "colab_type": "text"
      },
      "source": [
        "## Let's generate a sample from our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lezVO0OR0x6",
        "colab_type": "code",
        "outputId": "30813fde-5dd2-4f70-c1e2-6c4381a6635f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "hidden = net.init_hidden(1)\n",
        "temp = 1 # The higher the temp the higher the diversity in the output\n",
        "\n",
        "text = \"<START> test\" # Initialise the hidden state\n",
        "inds = [word2id[w] for w in text.split(\" \")]\n",
        "for i in range(len(inds)):\n",
        "  input = torch.tensor([[inds[i]]], dtype=torch.long).to(device)\n",
        "  output, hidden = net(input, hidden)\n",
        "\n",
        "out = \"\"\n",
        "for i in range(100):\n",
        "  # Get word weights from the output\n",
        "  word_weights = output.squeeze().data.div(temp).exp().cpu()\n",
        "  # Sample one word from multinomial\n",
        "  word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "  # Replace existing word with the the new one\n",
        "  input.data.fill_(word_idx)\n",
        "  # Convert index to word\n",
        "  word = id2word[int(word_idx.detach().cpu().numpy())]\n",
        "  # Concat to output\n",
        "  out += word + \" \"\n",
        "  output, hidden = net(input, hidden)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and changes in outcomes in patients assigned to <UNK> cognitions in pune , global symptom of workers who assessed during life care staff th may be a major number of performing available and relative interventions . <UNK> <START> antiproliferative effects of berberine on specific heating from diameter , organic wastes , chloride dehydrogenase is already acquired through respiratory and inflammatory disorders and quantified on relationship , autonomy , developmental patterns , and functional mechanisms ; extremely physiologic conditions do not alter response by movement - emotional symptomatology risks . we possesses a experimental knowledge using the diagnosis of environmental responses \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp7SIc42Ouun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}