{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TODO: Entity Extraction - RNN - Diseases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antsh3k/NN-learning/blob/master/6_Entity_Extraction_RNN_Diseases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnSXu4q-b_XK",
        "colab_type": "text"
      },
      "source": [
        "# Switch to GPU\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ueUgxlTcSZw",
        "colab_type": "text"
      },
      "source": [
        "## Imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkwdBt0EJ7t2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTS (try to organize/group your imports)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKO4vzEnuZvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Any global variables\n",
        "SEED = 15\n",
        "DATA_PATH = '/content/' # Used for Colab\n",
        "MAX_SEQ_LEN = 600\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Set SEEDs\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-1O90Smuuj-",
        "colab_type": "text"
      },
      "source": [
        "## Download the data\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Change directory accordingly if working locally or not on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tonLxa9uxFh",
        "colab_type": "code",
        "outputId": "f6b197aa-68c9-441f-e362-e1d313381393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "# TODO: Download the data from: https://github.com/w-is-h/DeepLearningNLP/raw/master/Session_7/data/diseases_medmentions.txt\n",
        "!wget https://github.com/w-is-h/DeepLearningNLP/raw/master/Session_7/data/diseases_medmentions.txt -P /content/\n",
        "# TODO: Show a couple of lines from the downloaded file\n",
        "!gunzip /content/diseases_medmentions.txt\n",
        "print(\"*\"*200)\n",
        "print(\"Head of the data: \\n\\n\")\n",
        "!head /content/diseases_medmentions.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-22 13:38:54--  https://github.com/w-is-h/DeepLearningNLP/raw/master/Session_7/data/diseases_medmentions.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/w-is-h/DeepLearningNLP/master/Session_7/data/diseases_medmentions.txt [following]\n",
            "--2019-10-22 13:38:54--  https://raw.githubusercontent.com/w-is-h/DeepLearningNLP/master/Session_7/data/diseases_medmentions.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4942710 (4.7M) [text/plain]\n",
            "Saving to: ‘/content/diseases_medmentions.txt’\n",
            "\n",
            "\rdiseases_medmention   0%[                    ]       0  --.-KB/s               \rdiseases_medmention 100%[===================>]   4.71M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-10-22 13:38:54 (40.9 MB/s) - ‘/content/diseases_medmentions.txt’ saved [4942710/4942710]\n",
            "\n",
            "gzip: /content/diseases_medmentions.txt: unknown suffix -- ignored\n",
            "********************************************************************************************************************************************************************************************************\n",
            "Head of the data: \n",
            "\n",
            "\n",
            "DCTN4\t0\t\n",
            "as\t0\t\n",
            "a\t0\t\n",
            "modifier\t0\t\n",
            "of\t0\t\n",
            "chronic\t1\t\n",
            "Pseudomonas\t1\t\n",
            "aeruginosa\t1\t\n",
            "infection\t1\t\n",
            "in\t0\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY4p4Zfxv5Bt",
        "colab_type": "text"
      },
      "source": [
        "## We need to put the data into the right format (x, y)\n",
        "\n",
        "The input data in `diseases_medmentions.txt` has the form:\n",
        "```\n",
        "<token>\\t<label>\n",
        "<token>\\t<label>\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "The documents are separated by a blank line. We need to read the tokens into `x,y` so that one row in `x,y` is one document:\n",
        "```\n",
        "x = [[<token>, <token>, <token>, ...], \n",
        "     [<token>, <token>, <label>, ...], \n",
        "     ...]\n",
        "\n",
        "y = [[<label>, <label>, <label>, ...], \n",
        "     [<label>, <label>, <label>, ...], \n",
        "     ...]\n",
        "``` \n",
        "\n",
        "---\n",
        "Notes:\n",
        "- **We will also lowercase the text**\n",
        "- Usually we would not load the data into memory, but in this case it is small so who cares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rnh88ZKurbD",
        "colab_type": "code",
        "outputId": "bfab6ec1-050d-4be8-a2e5-013f1b8b1d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Load the data\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "# TODO - open the file, read the data and put into 'x' and 'y'\n",
        "#\n",
        "#\n",
        "#\n",
        "x.append([])\n",
        "y.append([])\n",
        "for line in open(DATA_PATH + \"diseases_medmentions.txt\"):\n",
        "  if line.strip():\n",
        "    parts = line.split(\"\\t\")\n",
        "    token = parts[0].lower()\n",
        "    label = int(parts[1])\n",
        "    \n",
        "    x[-1].append(token)\n",
        "    y[-1].append(label)\n",
        "  else:\n",
        "    x.append([])\n",
        "    y.append([])\n",
        "\n",
        "del x[-1]\n",
        "del y[-1]\n",
        "# Random sanity check\n",
        "assert len(x[12]) == len(y[12])\n",
        "\n",
        "# Print an example\n",
        "print(\"Sample at position 0 for both x and y\")\n",
        "print(x[0])\n",
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample at position 0 for both x and y\n",
            "['dctn4', 'as', 'a', 'modifier', 'of', 'chronic', 'pseudomonas', 'aeruginosa', 'infection', 'in', 'cystic', 'fibrosis', 'pseudomonas', 'aeruginosa', '(', 'pa', ')', 'infection', 'in', 'cystic', 'fibrosis', '(', 'cf', ')', 'patients', 'is', 'associated', 'with', 'worse', 'long', '-', 'term', 'pulmonary', 'disease', 'and', 'shorter', 'survival', ',', 'and', 'chronic', 'pa', 'infection', '(', 'cpa', ')', 'is', 'associated', 'with', 'reduced', 'lung', 'function', ',', 'faster', 'rate', 'of', 'lung', 'decline', ',', 'increased', 'rates', 'of', 'exacerbations', 'and', 'shorter', 'survival', '.', 'by', 'using', 'exome', 'sequencing', 'and', 'extreme', 'phenotype', 'design', ',', 'it', 'was', 'recently', 'shown', 'that', 'isoforms', 'of', 'dynactin', '4', '(', 'dctn4', ')', 'may', 'influence', 'pa', 'infection', 'in', 'cf', ',', 'leading', 'to', 'worse', 'respiratory', 'disease', '.', 'the', 'purpose', 'of', 'this', 'study', 'was', 'to', 'investigate', 'the', 'role', 'of', 'dctn4', 'missense', 'variants', 'on', 'pa', 'infection', 'incidence', ',', 'age', 'at', 'first', 'pa', 'infection', 'and', 'chronic', 'pa', 'infection', 'incidence', 'in', 'a', 'cohort', 'of', 'adult', 'cf', 'patients', 'from', 'a', 'single', 'centre', '.', 'polymerase', 'chain', 'reaction', 'and', 'direct', 'sequencing', 'were', 'used', 'to', 'screen', 'dna', 'samples', 'for', 'dctn4', 'variants', '.', 'a', 'total', 'of', '121', 'adult', 'cf', 'patients', 'from', 'the', 'cochin', 'hospital', 'cf', 'centre', 'have', 'been', 'included', ',', 'all', 'of', 'them', 'carrying', 'two', 'cftr', 'defects', ':', '103', 'developed', 'at', 'least', '1', 'pulmonary', 'infection', 'with', 'pa', ',', 'and', '68', 'patients', 'of', 'them', 'had', 'cpa', '.', 'dctn4', 'variants', 'were', 'identified', 'in', '24', '%', '(', '29/121', ')', 'cf', 'patients', 'with', 'pa', 'infection', 'and', 'in', 'only', '17', '%', '(', '3/18', ')', 'cf', 'patients', 'with', 'no', 'pa', 'infection', '.', 'of', 'the', 'patients', 'with', 'cpa', ',', '29', '%', '(', '20/68', ')', 'had', 'dctn4', 'missense', 'variants', 'vs', '23', '%', '(', '8/35', ')', 'in', 'patients', 'without', 'cpa', '.', 'interestingly', ',', 'p', '.', 'tyr263cys', 'tend', 'to', 'be', 'more', 'frequently', 'observed', 'in', 'cf', 'patients', 'with', 'cpa', 'than', 'in', 'patients', 'without', 'cpa', '(', '4/68', 'vs', '0/35', ')', ',', 'and', 'dctn4', 'missense', 'variants', 'tend', 'to', 'be', 'more', 'frequent', 'in', 'male', 'cf', 'patients', 'with', 'cpa', 'bearing', 'two', 'class', 'ii', 'mutations', 'than', 'in', 'male', 'cf', 'patients', 'without', 'cpa', 'bearing', 'two', 'class', 'ii', 'mutations', '(', 'p', '=', '0.06', ')', '.', 'our', 'observations', 'reinforce', 'that', 'dctn4', 'missense', 'variants', ',', 'especially', 'p', '.', 'tyr263cys', ',', 'may', 'be', 'involved', 'in', 'the', 'pathogenesis', 'of', 'cpa', 'in', 'male', 'cf', '.']\n",
            "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sRhemhuxdae",
        "colab_type": "text"
      },
      "source": [
        "## Download the word embeddings\n",
        "\n",
        "The links contain the glove trained model (glove.840B.300d.zip), I've only converted it into keyed_vectors for gensim and saved on AWS to speed up things.\n",
        "\n",
        "\n",
        "---\n",
        "Notes: \n",
        "- Change directory accordingly if working locally or not on Colab\n",
        "- Pretrained word embeddings taken from: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifBSsBA6xLro",
        "colab_type": "code",
        "outputId": "33c31525-d1f5-4eca-954f-66cc70318b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat -P /content/\n",
        "!wget https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat.vectors.npy -P /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-22 14:41:11--  https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat\n",
            "Resolving zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)... 52.218.97.48\n",
            "Connecting to zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)|52.218.97.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120497892 (115M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘/content/keyed_vectors_840_300.dat.1’\n",
            "\n",
            "keyed_vectors_840_3 100%[===================>] 114.92M  29.1MB/s    in 4.3s    \n",
            "\n",
            "2019-10-22 14:41:16 (26.6 MB/s) - ‘/content/keyed_vectors_840_300.dat.1’ saved [120497892/120497892]\n",
            "\n",
            "--2019-10-22 14:41:17--  https://zkcl.s3-eu-west-1.amazonaws.com/keyed_vectors_840_300.dat.vectors.npy\n",
            "Resolving zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)... 52.218.36.67\n",
            "Connecting to zkcl.s3-eu-west-1.amazonaws.com (zkcl.s3-eu-west-1.amazonaws.com)|52.218.36.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2635219328 (2.5G) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘/content/keyed_vectors_840_300.dat.vectors.npy.1’\n",
            "\n",
            "keyed_vectors_840_3 100%[===================>]   2.45G  30.8MB/s    in 83s     \n",
            "\n",
            "2019-10-22 14:42:40 (30.4 MB/s) - ‘/content/keyed_vectors_840_300.dat.vectors.npy.1’ saved [2635219328/2635219328]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHlUWz0B7Wv2",
        "colab_type": "text"
      },
      "source": [
        "## Load the pretrained embeddings into a KeyedVectors Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ0P6wof6eTD",
        "colab_type": "code",
        "outputId": "652a841e-315f-4b8e-94ba-9f0d791aa6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "keyed_vectors = KeyedVectors.load(DATA_PATH + \"keyed_vectors_840_300.dat\")\n",
        "\n",
        "# Sanity check\n",
        "keyed_vectors.most_similar(\"fibrosis\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cystic', 0.8054730892181396),\n",
              " ('pulmonary', 0.7173466086387634),\n",
              " ('emphysema', 0.699813961982727),\n",
              " ('cirrhosis', 0.6976104974746704),\n",
              " ('lung', 0.6918838620185852),\n",
              " ('Fibrosis', 0.6711395978927612),\n",
              " ('sarcoidosis', 0.6456377506256104),\n",
              " ('bronchiectasis', 0.6421423554420471),\n",
              " ('renal', 0.6387313604354858),\n",
              " ('sclerosis', 0.6375002861022949)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaasP0PF7zCZ",
        "colab_type": "text"
      },
      "source": [
        "## Subset the pretrained embeddings to only the ones we need\n",
        "Create a vocabulary based on our current datasete `x`. We use this to subset the full keyed_vectors from glove.\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Usually we would not do this, unles we know that our current dataset contains all words that we are ever going to see in the future. Here we are only doing it to speed things up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvy52BB77wcq",
        "colab_type": "code",
        "outputId": "2c646eae-9c10-4ee2-adb4-28a223f3083d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "current_vocab = []\n",
        "#? TODO\n",
        "for sample in x:\n",
        "  for token in sample:\n",
        "    current_vocab.append(token)\n",
        "\n",
        "# Convert to set to remove duplicates\n",
        "current_vocab = set(current_vocab)\n",
        "print(len(current_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6L8AyxV9Utv",
        "colab_type": "text"
      },
      "source": [
        "## From the gensim model take only what we need\n",
        "\n",
        "`embeddings` - a list of vectors, where each row represents the embedding of one word\n",
        "\n",
        "`id2word` - map from index in the embeddings list to words\n",
        "\n",
        "`word2id` - map from word to index in the embeddings list\n",
        "\n",
        "\n",
        "Once done we should be able to get the embedding for word \"house\" like this:\n",
        "`embeddings[word2id['house']]`\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- We only want to have the embeddings for the words in `current_vocab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_unJ18kO8Y22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = [] # A list of embeddings for each word in the word2vec vocab\n",
        "\n",
        "# Embeddings is a list, meaning we know that embeddings[1] is a vector for the \n",
        "#word with ID=1, but we don't know what word is that. That is why we need \n",
        "#the id2word and word2id mappings.\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "\n",
        "# TODO:???\n",
        "for word in current_vocab:\n",
        "  if word in keyed_vectors.vocab.keys():\n",
        "    id = len(embeddings)  # What is the position of this word in the embeddings list?\n",
        "    id2word[id] = word  # Add mapping from ID to word\n",
        "    word2id[word] = id  # From word 2 id\n",
        "    embeddings.append(keyed_vectors[word])  # Add the embedding for 'word', embeddings are available in the 'model'\n",
        "  \n",
        "# Add <UNK> and <PAD>\n",
        "word = \"<UNK>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "  \n",
        "# For the word '<PAD>', embedding is set to all zeros\n",
        "word = \"<PAD>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.zeros(len(embeddings[0])))\n",
        "\n",
        "\n",
        "# Convert the embeddings list into a tensor\n",
        "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# Sanity\n",
        "assert len(embeddings) == len(id2word) == len(word2id)\n",
        "assert keyed_vectors['house'][0] == embeddings[word2id['house']][0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeA9Lbuo-JkE",
        "colab_type": "text"
      },
      "source": [
        "## Convert x into a list of indices instead of words\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- If a word does not exist in our `word2id` map we use the index for `<UNK>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBrt5hdc98__",
        "colab_type": "code",
        "outputId": "9fea9cb6-b9d6-4ba9-d718-a89840d0a4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "x_ind = [] # TODO\n",
        "for sample in x:\n",
        "  n_sample = []\n",
        "  for word in sample:\n",
        "    if word in word2id:\n",
        "      n_sample.append(word2id[word])\n",
        "    else:\n",
        "      n_sample.append(word2id['<UNK>'])\n",
        "  x_ind.append(n_sample)\n",
        "    \n",
        "    \n",
        "print(x[0])\n",
        "print(x_ind[0])\n",
        "assert len(x_ind[0]) == len(y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dctn4', 'as', 'a', 'modifier', 'of', 'chronic', 'pseudomonas', 'aeruginosa', 'infection', 'in', 'cystic', 'fibrosis', 'pseudomonas', 'aeruginosa', '(', 'pa', ')', 'infection', 'in', 'cystic', 'fibrosis', '(', 'cf', ')', 'patients', 'is', 'associated', 'with', 'worse', 'long', '-', 'term', 'pulmonary', 'disease', 'and', 'shorter', 'survival', ',', 'and', 'chronic', 'pa', 'infection', '(', 'cpa', ')', 'is', 'associated', 'with', 'reduced', 'lung', 'function', ',', 'faster', 'rate', 'of', 'lung', 'decline', ',', 'increased', 'rates', 'of', 'exacerbations', 'and', 'shorter', 'survival', '.', 'by', 'using', 'exome', 'sequencing', 'and', 'extreme', 'phenotype', 'design', ',', 'it', 'was', 'recently', 'shown', 'that', 'isoforms', 'of', 'dynactin', '4', '(', 'dctn4', ')', 'may', 'influence', 'pa', 'infection', 'in', 'cf', ',', 'leading', 'to', 'worse', 'respiratory', 'disease', '.', 'the', 'purpose', 'of', 'this', 'study', 'was', 'to', 'investigate', 'the', 'role', 'of', 'dctn4', 'missense', 'variants', 'on', 'pa', 'infection', 'incidence', ',', 'age', 'at', 'first', 'pa', 'infection', 'and', 'chronic', 'pa', 'infection', 'incidence', 'in', 'a', 'cohort', 'of', 'adult', 'cf', 'patients', 'from', 'a', 'single', 'centre', '.', 'polymerase', 'chain', 'reaction', 'and', 'direct', 'sequencing', 'were', 'used', 'to', 'screen', 'dna', 'samples', 'for', 'dctn4', 'variants', '.', 'a', 'total', 'of', '121', 'adult', 'cf', 'patients', 'from', 'the', 'cochin', 'hospital', 'cf', 'centre', 'have', 'been', 'included', ',', 'all', 'of', 'them', 'carrying', 'two', 'cftr', 'defects', ':', '103', 'developed', 'at', 'least', '1', 'pulmonary', 'infection', 'with', 'pa', ',', 'and', '68', 'patients', 'of', 'them', 'had', 'cpa', '.', 'dctn4', 'variants', 'were', 'identified', 'in', '24', '%', '(', '29/121', ')', 'cf', 'patients', 'with', 'pa', 'infection', 'and', 'in', 'only', '17', '%', '(', '3/18', ')', 'cf', 'patients', 'with', 'no', 'pa', 'infection', '.', 'of', 'the', 'patients', 'with', 'cpa', ',', '29', '%', '(', '20/68', ')', 'had', 'dctn4', 'missense', 'variants', 'vs', '23', '%', '(', '8/35', ')', 'in', 'patients', 'without', 'cpa', '.', 'interestingly', ',', 'p', '.', 'tyr263cys', 'tend', 'to', 'be', 'more', 'frequently', 'observed', 'in', 'cf', 'patients', 'with', 'cpa', 'than', 'in', 'patients', 'without', 'cpa', '(', '4/68', 'vs', '0/35', ')', ',', 'and', 'dctn4', 'missense', 'variants', 'tend', 'to', 'be', 'more', 'frequent', 'in', 'male', 'cf', 'patients', 'with', 'cpa', 'bearing', 'two', 'class', 'ii', 'mutations', 'than', 'in', 'male', 'cf', 'patients', 'without', 'cpa', 'bearing', 'two', 'class', 'ii', 'mutations', '(', 'p', '=', '0.06', ')', '.', 'our', 'observations', 'reinforce', 'that', 'dctn4', 'missense', 'variants', ',', 'especially', 'p', '.', 'tyr263cys', ',', 'may', 'be', 'involved', 'in', 'the', 'pathogenesis', 'of', 'cpa', 'in', 'male', 'cf', '.']\n",
            "[22764, 14452, 1022, 15640, 17608, 16359, 6086, 6618, 1106, 10592, 7555, 11823, 6086, 6618, 16757, 1100, 399, 1106, 10592, 7555, 11823, 16757, 11032, 399, 1388, 20967, 11989, 1244, 105, 11034, 6881, 9786, 21194, 17627, 639, 11511, 16706, 20678, 639, 16359, 1100, 1106, 16757, 10290, 399, 20967, 11989, 1244, 20043, 16391, 22533, 20678, 22189, 1832, 17608, 16391, 8801, 20678, 18883, 14367, 17608, 757, 639, 11511, 16706, 296, 8560, 17293, 11596, 13580, 639, 8245, 14329, 4332, 20678, 16011, 14129, 3193, 21057, 10905, 5402, 17608, 14833, 20395, 16757, 22764, 399, 11198, 10466, 1100, 1106, 10592, 11032, 20678, 18288, 12266, 105, 21511, 17627, 296, 11114, 20916, 17608, 2804, 16904, 14129, 12266, 15113, 11114, 6703, 17608, 22764, 16711, 286, 20693, 1100, 1106, 20581, 20678, 657, 7975, 22280, 1100, 1106, 639, 16359, 1100, 1106, 20581, 10592, 1022, 16859, 17608, 8122, 11032, 1388, 18553, 1022, 10028, 986, 296, 20996, 6676, 2753, 639, 9670, 13580, 4676, 1801, 12266, 3952, 8381, 6090, 6862, 22764, 286, 296, 1022, 3235, 17608, 7014, 8122, 11032, 1388, 18553, 11114, 11651, 21764, 11032, 986, 472, 1602, 15359, 20678, 16267, 17608, 9969, 7998, 3622, 22764, 1185, 12694, 13323, 6931, 7975, 5042, 9748, 21194, 1106, 1244, 1100, 20678, 639, 7120, 1388, 17608, 9969, 8017, 10290, 296, 22764, 286, 4676, 19518, 10592, 16707, 14852, 16757, 22764, 399, 11032, 1388, 1244, 1100, 1106, 639, 10592, 5929, 20257, 14852, 16757, 15726, 399, 11032, 1388, 1244, 15084, 1100, 1106, 296, 17608, 11114, 1388, 1244, 10290, 20678, 3368, 14852, 16757, 22764, 399, 8017, 22764, 16711, 286, 17927, 13059, 14852, 16757, 1267, 399, 10592, 1388, 22529, 10290, 296, 11312, 20678, 8065, 296, 22764, 15500, 12266, 6876, 12237, 3156, 22441, 10592, 11032, 1388, 1244, 10290, 21390, 10592, 1388, 22529, 10290, 16757, 6544, 17927, 12717, 399, 20678, 639, 22764, 16711, 286, 15500, 12266, 6876, 12237, 6553, 10592, 18459, 11032, 1388, 1244, 10290, 3186, 3622, 21630, 18096, 6241, 21390, 10592, 18459, 11032, 1388, 22529, 10290, 3186, 3622, 21630, 18096, 6241, 16757, 8065, 3806, 14183, 399, 296, 21468, 16988, 18242, 10905, 22764, 16711, 286, 20678, 12569, 8065, 296, 22764, 20678, 11198, 6876, 4051, 10592, 11114, 16670, 17608, 10290, 10592, 18459, 11032, 296]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJCVJMot-SkT",
        "colab_type": "code",
        "outputId": "3ded4e31-65a8-4720-84ae-447217db3b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Sanity Check: convert the indicies for x_ind[0] back to words\n",
        "\" \".join([id2word[i] for i in x_ind[0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK> as a modifier of chronic pseudomonas aeruginosa infection in cystic fibrosis pseudomonas aeruginosa ( pa ) infection in cystic fibrosis ( cf ) patients is associated with worse long - term pulmonary disease and shorter survival , and chronic pa infection ( cpa ) is associated with reduced lung function , faster rate of lung decline , increased rates of exacerbations and shorter survival . by using exome sequencing and extreme phenotype design , it was recently shown that isoforms of dynactin 4 ( <UNK> ) may influence pa infection in cf , leading to worse respiratory disease . the purpose of this study was to investigate the role of <UNK> missense variants on pa infection incidence , age at first pa infection and chronic pa infection incidence in a cohort of adult cf patients from a single centre . polymerase chain reaction and direct sequencing were used to screen dna samples for <UNK> variants . a total of 121 adult cf patients from the cochin hospital cf centre have been included , all of them carrying two <UNK> defects : 103 developed at least 1 pulmonary infection with pa , and 68 patients of them had cpa . <UNK> variants were identified in 24 % ( <UNK> ) cf patients with pa infection and in only 17 % ( 3/18 ) cf patients with no pa infection . of the patients with cpa , 29 % ( <UNK> ) had <UNK> missense variants vs 23 % ( 8/35 ) in patients without cpa . interestingly , p . <UNK> tend to be more frequently observed in cf patients with cpa than in patients without cpa ( 4/68 vs 0/35 ) , and <UNK> missense variants tend to be more frequent in male cf patients with cpa bearing two class ii mutations than in male cf patients without cpa bearing two class ii mutations ( p = 0.06 ) . our observations reinforce that <UNK> missense variants , especially p . <UNK> , may be involved in the pathogenesis of cpa in male cf .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_hcFpBAgqn",
        "colab_type": "text"
      },
      "source": [
        "## Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iEHoNW0-zR5",
        "colab_type": "code",
        "outputId": "ddf7835b-0fa9-4615-96e0-e96eb0d8a8d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "doc_lengths = [len(doc) for doc in x_ind] # Document lengths\n",
        "pos = np.sum(np.sum(y)) # Number of positive examples\n",
        "neg = np.sum([len(one) for one in y]) - pos # Number of negative examples \n",
        "avg = np.average(doc_lengths) # Average doc length\n",
        "md = np.median(doc_lengths) # Median doc length\n",
        "mx = np.max(doc_lengths) # Maximum doc length\n",
        "mi = np.min(doc_lengths) # Minimum doc length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(pos))\n",
        "print(\"Number of negative examples: {}\".format(neg))\n",
        "print(\"Average length of the doc:   {:.2f}\".format(avg))\n",
        "print(\"Median length of the doc:    {}\".format(md))\n",
        "print(\"Max length of the doc:       {}\".format(mx))\n",
        "print(\"Min length of the doc:       {}\".format(mi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 19780\n",
            "Number of negative examples: 535604\n",
            "Average length of the doc:   288.51\n",
            "Median length of the doc:    294.0\n",
            "Max length of the doc:       833\n",
            "Min length of the doc:       29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Q5Mkh3AbB7",
        "colab_type": "text"
      },
      "source": [
        "## Trim sentences, create masks and add padding\n",
        "\n",
        "We are doing three things in this step:\n",
        "1. Limiting the size of documents to `MAX_SEQ_LEN`\n",
        "2. Adding padding to short documents to match `MAX_SEQ_LEN`\n",
        "3. Creating a mask for each input document to have `1` where a real word from the document is and `0` where padding is.\n",
        "\n",
        "An example (here I will show 'words' but normally we work with indices):\n",
        "```\n",
        "MAX_SEQ_LEN = 5\n",
        "\n",
        "doc = ['i', 'was', 'running']\n",
        "y = [0, 0, 1]\n",
        "\n",
        "doc_pad = ['i', 'was', 'running', '<PAD>', '<PAD>']\n",
        "mask = [1, 1, 1, 0, 0]\n",
        "# We have to add padding to y, which is 0s\n",
        "y = [0, 0, 1, 0, 0]\n",
        "```\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- Here we can not have documents of length 0, usually we would remove/discard them beforhand. In our case I have done that in the data preparation phase.\n",
        "- Whatever change to the shape of `x` is done, the same must be done to `y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6UNUR6EE0yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add padding and limit to MAX_SEQ_LEN\n",
        "x_ind_pad = [(sample + [word2id['<PAD>']] * max(0, MAX_SEQ_LEN - len(sample)))[0:MAX_SEQ_LEN] for sample in x_ind]\n",
        "y_pad = [(sample + [0] * max(0, MAX_SEQ_LEN - len(sample)))[0:MAX_SEQ_LEN] for sample in y] #? TODO\n",
        "\n",
        "# Sanity\n",
        "assert x_ind_pad[-1][0] == x_ind[-1][0]\n",
        "assert np.average([len(s) for s in x_ind_pad]) == MAX_SEQ_LEN\n",
        "assert np.average([len(s) for s in y_pad]) == MAX_SEQ_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZX6naD4Q9-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a mask for each document in 'x'\n",
        "masks = [[1]*min(MAX_SEQ_LEN, doc_len)+[0]*max(0, MAX_SEQ_LEN - doc_len) for doc_len in doc_lengths]\n",
        "\n",
        "# Sanity\n",
        "assert np.sum(masks[-1]) == doc_lengths[-1]\n",
        "assert np.average([len(mask) for mask in masks]) == MAX_SEQ_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5iYh-zuHYDl",
        "colab_type": "text"
      },
      "source": [
        "## Split into train/test and convert to tensors\n",
        "\n",
        "---\n",
        "Notes:\n",
        "- We are still not moving to 'device'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83qBPueVBEMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test, masks_train, masks_test = train_test_split(x_ind_pad, y_pad, masks, test_size=0.2, random_state=SEED)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "masks_train = torch.tensor(masks_train, dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "masks_test = torch.tensor(masks_test, dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5S5Fbl8UQD8",
        "colab_type": "text"
      },
      "source": [
        "## Make a function that creates batches\n",
        "\n",
        "To create a batch we need to know:\n",
        "\n",
        "`ind` - index of this batch in the whole data\n",
        "\n",
        "`batch_size` - size of the batch\n",
        "\n",
        "`x, y` - the data and labels/targets\n",
        "\n",
        "`masks` - for choosing the right masks for the new batch\n",
        "\n",
        "`device` - for moving the batch onto the right device\n",
        "\n",
        "\n",
        "An example (only for masks, same is for everything else):\n",
        "```\n",
        "masks = [[1, 1, 1, 1, 0],\n",
        "         [1, 1, 1, 0, 0],\n",
        "         [1, 1, 1, 1, 1]]\n",
        "\n",
        "batch_size = 2\n",
        "ind = 0\n",
        "\n",
        "# Calculate given the forumals in the get_batch function\n",
        "start = 0\n",
        "end = 2\n",
        "\n",
        "# Now we can see that for the first batch we can reduce the MAX_SEQ_LEN to 4 instead of 5 as the last column is '0' for all the selected rows.\n",
        "mask_batch = [[1, 1, 1, 1],\n",
        "              [1, 1, 1, 0]]\n",
        "```\n",
        "---\n",
        "Notes:\n",
        "- When working with pytorch batches can be of different shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7cdjqczHK0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(ind, batch_size, x, y, masks, device):\n",
        "  # Get the start/end index for this batch\n",
        "  start = ind * batch_size\n",
        "  end = (ind+1) * batch_size\n",
        "    \n",
        "  # Get the batch\n",
        "  x_batch = x[start:end]\n",
        "  y_batch = y[start:end] #? TODO\n",
        "  mask_batch = masks[start:end] #? TODO\n",
        "  \n",
        "  # Get the longest sequence\n",
        "  max_len = max(mask_batch.sum(1).int()) #? TODO:\n",
        "  \n",
        "  # Cut off the unnecessary part\n",
        "  x_batch = x_batch[:, 0:max_len]\n",
        "  y_batch = y_batch[:, 0:max_len] #? TODO\n",
        "  mask_batch = mask_batch[:, 0:max_len] #? TODO\n",
        "  \n",
        "  # Return and move the batches to the right device\n",
        "  return x_batch.to(device), y_batch.to(device), mask_batch.to(device) #? TODO: order is x, y, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1C2cojdVvtc",
        "colab_type": "text"
      },
      "source": [
        "## Create the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC-KnILOHpoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, embeddings, padding_idx):\n",
        "    super(RNN, self).__init__()\n",
        "    # Get the required sizes\n",
        "    vocab_size = len(embeddings)\n",
        "    embedding_size = len(embeddings[0])\n",
        "    \n",
        "    # Create embeddings\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx) #?\n",
        "    # Load existing weights\n",
        "    self.embeddings.load_state_dict({'weight': embeddings}) #?\n",
        "    # Disable training for the embeddings - IMPORTANT\n",
        "    self.embeddings.weight.requires_grad = False #?\n",
        "    \n",
        " \n",
        "    hidden_size = 300\n",
        "    bid = True # Is the network bidirectional\n",
        "\n",
        "    # Create the RNN cell - devide \n",
        "    self.rnn = nn.LSTM(input_size=300, \n",
        "                       hidden_size=hidden_size // (2 if bid else 1), \n",
        "                       num_layers=2, \n",
        "                       dropout=0.5, \n",
        "                       bidirectional=bid)\n",
        "    self.fc1 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # Embed the input: from id -> vec\n",
        "    x = self.embeddings(x) # x.shape = batch_size x sequence_length x emb_size\n",
        "\n",
        "    # Tell RNN to ignore padding and set the batch_first to True\n",
        "    x = nn.utils.rnn.pack_padded_sequence(x, mask.sum(1).int(), batch_first=True,\n",
        "                                          enforce_sorted=False)  #? TODO\n",
        "\n",
        "    # Run 'x' through the RNN\n",
        "    x, hidden = self.rnn(x) #? TODO\n",
        "\n",
        "    # Add the padding again\n",
        "    x, hidden = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True) #? TODO\n",
        "\n",
        "    # Push x through the fc network\n",
        "    x = self.fc1(x) #? TODO\n",
        "\n",
        "    # Multiply by mask to ignore places where we have padding - IMPORTANT\n",
        "    x *= mask.unsqueeze(2)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHDrFXv6WdBW",
        "colab_type": "text"
      },
      "source": [
        "## Create the device, network, criterion and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2YaNTEYHx6l",
        "colab_type": "code",
        "outputId": "aaf8c91d-929e-4975-df1e-97a5748d8436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "device = torch.device(DEVICE) # Create a torch device\n",
        "net = RNN(embeddings, padding_idx=word2id['<PAD>']) # Create an instance of the RNN, take care what input parameters does it require\n",
        "criterion = nn.CrossEntropyLoss() # Set the criterion to Cross Entropy Loss\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters()) # Get only the parameters that require training\n",
        "optimizer = optim.Adam(parameters, lr=0.001) # Set the optimizer to Adam with lr = 0.001\n",
        "net.to(device) # Move the network to device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embeddings): Embedding(22766, 300, padding_idx=22765)\n",
              "  (rnn): LSTM(300, 150, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc1): Linear(in_features=300, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQYX9ILPH3wW",
        "colab_type": "code",
        "outputId": "99fba1b3-d32c-485c-e1c1-d8d99315f0ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 40\n",
        "# Calculate the number of batches given training size len(x_train)\n",
        "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
        "for epoch in range(50):\n",
        "  # Switch network to train mode\n",
        "  net.train()\n",
        "\n",
        "  # Create the running loss array\n",
        "  running_loss = []\n",
        "  for i in range(num_batches):\n",
        "    x_train_batch, y_train_batch, mask_train_batch = get_batch(ind=i, \n",
        "                                                               batch_size=batch_size, \n",
        "                                                               x=x_train, \n",
        "                                                               y=y_train, \n",
        "                                                               masks=masks_train, \n",
        "                                                               device=device)\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad() #? TODO\n",
        "    # Get outputs for our batch\n",
        "    outputs = net(x_train_batch, mask_train_batch) #? TODO\n",
        "    # Get loss\n",
        "    loss = criterion(outputs.view(-1,2), y_train_batch.view(-1))#? TODO\n",
        "    # Do the backward step\n",
        "    loss.backward() #? TODO\n",
        "    \n",
        "    # Clip grads\n",
        "    parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "    torch.nn.utils.clip_grad_norm_(parameters, 0.25)\n",
        "    \n",
        "    # Do the optimizer step\n",
        "    optimizer.step() #? TODO\n",
        "\n",
        "    # Add the loss to the running_loss\n",
        "    running_loss.append(loss.item()) #? TODO\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    net.eval()\n",
        "    x_test_batch, y_test_batch, masks_test_batch = get_batch(ind=0, \n",
        "                                                              batch_size=len(x_test), \n",
        "                                                              x=x_test, \n",
        "                                                              y=y_test, \n",
        "                                                              masks=masks_test, \n",
        "                                                              device=device)\n",
        "    outputs_test = net(x_test_batch, masks_test_batch)\n",
        "    outputs_test = outputs_test.view(-1, 2)\n",
        "\n",
        "    train_loss = np.average(running_loss) #? TODO\n",
        "    test_loss = criterion(outputs_test, y_test_batch.view(-1)).item() #? TODO\n",
        "\n",
        "    print(\"Train Loss: {:5}\\nTest Loss:  {:5}\\n\\n\".format(train_loss, test_loss))\n",
        "    print(sklearn.metrics.classification_report(y_test_batch.view(-1).cpu().numpy(), \n",
        "                                                torch.max(outputs_test, 1)[1].cpu().detach().numpy()))    \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 0.38484168434754396\n",
            "Test Loss:  0.3705970048904419\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    202549\n",
            "           1       0.73      0.24      0.36      3811\n",
            "\n",
            "    accuracy                           0.98    206360\n",
            "   macro avg       0.86      0.62      0.68    206360\n",
            "weighted avg       0.98      0.98      0.98    206360\n",
            "\n",
            "Train Loss: 0.31017564886655563\n",
            "Test Loss:  0.35322946310043335\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    202549\n",
            "           1       0.74      0.68      0.71      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.87      0.84      0.85    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.3008638471364975\n",
            "Test Loss:  0.35312479734420776\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99    202549\n",
            "           1       0.72      0.76      0.74      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.86      0.88      0.87    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.29466438599121875\n",
            "Test Loss:  0.3573925793170929\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    202549\n",
            "           1       0.77      0.70      0.73      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.88      0.85      0.86    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.2878618977772884\n",
            "Test Loss:  0.3660046458244324\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    202549\n",
            "           1       0.75      0.74      0.74      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.87      0.87      0.87    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.28927898445190525\n",
            "Test Loss:  0.37415811419487\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    202549\n",
            "           1       0.86      0.53      0.66      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.92      0.77      0.83    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.28353446339949584\n",
            "Test Loss:  0.38375982642173767\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    202549\n",
            "           1       0.81      0.65      0.72      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.90      0.82      0.86    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.28318202877656007\n",
            "Test Loss:  0.3890112042427063\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    202549\n",
            "           1       0.81      0.64      0.72      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.90      0.82      0.86    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n",
            "Train Loss: 0.28282142602480376\n",
            "Test Loss:  0.39238494634628296\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99    202549\n",
            "           1       0.70      0.77      0.74      3811\n",
            "\n",
            "    accuracy                           0.99    206360\n",
            "   macro avg       0.85      0.88      0.86    206360\n",
            "weighted avg       0.99      0.99      0.99    206360\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-734b9363bb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#? TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Get outputs for our batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_train_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#? TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#? TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-87397c8b2b9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Add the padding again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#? TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Push x through the fc network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsorted_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mbatch_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munsorted_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rg16r_kHjEU",
        "colab_type": "text"
      },
      "source": [
        "## Print one example from the test set - Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju0AxxDlIVwO",
        "colab_type": "code",
        "outputId": "731ec31c-218f-48dd-dca6-b555ac29fc74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "net.eval()\n",
        "s_ind = 1 # Sample index\n",
        "predictions = torch.max(torch.softmax(net(x_test_batch, masks_test_batch), dim=2), 2)[1].cpu().detach().numpy()\n",
        "for ind, i in enumerate(x_test[s_ind].detach().cpu().numpy()[0:20]): # 20 tokens\n",
        "  print(id2word[i], predictions[s_ind][ind], y_test_batch[s_ind].detach().cpu().numpy()[ind])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation 0 0\n",
            "of 0 0\n",
            "choroidal 0 0\n",
            "thickness 0 0\n",
            "in 0 0\n",
            "patients 0 0\n",
            "with 0 0\n",
            "pseudoexfoliation 1 1\n",
            "syndrome 1 1\n",
            "and 0 0\n",
            "pseudoexfoliation 1 1\n",
            "glaucoma 1 1\n",
            "purpose 0 0\n",
            ". 0 0\n",
            "to 0 0\n",
            "compare 0 0\n",
            "the 0 0\n",
            "macular 0 0\n",
            "and 0 0\n",
            "peripapillary 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYg_ArYdMygF",
        "colab_type": "text"
      },
      "source": [
        "## Write our own document and do diseases detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJLqvTDIExtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write a document that contains a disease \n",
        "document = \"The patient suffered from a non-epileptic seizure on wednesday he suffers from epilepsy and Creutzfeldt–Jakob disease (CJD)\" #? TODO\n",
        "# Convert to tokens and then indicies - take care of OOV\n",
        "tkns = [token.lower() for token in document.split(\" \")] #? TODO\n",
        "ind_tkns = [word2id.get(tkn, word2id['<UNK>']) for tkn in tkns] #? TODO\n",
        "\n",
        "# Convert to torch tensor - note that it has to be a batch\n",
        "input = torch.tensor([ind_tkns], dtype=torch.long).to(device)\n",
        "# Create the mask\n",
        "masks = torch.tensor([[1] * len(ind_tkns)], dtype=torch.long).to(device)\n",
        "\n",
        "# Evaluate\n",
        "net.eval()\n",
        "predictions = torch.max(torch.softmax(net(input, masks), dim=2), 2)[1].cpu().detach().numpy()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Wl14lEFj5Z",
        "colab_type": "code",
        "outputId": "3a7f54e4-2c5c-4c9c-81f0-4d204485ae6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Print the tokens and predictions\n",
        "for ind, tkn in enumerate(tkns):\n",
        "  print(\"{:10} - {}\".format(tkn, (\"disease\" if predictions[ind] else \"\")))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the        - \n",
            "patient    - \n",
            "suffered   - \n",
            "from       - \n",
            "a          - \n",
            "non-epileptic - \n",
            "seizure    - \n",
            "on         - \n",
            "wednesday  - \n",
            "he         - \n",
            "suffers    - \n",
            "from       - \n",
            "epilepsy   - disease\n",
            "and        - \n",
            "creutzfeldt–jakob - disease\n",
            "disease    - disease\n",
            "(cjd)      - \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jii9Ju-jq40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}