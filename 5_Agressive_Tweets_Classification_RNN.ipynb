{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of TODO: Agressive Tweets Classification RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antsh3k/NN-learning/blob/master/5_Agressive_Tweets_Classification_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sxzowbvjr6dt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "BEfore doing anything **Switch to GPU**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2gMRbn0Zvsm6"
      },
      "source": [
        "# Prepare your environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mWCYuCdSrHao",
        "colab": {}
      },
      "source": [
        "# IMPORTS (try to organize/group your imports)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, \\\n",
        "                            recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pjxy_DO1VVBu",
        "colab": {}
      },
      "source": [
        "# Any global variables\n",
        "SEED = 15\n",
        "DATA_PATH = '/tmp/'\n",
        "MAX_SEQ_LEN = 40\n",
        "nlp = spacy.load('en')\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_FCp4JKKsduN",
        "colab": {}
      },
      "source": [
        "# Set SEEDs\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIgXsF_6vnLs"
      },
      "source": [
        "# Examine and Prepare the Data\n",
        "\n",
        "\n",
        "## In deep learning it is not very often that we load the whole dataset into memory, especially the text portion of datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GT3nWZJtv4bc",
        "colab": {}
      },
      "source": [
        "# Download the data and save into raw_data.txt\n",
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/w-is-h/DeepLearningNLP-Medical/master/Session_5/data/tweets.json\"\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# Save the dataset into a file\n",
        "f_raw_data = open(path.join(DATA_PATH, 'raw_data.txt'), 'wb')\n",
        "f_raw_data.write(response.content)\n",
        "f_raw_data.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4_rWM75Wp7W",
        "outputId": "26d2651b-3944-40e0-b5f5-956adca3466a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To checkout files we now use bash commands\n",
        "!head /content/raw_data.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head: cannot open '/content/raw_data.txt' for reading: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5WXv-mY0A5M",
        "colab": {}
      },
      "source": [
        "f_x_raw = open(path.join(DATA_PATH, 'x_raw.txt'), 'w')\n",
        "# We load labels because they are small\n",
        "y = []\n",
        "\n",
        "\n",
        "for line in open(path.join(DATA_PATH, 'raw_data.txt'), 'r'):\n",
        "  # Each line is in fact a json document\n",
        "  doc = json.loads(line) \n",
        "\n",
        "  # : Write text to the file and append labels to 'y',\n",
        "  #each row must contain the text of one tweet\n",
        "  f_x_raw.write(\"{}\\n\".format(doc['content']))\n",
        "  y.append(int(doc['annotation']['label'][0]))\n",
        "\n",
        "# Close the file\n",
        "f_x_raw.close()\n",
        "\n",
        "# This is a typical way to add sanity checks to your code, can be very helpful.\n",
        "assert type(y[0]) == int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SO_6oD_xgOHz"
      },
      "source": [
        "### Before cleaning we should analyse the dataset and understand what to remove or keep, but I've already done that so we skip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ax8Lrwvv3yt4",
        "colab": {}
      },
      "source": [
        "# : Cleaning\n",
        "# Every time a character (excluding numbers) is repeated more than 2 times, \n",
        "# reduce to 2 - e.g. \"0000 yesssssssss!!!!!!\" -> \"0000 yess!!\"\n",
        "def clean_text(text):\n",
        "  clean_text = re.sub(r'([^0-9]{1})\\1{2,}', r'\\1\\1', text)\n",
        "  return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qjYEC4kB0s63",
        "outputId": "a73810e1-bbd9-49fd-9361-b034cd266c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test the clean_text function\n",
        "test_text = \"0000 yesssssss!!!!!!\"\n",
        "test_out = clean_text(test_text)\n",
        "print(test_out)\n",
        "\n",
        "real_out = \"0000 yess!!\"\n",
        "assert real_out == test_out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0000 yess!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "avAo39Tb74u-"
      },
      "source": [
        "# Download Word Embeddings\n",
        "\n",
        "It is very rare to train your own embeddings, if your domain is not exteremly specific. Usually we use pretrained embeddings.\n",
        "\n",
        "In this case we are going to use embeddings from GloVe: Global Vectors for Word Representation. They have pretrained vectors for twitter datasets. \n",
        "\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "The downside of doing this is that we can't continue the trainig of the vectors unless they are in the gensim word2vec format. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "More info at: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "syTY4srd-kJh",
        "outputId": "d4f68a50-4456-4134-8823-4820d58ef9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# JUPYTER/COLAB ONLY!!!\n",
        "# Download the data\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-08 13:49:05--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2019-10-08 13:49:05--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2019-10-08 13:49:05--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  9.94MB/s    in 1m 55s  \n",
            "\n",
            "2019-10-08 13:51:00 (12.6 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Er4vmxdi_eLe",
        "outputId": "644f393d-16e5-4acf-b468-3270bd10527a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Load the vectors\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# Convert glove file to word2vec format\n",
        "glove_file = datapath('/content/glove.twitter.27B.200d.txt')\n",
        "tmp_file = get_tmpfile(\"tmp_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "# Load the newly generated file\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PiESc4WsDpBp",
        "outputId": "ed28535f-a92b-43a8-fabc-5d6cda0896c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Sanity - Check similarity \n",
        "model.most_similar(\"house\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('room', 0.799299418926239),\n",
              " ('home', 0.7727779746055603),\n",
              " ('apartment', 0.7143650054931641),\n",
              " ('party', 0.7122235894203186),\n",
              " ('out', 0.6893113255500793),\n",
              " ('my', 0.683701753616333),\n",
              " ('dad', 0.680922269821167),\n",
              " (\"'s\", 0.6800175309181213),\n",
              " ('going', 0.6792358756065369),\n",
              " ('up', 0.6730260848999023)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "av6z5kuuBmXp",
        "colab": {}
      },
      "source": [
        "embeddings = [] # A list of embeddings for each word in the word2vec vocab\n",
        "\n",
        "# Embeddings is a list, meaning we know that embeddings[1] is a vector for the \n",
        "#word with ID=1, but we don't know what word is that. That is why we need \n",
        "#the id2word and word2id mappings.\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "\n",
        "# Loop over all words in the vocabulary and add the values\n",
        "for word in model.vocab.keys():\n",
        "  id2word[len(embeddings)] = word\n",
        "  word2id[word] = len(embeddings)\n",
        "  embeddings.append(model[word])\n",
        "\n",
        "# Add <UNK> and <PAD>\n",
        "word = \"<UNK>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "word = \"<PAD>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.zeros(len(embeddings[0])))\n",
        "\n",
        "# : Convert the embeddings list into a numpy array\n",
        "#embeddings = 0#?\n",
        "\n",
        "# Convert the embeddings list into a tensor\n",
        "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# Sanity\n",
        "assert len(embeddings) == len(id2word) == len(word2id)\n",
        "assert model['house'][0] == embeddings[word2id['house']][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EXbEK9VOFtkG"
      },
      "source": [
        "# Convert words to integers\n",
        "\n",
        "Usually we don't want to keep our input in the string format, it is very time/memory costly to load text all the time. We want to convert text into integers. That is why we have our mapping `word2id`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fumksoMuGDFA",
        "colab": {}
      },
      "source": [
        "x_ind = []\n",
        "for text in open(path.join(DATA_PATH, 'x_raw.txt')):\n",
        "  # : clean text\n",
        "  text = clean_text(text.strip())\n",
        "  # Covnert text to lowercased tokens, skip punct and white-space\n",
        "  tkns = [tkn.lower_ for tkn in nlp.tokenizer(text) if not tkn.is_punct and\n",
        "          len(tkn.lower_.strip()) > 0]\n",
        "  # Convert each token into its id\n",
        "  ind_tkns = [word2id.get(tkn, word2id.get(\"<UNK>\")) for tkn in tkns]\n",
        "  # Append to x_ind\n",
        "  x_ind.append(ind_tkns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XlEwkAh8VTyU",
        "outputId": "78ee98d1-8235-4eba-c1e8-d6cbe45988da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_ind[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[147, 32, 124, 2567, 124, 109, 243, 26, 45, 80773, 1193514, 13, 25700, 70, 55, 408, 22480, 33, 41, 11, 1697, 183, 15927, 273, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gv5eSfp4c_f2",
        "outputId": "eb2d7b6f-c4a3-4771-8868-f5e4695efe9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# : convert the indexes for x_ind[1] back to words\n",
        "\" \".join([id2word[i] for i in x_ind[1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"she is as dirty as they come and that crook <UNK> the dems are so fucking corrupt it 's a joke make republicans look like\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I_tr32vkdH94"
      },
      "source": [
        "# Analyse the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OhKFnOtrhKwc",
        "outputId": "f802fcdd-5d6a-4c37-962a-436ac8c1c4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#calculate all the statistics\n",
        "tweet_lengths = [len(tweet) for tweet in x_ind]\n",
        "pos = np.sum(y) # Number of positive examples\n",
        "neg = len(y) - pos # Number of negative examples \n",
        "avg = np.average(tweet_lengths) # Average tweet length\n",
        "md = np.median(tweet_lengths) # Median tweet length\n",
        "mx = np.max(tweet_lengths) # Maximum tweet length\n",
        "mi = np.min(tweet_lengths) # Minimum tweet length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(neg))\n",
        "print(\"Number of negative examples: {}\".format(pos))\n",
        "print(\"Average length of the tweets: {}\".format(avg))\n",
        "print(\"Median length of the tweets: {}\".format(md))\n",
        "print(\"Max length of the tweets: {}\".format(mx))\n",
        "print(\"Min length of the tweets: {}\".format(mi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 12179\n",
            "Number of negative examples: 7822\n",
            "Average length of the tweets: 13.369781510924454\n",
            "Median length of the tweets: 12.0\n",
            "Max length of the tweets: 363\n",
            "Min length of the tweets: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CthdgmpzUv7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Calculate the tweet lengths, if length > 40 set to 40, if length == 0 set to 1\n",
        "prim_tweet_lens = [] # Append tweet lengths here\n",
        "for tweet in x_ind:\n",
        "  if len(tweet) > 40:\n",
        "    prim_tweet_lens.append(40)\n",
        "  elif len(tweet) == 0:\n",
        "    prim_tweet_lens.append(1)\n",
        "  else:\n",
        "    prim_tweet_lens.append(len(tweet))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-SqG_957EiR"
      },
      "source": [
        "# Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UfAKFrxZkP8d",
        "outputId": "5e6d8219-e5c6-4caf-c3b9-6b9923c87922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Pad everything to the same length, or remove the extra\n",
        "x_ind_pad = []\n",
        "for i in range(len(x_ind)):\n",
        "  tweet = x_ind[i]\n",
        "  tweet = tweet[0:MAX_SEQ_LEN]\n",
        "  if len(tweet) < MAX_SEQ_LEN:\n",
        "    tweet.extend([word2id['<PAD>']] * (MAX_SEQ_LEN - len(tweet)))\n",
        "  x_ind_pad.append(tweet)\n",
        "\n",
        "# Print the stats again\n",
        "tweet_lengths = [len(tweet) for tweet in x_ind_pad]\n",
        "pos = np.sum(y) # Number of positive examples\n",
        "neg = len(y) - pos # Number of negative examples \n",
        "avg = np.average(tweet_lengths) # Average tweet length\n",
        "md = np.median(tweet_lengths) # Median tweet length\n",
        "mx = np.max(tweet_lengths) # Maximum tweet length\n",
        "mi = np.min(tweet_lengths) # Minimum tweet length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(neg))\n",
        "print(\"Number of negative examples: {}\".format(pos))\n",
        "print(\"Average length of the tweets: {}\".format(avg))\n",
        "print(\"Median length of the tweets: {}\".format(md))\n",
        "print(\"Max length of the tweets: {}\".format(mx))\n",
        "print(\"Min length of the tweets: {}\".format(mi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 12179\n",
            "Number of negative examples: 7822\n",
            "Average length of the tweets: 40.0\n",
            "Median length of the tweets: 40.0\n",
            "Max length of the tweets: 40\n",
            "Min length of the tweets: 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T_PU9YGjo-Ff",
        "colab": {}
      },
      "source": [
        "# Split into train/test and move to pytorch \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test, l_train, l_test = train_test_split(x_ind_pad, y, prim_tweet_lens, test_size=0.2, random_state=SEED)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "l_train = torch.tensor(l_train, dtype=torch.long)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "l_test = torch.tensor(l_test, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k9U-7uYTCQnl"
      },
      "source": [
        "# Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtJdSi1Xplft",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, embeddings, padding_idx):\n",
        "    super(RNN, self).__init__()\n",
        "    # Get the required sizes\n",
        "    vocab_size = len(embeddings)\n",
        "    embedding_size = len(embeddings[0])\n",
        "    \n",
        "    # Initialize embeddings\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
        "    self.embeddings.load_state_dict({'weight': embeddings})\n",
        "    # Disable training for the embeddings - IMPORTANT\n",
        "    self.embeddings.weight.requires_grad = False\n",
        "\n",
        "    hidden_size = 300 # the weight dimension\n",
        "\n",
        "    # Create the RNN cell\n",
        "    #self.rnn = nn.LSTM(input_size=200, hidden_size=hidden_size, num_layers=1, dropout=0.5) to make an LSTM\n",
        "    #self.rnn = nn.GRU(input_size=200, hidden_size=hidden_size, num_layers=1, dropout=0.5) to make a GRU\n",
        "    self.rnn = nn.RNN(input_size=200, hidden_size=hidden_size, num_layers=1, dropout=0.5) \n",
        "    self.fc1 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "  def forward(self, x, lns):\n",
        "    # Embed the input: from id -> vec\n",
        "    x = self.embeddings(x) # x.shape = batch_size x sequence_length x emb_size\n",
        "    \n",
        "    # Tell RNN to ignore padding and set the batch_first to True\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lns, batch_first=True, enforce_sorted=False) \n",
        "\n",
        "    # TODO: run 'x' through the RNN\n",
        "    x, hidden = self.rnn(x) #?\n",
        "\n",
        "    # Add the padding again\n",
        "    x, hidden = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "    \n",
        "    # For each example in batch select the value at the length of that sentence\n",
        "    row_indices = torch.arange(0, x.size(0)).long()\n",
        "    x = x[row_indices, lns-1, :]\n",
        "\n",
        "    # TODO: Push x through the fc network\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEzGZJfm-RTF",
        "outputId": "564f5f7e-fbf9-4856-822b-594cecd5ae94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# TODO:\n",
        "device =  torch.device(DEVICE) # Create a torch device\n",
        "net = RNN(embeddings, padding_idx=word2id['<PAD>']) # Create an instance of the RNN, take care what input parameters does it require\n",
        "criterion = nn.CrossEntropyLoss() # Set the criterion to Cross Entropy Loss\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters()) # Get only the parameters that require training\n",
        "optimizer = optim.Adam(parameters, lr=0.001)  # Set the optimizer to Adam with lr = 0.001\n",
        "net.to(device) # Move the network to device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embeddings): Embedding(1193516, 200, padding_idx=1193515)\n",
              "  (rnn): RNN(200, 300, dropout=0.5)\n",
              "  (fc1): Linear(in_features=300, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D23v2YfGCWs3"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PB9j6GSkCFeC",
        "outputId": "5df09944-9e8b-440e-e50e-48b97135d4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "device = torch.device(DEVICE)\n",
        "# Move data to the right device only test, train is in batches\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "l_test = l_test.to(device)\n",
        "\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "# : calculate the number of batches given training size len(x_train)\n",
        "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
        "for epoch in range(80):\n",
        "  # : Switch network to train mode\n",
        "  net.train()\n",
        "\n",
        "  # Create the running loss array\n",
        "  running_loss = []\n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i+1) * batch_size\n",
        "    \n",
        "    # : Get the batch\n",
        "    x_train_batch = x_train[start:end] #?\n",
        "    y_train_batch = y_train[start:end] #?  \n",
        "    l_train_batch = l_train[start:end] #? \n",
        "\n",
        "    # : Move the batches to the right device\n",
        "    x_train_batch = x_train_batch.to(device) #?\n",
        "    y_train_batch = y_train_batch.to(device) #?\n",
        "    l_train_batch = l_train_batch.to(device) #?\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Get outputs for our batch\n",
        "    outputs = net(x_train_batch, l_train_batch)\n",
        "    # Get loss\n",
        "    loss = criterion(outputs, y_train_batch)\n",
        "    # Do the backward step\n",
        "    loss.backward()\n",
        "    # Do the optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the loss to the running_loss\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "      net.eval()\n",
        "      outputs = net(x_train_batch, l_train_batch)\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in torch.max(outputs, 1)[1].cpu().detach().numpy()], y_train_batch.cpu().numpy())\n",
        "      outputs_dev = net(x_test, l_test)\n",
        "      acc_dev = sklearn.metrics.accuracy_score(torch.max(outputs_dev, 1)[1].cpu().detach().numpy(), y_test.cpu().numpy())\n",
        "      f1_dev = f1_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      p_dev = precision_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      r_dev = recall_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f} F1 Dev: {:.3f} p Dev: {:.3f} r Dev: {:.3f}\".format(epoch, np.average(running_loss), acc, acc_dev, f1_dev, p_dev, r_dev))\n",
        "      \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.65219 Acc: 0.657 Acc Dev: 0.667 F1 Dev: 0.569 p Dev: 0.566 r Dev: 0.572\n",
            "Epoch:    5 Loss: 0.49421 Acc: 0.764 Acc Dev: 0.726 F1 Dev: 0.649 p Dev: 0.639 r Dev: 0.660\n",
            "Epoch:   10 Loss: 0.40767 Acc: 0.826 Acc Dev: 0.731 F1 Dev: 0.640 p Dev: 0.659 r Dev: 0.622\n",
            "Epoch:   15 Loss: 0.35099 Acc: 0.858 Acc Dev: 0.756 F1 Dev: 0.707 p Dev: 0.656 r Dev: 0.767\n",
            "Epoch:   20 Loss: 0.29109 Acc: 0.895 Acc Dev: 0.789 F1 Dev: 0.732 p Dev: 0.715 r Dev: 0.750\n",
            "Epoch:   25 Loss: 0.19174 Acc: 0.929 Acc Dev: 0.796 F1 Dev: 0.758 p Dev: 0.696 r Dev: 0.832\n",
            "Epoch:   30 Loss: 0.14220 Acc: 0.953 Acc Dev: 0.838 F1 Dev: 0.808 p Dev: 0.742 r Dev: 0.887\n",
            "Epoch:   35 Loss: 0.08511 Acc: 0.971 Acc Dev: 0.842 F1 Dev: 0.818 p Dev: 0.735 r Dev: 0.921\n",
            "Epoch:   40 Loss: 0.07452 Acc: 0.976 Acc Dev: 0.865 F1 Dev: 0.836 p Dev: 0.785 r Dev: 0.895\n",
            "Epoch:   45 Loss: 0.03410 Acc: 0.988 Acc Dev: 0.884 F1 Dev: 0.861 p Dev: 0.794 r Dev: 0.941\n",
            "Epoch:   50 Loss: 0.02309 Acc: 0.992 Acc Dev: 0.880 F1 Dev: 0.857 p Dev: 0.788 r Dev: 0.940\n",
            "Epoch:   55 Loss: 0.01974 Acc: 0.991 Acc Dev: 0.874 F1 Dev: 0.852 p Dev: 0.775 r Dev: 0.947\n",
            "Epoch:   60 Loss: 0.01836 Acc: 0.991 Acc Dev: 0.873 F1 Dev: 0.851 p Dev: 0.773 r Dev: 0.948\n",
            "Epoch:   65 Loss: 0.01847 Acc: 0.992 Acc Dev: 0.868 F1 Dev: 0.847 p Dev: 0.762 r Dev: 0.952\n",
            "Epoch:   70 Loss: 0.01649 Acc: 0.993 Acc Dev: 0.871 F1 Dev: 0.850 p Dev: 0.768 r Dev: 0.951\n",
            "Epoch:   75 Loss: 0.01587 Acc: 0.994 Acc Dev: 0.865 F1 Dev: 0.844 p Dev: 0.760 r Dev: 0.950\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqOGokjFZ1GT",
        "colab_type": "code",
        "outputId": "5c877314-7a74-45ea-d68c-7e59220a4b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweet = \"This NN class is Class is fabbb\" # Write a tweet\n",
        "print(tweet)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This NN class is Class is fabbb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cleknlh4nDnv",
        "colab_type": "code",
        "outputId": "4a9b24fd-c11f-46f4-a332-6cbd529bc158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Tokenize using the same procedure as used for the dataset\n",
        "\n",
        "tweet = clean_text(tweet.strip())\n",
        "# Covnert text to lowercased tokens, skip punct and white-space\n",
        "tkns = [tkn.lower_ for tkn in nlp.tokenizer(tweet) if not tkn.is_punct and\n",
        "          len(tkn.lower_.strip()) > 0] \n",
        "print(tkns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'nn', 'class', 'is', 'class', 'is', 'fabb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auBF-vsfnEth",
        "colab_type": "code",
        "outputId": "718dead3-b307-414e-8633-e8678d399c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convert tokens to indices \n",
        "# Convert tokens to indices - watch out for unk words\n",
        "inds = [word2id.get(tkn, word2id.get(\"<UNK>\")) for tkn in tkns]\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[53, 222, 726, 32, 726, 32, 368814]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMLbNu4vnXnG",
        "colab_type": "code",
        "outputId": "91cba38d-e71e-4fbb-f592-4f053601fe77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Add padding to match len(inds) == 40\n",
        "\n",
        "inds.extend([word2id['<PAD>']] * (MAX_SEQ_LEN - len(inds)))\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[53, 222, 726, 32, 726, 32, 368814, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49GDegynZ6s",
        "colab_type": "code",
        "outputId": "036eb56c-1bb6-4b96-a04a-7a7f8c0e1d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Move to torch\n",
        "inds = torch.tensor([inds]).to(device)\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[     53,     222,     726,      32,     726,      32,  368814, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLtkfPfenbx2",
        "colab_type": "code",
        "outputId": "5f0c80a8-c94a-4ef0-cfd0-66cf2ca73b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Predict\n",
        "net.eval() # Switch network to eval mode\n",
        "torch.softmax(net(inds, torch.tensor([5])), dim=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9497, 0.0503]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1yQIKqFniDo",
        "colab_type": "code",
        "outputId": "2f05f7ef-96fd-4f71-f478-12aedc9220d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# TODO: Get all predictions for x_test and apply softmax\n",
        "out = torch.softmax(net(x_test, l_test), dim=1)\n",
        "#?\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.1310e-06, 1.0000e+00],\n",
            "        [3.4313e-04, 9.9966e-01],\n",
            "        [9.9994e-01, 5.5441e-05],\n",
            "        ...,\n",
            "        [9.9758e-01, 2.4162e-03],\n",
            "        [1.0032e-04, 9.9990e-01],\n",
            "        [1.1559e-04, 9.9988e-01]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5DDnjNOnokJ",
        "colab_type": "code",
        "outputId": "317a99ec-ca83-4443-f8e6-96dfd077c77f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Find a couple of examples where the Net is sure it is correct\n",
        "out = out.detach().cpu().numpy()\n",
        "#\n",
        "for i in range(200):\n",
        "  pred = np.argmax(out[i])\n",
        "  if pred != y_test[i] and out[i][pred] > 0.9:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "22\n",
            "24\n",
            "33\n",
            "39\n",
            "40\n",
            "50\n",
            "62\n",
            "64\n",
            "69\n",
            "78\n",
            "82\n",
            "115\n",
            "138\n",
            "151\n",
            "155\n",
            "162\n",
            "184\n",
            "189\n",
            "192\n",
            "198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilzwoAkBnrp4",
        "colab_type": "code",
        "outputId": "97b154b3-9771-4752-cb1b-ae9e8d129055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Print one example\n",
        "ind = 189\n",
        "print(y_test[ind])\n",
        "print(out[ind])\n",
        "print(\" \".join([id2word[i] for i in x_test[ind].cpu().detach().numpy() if id2word[i] != '<PAD>']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0, device='cuda:0')\n",
            "[1.3419524e-04 9.9986577e-01]\n",
            "ahahaahahahaha yourr a funny kid\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}