{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TODO: Copy of Agressive Tweets Classification - Simple ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antsh3k/NN-learning/blob/master/4_Agressive_Tweets_Classification_Simple_ANN_and_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxzowbvjr6dt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before doing anything **Switch to GPU**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gMRbn0Zvsm6",
        "colab_type": "text"
      },
      "source": [
        "# Prepare your environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWCYuCdSrHao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTS (try to organize/group your imports)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, \\\n",
        "                            recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjxy_DO1VVBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Any global variables\n",
        "SEED = 15\n",
        "DATA_PATH = '/content/'\n",
        "MAX_SEQ_LEN = 40  # this is to standardise the dataset\n",
        "nlp = spacy.load('en')\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FCp4JKKsduN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set SEEDs - Very important - without this no one will ever be able \n",
        "#to reproduce the results you got.\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIgXsF_6vnLs",
        "colab_type": "text"
      },
      "source": [
        "# Examine and Prepare the Data\n",
        "\n",
        "\n",
        "## In deep learning it is not very often that we load the whole dataset into memory, especially the text portion of datasets.\n",
        "\n",
        "We want to keep our data/text on the disk and load only what is necessary. Normally we create an iterator that reads the sentences one by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT3nWZJtv4bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the data and save into raw_data.txt\n",
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/w-is-h/DeepLearningNLP-Medical/master/Session_5/data/tweets.json\"\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# Save the dataset into a file\n",
        "f_raw_data = open(path.join(DATA_PATH, 'raw_data.txt'), 'wb')\n",
        "f_raw_data.write(response.content)\n",
        "f_raw_data.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4_rWM75Wp7W",
        "colab_type": "code",
        "outputId": "3a414a68-bee4-4fd1-f9b6-043c3f9f2787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# To checkout files we now use bash commands\n",
        "!head /content/raw_data.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"content\": \"Get fucking real dude.\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"She is as dirty as they come  and that crook Rengel  the Dems are so fucking corrupt it's a joke. Make Republicans look like  ...\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"why did you fuck it up. I could do it all day too. Let's do it when you have an hour. Ping me later to sched writing a book here.\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"Dude they dont finish enclosing the fucking showers. I hate half assed jobs. Whats the reasononing behind it? Makes no sense.\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"WTF are you talking about Men? No men thats not a menage  that's just gay.\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"Ill save you the trouble sister. Here comes a big ol fuck France block coming your way here on the twitter.\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"Im dead serious.Real athletes never cheat don't even have the appearance of at his level. Fuck him dude seriously  I think he did\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"...go absolutely insane.hate to be the bearer of bad news..LoL..dont shoot the messenger (cause we all know you bought that pistol\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"Lmao  im watching the same thing ahaha. The gay guy is hilarious! \\\"Dede having a good day and I dont want anyone to mess it up.\\\"\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n",
            "{\"content\": \"LOL  no he said  What do you call a jail cell to a gay guy? Paradise! ahaha.\",\"annotation\":{\"notes\":\"\",\"label\":[\"1\"]},\"extras\":null,\"metadata\":{\"first_done_at\":1527503426000,\"last_updated_at\":1527503426000,\"sec_taken\":0,\"last_updated_by\":\"jI67aE5hwwdh6l16bcfFVnpyREd2\",\"status\":\"done\",\"evaluation\":\"NONE\"}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5WXv-mY0A5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_x_raw = open(path.join(DATA_PATH, 'x_raw.txt'), 'w')\n",
        "# We load labels because they are small\n",
        "y = []\n",
        "\n",
        "\n",
        "for line in open(path.join(DATA_PATH, 'raw_data.txt'), 'rb'):\n",
        "  # Each line is in fact a json document\n",
        "  doc = json.loads(line) \n",
        "\n",
        "  # TODO: Write text to the file and append labels to 'y',\n",
        "  #each row must contain the text of one tweet. Each line in the f_x_raw\n",
        "  #must contain one tweet\n",
        "  text = doc['content']\n",
        "  f_x_raw.write(\"{}\\n\".format(text)) #?\n",
        "  \n",
        "  label = int(doc[\"annotation\"]['label'][0])\n",
        "  y.append(label) #? append the label for this doc/tweet\n",
        "\n",
        "# Close the file\n",
        "f_x_raw.close()\n",
        "\n",
        "# This is a typical way to add sanity checks to your code, can be very helpful.\n",
        "assert type(y[0]) == int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqD0ieamI5hJ",
        "colab_type": "code",
        "outputId": "b6da2616-bef2-4778-fd42-504ec832dcc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!head /content/x_raw.txt # use this bash command to visualise any errors in your code. Is this the right file etc? double checking for errors at each step is good practise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get fucking real dude.\n",
            "She is as dirty as they come  and that crook Rengel  the Dems are so fucking corrupt it's a joke. Make Republicans look like  ...\n",
            "why did you fuck it up. I could do it all day too. Let's do it when you have an hour. Ping me later to sched writing a book here.\n",
            "Dude they dont finish enclosing the fucking showers. I hate half assed jobs. Whats the reasononing behind it? Makes no sense.\n",
            "WTF are you talking about Men? No men thats not a menage  that's just gay.\n",
            "Ill save you the trouble sister. Here comes a big ol fuck France block coming your way here on the twitter.\n",
            "Im dead serious.Real athletes never cheat don't even have the appearance of at his level. Fuck him dude seriously  I think he did\n",
            "...go absolutely insane.hate to be the bearer of bad news..LoL..dont shoot the messenger (cause we all know you bought that pistol\n",
            "Lmao  im watching the same thing ahaha. The gay guy is hilarious! \"Dede having a good day and I dont want anyone to mess it up.\"\n",
            "LOL  no he said  What do you call a jail cell to a gay guy? Paradise! ahaha.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO_6oD_xgOHz",
        "colab_type": "text"
      },
      "source": [
        "### Before cleaning we should analyse the dataset and understand what to remove or keep, but I've already done that so we skip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax8Lrwvv3yt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Cleaning\n",
        "# Every time a character (excluding numbers) is repeated more than 2 times, \n",
        "# reduce to 2 - e.g. \"0000 yesssssssss!!!!!!\" -> \"0000 yess!!\"\n",
        "def clean_text(text):\n",
        "  clean_text = re.sub(r'([^0-9]{1})\\1{2,}', r'\\1\\1', text) #the one without triplets\n",
        "  return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjYEC4kB0s63",
        "colab_type": "code",
        "outputId": "5845fa74-b1c6-4b9f-c78d-bc7e1195ac37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test the clean_text function\n",
        "test_text = \"0000 yesssssss!!!!!!\"\n",
        "test_out = clean_text(test_text)\n",
        "print(test_out)\n",
        "\n",
        "real_out = \"0000 yess!!\"\n",
        "assert real_out == test_out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0000 yess!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avAo39Tb74u-",
        "colab_type": "text"
      },
      "source": [
        "# Download Word Embeddings\n",
        "\n",
        "It is very rare to train your own embeddings, if your domain is not exteremly specific. Usually we use pretrained embeddings.\n",
        "\n",
        "In this case we are going to use embeddings from GloVe: Global Vectors for Word Representation. They have pretrained vectors for twitter datasets. \n",
        "\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "The downside of doing this is that we can't continue the trainig of the vectors unless they are in the gensim word2vec format. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "More info at: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syTY4srd-kJh",
        "colab_type": "code",
        "outputId": "790b852a-378d-4056-afcc-d356de6b5251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# JUPYTER/COLAB ONLY!!!\n",
        "# Download the data\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip  # downloads the file\n",
        "!unzip glove*.zip   # unzips the file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-10 14:49:48--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2019-09-10 14:49:48--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2019-09-10 14:49:49--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip.3’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  35.9MB/s    in 40s     \n",
            "\n",
            "2019-09-10 14:50:29 (35.9 MB/s) - ‘glove.twitter.27B.zip.3’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "replace glove.twitter.27B.25d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  A\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er4vmxdi_eLe",
        "colab_type": "code",
        "outputId": "6fd328e4-80be-456a-b706-47711e70988c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Load the vectors\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# Convert glove file to word2vec format\n",
        "glove_file = datapath('/content/glove.twitter.27B.200d.txt')\n",
        "tmp_file = get_tmpfile(\"tmp_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "# Load the newly generated file\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-b17da1c08705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the newly generated file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiESc4WsDpBp",
        "colab_type": "code",
        "outputId": "a4d4a4d9-2264-4216-b68e-dce9ef766345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Sanity - Check similarity \n",
        "model.most_similar(\"house\") # TODO: print words most similar to the word \"house\"\n",
        "\n",
        "\"\"\" \n",
        "model[\"house\"] # this is the vector representation of the house\n",
        "list(model.wv.vocab.keys()) # this loads all the words\n",
        "len list(model.wv.vocab.keys()) # this load the len of all words\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nmodel[\"house\"] # this is the vector representation of the house\\nlist(model.wv.vocab.keys()) # this loads all the words\\nlen list(model.wv.vocab.keys()) # this load the len of all words\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av6z5kuuBmXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = [] # A list of embeddings for each word in the word2vec vocab\n",
        "\n",
        "# Embeddings is a list, meaning we know that embeddings[1] is a vector for the \n",
        "#word with ID=1, but we don't know what word is that. That is why we need \n",
        "#the id2word and word2id mappings.\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "\n",
        "# TODO: Loop over all words in the vocabulary and add the values\n",
        "for word in model.vocab.keys():\n",
        "  id = len(embeddings) #? What is the position of this word in the embeddings list?\n",
        "  id2word[id] = word # Add mapping from ID to word\n",
        "  word2id[word] = id # From word 2 id\n",
        "  embeddings.append(model[word]) # Add the embedding for 'word', embeddings are available in the 'model'\n",
        "\n",
        "# Add <UNK> (unknown tokens) and <PAD> (padding to help everything to have the same length)\n",
        "word = \"<UNK>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "\n",
        "# TODO - Add the word '<PAD>' and set the embedding to all zeros\n",
        "word = \"<PAD>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.zeros(len(embeddings[0])))\n",
        "\n",
        "# TODO: Convert the embeddings list into a tensor of type float32\n",
        "embeddings = torch.tensor(embeddings, dtype= torch.float32) #?\n",
        "\n",
        "# Sanity\n",
        "assert len(embeddings) == len(id2word) == len(word2id)\n",
        "assert model['house'][0] == embeddings[word2id['house']][0] # if these embeddings are not the same there will be an assertion error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXbEK9VOFtkG",
        "colab_type": "text"
      },
      "source": [
        "# Convert words to integers\n",
        "\n",
        "Usually we don't want to keep our input in the string format, it is very time/memory costly to load text all the time. We want to convert text into integers. That is why we have our mapping `word2id`\n",
        "\n",
        "\n",
        "Example:\n",
        "```\n",
        "our_vocab = ['i', 'house', 'was', 'test', '<UNK>', '<PAD>']\n",
        "text = \"I was running.\"\n",
        "tkns = ['i', 'was', 'running']\n",
        "ind_tkns = [0, 2, 4] \n",
        "```\n",
        "\n",
        "#### Remember we have a word2id mapping that you can use to go from tokens to inds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fumksoMuGDFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_ind = []\n",
        "for text in open(path.join(DATA_PATH, 'x_raw.txt')):\n",
        "  # TODO: clean text\n",
        "  text = clean_text(text) #?\n",
        "  # Covnert text to lowercased tokens, skip punct and white-space\n",
        "  tkns = [tkn.lower_ for tkn in nlp.tokenizer(text) if not tkn.is_punct and len(tkn.lower_.strip()) >0] #?\n",
        "  # Convert each token into its id, take care that words not in the vocabulary\n",
        "  #should have the ID of the '<UNK>' token at that place. \n",
        "  ind_tkns = [word2id[tkn] if tkn in word2id else word2id['<UNK>'] for tkn in tkns] #?\n",
        "  # Append to x_ind\n",
        "  x_ind.append(ind_tkns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlEwkAh8VTyU",
        "colab_type": "code",
        "outputId": "76111cbb-5e62-4967-88ad-5bc144917e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_ind[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[147, 32, 124, 2567, 124, 109, 243, 26, 45, 80773, 1193514, 13, 25700, 70, 55, 408, 22480, 33, 41, 11, 1697, 183, 15927, 273, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFibwmAbwH5x",
        "colab_type": "text"
      },
      "source": [
        "#### When needed we can easily go back to words for an input sentence using the id2word mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv5eSfp4c_f2",
        "colab_type": "code",
        "outputId": "02244fbe-bb6f-4314-a3f6-5ecfa72e5a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: convert the indices for x_ind[1] back to words\n",
        "print(\" \".join([id2word[id] for id in x_ind[1]])) #?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "she is as dirty as they come and that crook <UNK> the dems are so fucking corrupt it 's a joke make republicans look like\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_tr32vkdH94",
        "colab_type": "text"
      },
      "source": [
        "# Analyse the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cen2fcZUbdgx",
        "colab_type": "code",
        "outputId": "678ba4ea-7a2e-4e9e-f6d7-e904824dcab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "### visualise the data first\n",
        "\n",
        "plt.hist(y) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([12179.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,  7822.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhKFnOtrhKwc",
        "colab_type": "code",
        "outputId": "18cfda78-af04-42f5-e6df-8e89bbe8b4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# TODO: For each tweet calculate its length and add to tweet_lengths, plus \n",
        "#calculate all the statistics\n",
        "tweet_lengths = [len(tweet) for tweet in x_ind] #? Array of tweet lengths\n",
        "pos = np.sum(y) # Number of positive examples\n",
        "neg = len(y) - pos # Number of negative examples \n",
        "avg = np.mean(tweet_lengths) # Average tweet length\n",
        "md = np.median(tweet_lengths)# Median tweet length\n",
        "mx = np.max(tweet_lengths)# Maximum tweet length\n",
        "mi = np.min(tweet_lengths) # Minimum tweet length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(pos))\n",
        "print(\"Number of negative examples: {}\".format(neg))\n",
        "print(\"Average length of the tweets: {}\".format(avg))\n",
        "print(\"Median length of the tweets: {}\".format(md))\n",
        "print(\"Max length of the tweets: {}\".format(mx))\n",
        "print(\"Min length of the tweets: {}\".format(mi))\n",
        "\n",
        "prim_tweet_lens = np.array([ln if ln > 0 else 1 for ln in tweet_lengths])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 7822\n",
            "Number of negative examples: 12179\n",
            "Average length of the tweets: 13.369781510924454\n",
            "Median length of the tweets: 12.0\n",
            "Max length of the tweets: 363\n",
            "Min length of the tweets: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-SqG_957EiR",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the dataset\n",
        "\n",
        "We want all the tweets to have the same length. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 4\n",
        "\n",
        "Examples:\n",
        "```\n",
        "pad_ind = word2id['<PAD>']\n",
        "\n",
        "tweet_1 = [1, 32]\n",
        "# Becomes\n",
        "tweet_1 = [1, 32, pad_ind, pad_ind]\n",
        "\n",
        "tweet_2 = [1, 2, 32, 32, 43, 45]\n",
        "# Becomes\n",
        "tweet_2 = [1, 2, 32, 32]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfAKFrxZkP8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad everything to the same length, or remove the extra words\n",
        "x_ind_pad = []\n",
        "for i in range(len(x_ind)):\n",
        "  tweet = x_ind[i]\n",
        "  # Get the pad ID\n",
        "  pad_id = word2id['<PAD>'] #?\n",
        "  \n",
        "  # Remove extra words or add the ID for <PAD> if < MAX_SEQ_LEN\n",
        "  if len(tweet) < MAX_SEQ_LEN:\n",
        "    tweet.extend([pad_id] * (MAX_SEQ_LEN - len(tweet)))#?\n",
        "  elif len(tweet) > MAX_SEQ_LEN:\n",
        "    tweet = tweet[0:MAX_SEQ_LEN]#?\n",
        "  x_ind_pad.append(tweet)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK2TlgfGpbTr",
        "colab_type": "code",
        "outputId": "4b93d48f-1c95-4f8f-da4d-433ba075b231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "tweet_lengths = [len(tweet) for tweet in x_ind_pad]\n",
        "# TODO: Print again the stats from above\n",
        "\n",
        "pos = np.sum(y) # Number of positive examples\n",
        "neg = len(y) - pos # Number of negative examples \n",
        "avg = np.mean(tweet_lengths) # Average tweet length\n",
        "md = np.median(tweet_lengths)# Median tweet length\n",
        "mx = np.max(tweet_lengths)# Maximum tweet length\n",
        "mi = np.min(tweet_lengths) # Minimum tweet length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(pos))\n",
        "print(\"Number of negative examples: {}\".format(neg))\n",
        "print(\"Average length of the tweets: {}\".format(avg))\n",
        "print(\"Median length of the tweets: {}\".format(md))\n",
        "print(\"Max length of the tweets: {}\".format(mx))\n",
        "print(\"Min length of the tweets: {}\".format(mi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 7822\n",
            "Number of negative examples: 12179\n",
            "Average length of the tweets: 40.0\n",
            "Median length of the tweets: 40.0\n",
            "Max length of the tweets: 40\n",
            "Min length of the tweets: 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_PU9YGjo-Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into train/test and move to pytorch \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test, l_train, l_test = train_test_split(x_ind_pad, y, prim_tweet_lens, test_size=0.2, random_state=SEED)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "l_train = torch.tensor(l_train, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# TODO: do the same for the test \n",
        "x_test = torch.tensor(x_test, dtype=torch.long) #?\n",
        "y_test = torch.tensor(y_test, dtype=torch.long) #?\n",
        "l_test = torch.tensor(l_test, dtype=torch.float32).reshape(-1, 1) #?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9U-7uYTCQnl",
        "colab_type": "text"
      },
      "source": [
        "# Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtJdSi1Xplft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, embeddings):\n",
        "    super(Net, self).__init__()\n",
        "    # Get the required sizes\n",
        "    vocab_size = len(embeddings)\n",
        "    embedding_size = len(embeddings[0])\n",
        "    # Initialize embeddings\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "    self.embeddings.load_state_dict({'weight': embeddings})\n",
        "    \n",
        "    # Disable training for the embeddings - IMPORTANT\n",
        "    self.embeddings.weight.requires_grad = False\n",
        "\n",
        "    # TODO add three layers, \n",
        "    # 1) 200 neurons \n",
        "    # 2) 50 neurons \n",
        "    # 3) 2 neurons\n",
        "    self.fc1 = nn.Linear(embedding_size, 200,) #?\n",
        "    self.fc2 = nn.Linear(200, 50) #?\n",
        "    self.fc3 = nn.Linear(50, 2) #?\n",
        "\n",
        "    # TODO: create one dropout layer with p=0.5\n",
        "    self.d1 = nn.Dropout(0.5)#?\n",
        "\n",
        "  def forward(self, x, lns):\n",
        "    # Embed the input: from id -> vec\n",
        "    x = self.embeddings(x)\n",
        "    \n",
        "    # We did not average the words per sentence until now, we'll\n",
        "    #do it here\n",
        "    x = torch.sum(x, dim=1) \n",
        "    x = x / lns\n",
        "\n",
        "    # TODO: run 'x' through the layers, add dropout to the first and second\n",
        "    x = self.d1(torch.relu(self.fc1(x))) #? relu activation + dropout\n",
        "    x = self.d1(torch.relu(self.fc2(x))) #? relu activation + dropout\n",
        "    x = torch.sigmoid(self.fc3(x)) #? sigmoid activation NO dropout\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEzGZJfm-RTF",
        "colab_type": "code",
        "outputId": "f19c09dd-2c92-4708-f577-34137d90b51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Device\n",
        "device = torch.device(DEVICE)\n",
        "# Create the network\n",
        "net = Net(embeddings)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Make a SGD optimizer with lr=0.002 and momentum=0.99\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "print(parameters)\n",
        "optimizer = optim.SGD(parameters, lr=0.01, momentum=0.99)\n",
        "# Move the net to the device\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<filter object at 0x7f9ac2528ba8>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (embeddings): Embedding(1193516, 200)\n",
              "  (fc1): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (fc2): Linear(in_features=200, out_features=50, bias=True)\n",
              "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
              "  (d1): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D23v2YfGCWs3",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB9j6GSkCFeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(DEVICE)\n",
        "# Move data to the right device only test, train is in batches\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "l_test = l_test.to(device)\n",
        "\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "# TODO: calculate the number of batches given training size len(x_train)\n",
        "num_batches = int(len(x_train / batch_size)) #?\n",
        "for epoch in range(200):\n",
        "  # TODO: Switch network to train mode\n",
        "  net.train()\n",
        "\n",
        "  # Create the running loss array\n",
        "  running_loss = []\n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i+1) * batch_size\n",
        "    \n",
        "    # TODO: Get the batch\n",
        "    x_train_batch = x_train[start:end] #?\n",
        "    y_train_batch = y_train[start:end]#?  \n",
        "    l_train_batch = l_train[start:end]#? \n",
        "\n",
        "    # TODO: Move the batches to the right device\n",
        "    x_train_batch = x_train_batch.to(device) #?\n",
        "    y_train_batch = y_train_batch.to(device) #?\n",
        "    l_train_batch = l_train_batch.to(device) #?\n",
        "\n",
        "    # TODO: zero gradients\n",
        "    optimizer.zero_grad() #?\n",
        "    # Get outputs for our batch\n",
        "    outputs = net(x_train_batch, l_train_batch) #?\n",
        "    # Get loss\n",
        "    loss = criterion(outputs, y_train_batch) #?\n",
        "    # Do the backward step\n",
        "    loss.backward() #?\n",
        "    # Do the optimizer step\n",
        "    optimizer.step() #?\n",
        "\n",
        "    # Add the loss to the running_loss\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "      net.eval()\n",
        "      outputs = net(x_train_batch, l_train_batch)\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in torch.max(outputs, 1)[1].cpu().detach().numpy()], y_train_batch.cpu().numpy())\n",
        "\n",
        "      outputs_dev = net(x_test, l_test)\n",
        "      acc_dev = sklearn.metrics.accuracy_score(torch.max(outputs_dev, 1)[1].cpu().detach().numpy(), y_test.cpu().numpy())\n",
        "      f1_dev = f1_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      p_dev = precision_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      r_dev = recall_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f} F1 Dev: {:.3f} p Dev: {:.3f} r Dev: {:.3f}\".format(epoch, np.average(running_loss), acc, acc_dev, f1_dev, p_dev, r_dev))\n",
        "      \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckhYanhOGJIr",
        "colab_type": "text"
      },
      "source": [
        "# ConvNets\n",
        "\n",
        "In pytorch a convolutional layer is defined as:\n",
        "```\n",
        "nn.Conv2d(<number of input chanels>, <number of filters>, kernel_size)\n",
        "```\n",
        "$\\text{number of input chanels}$ - The depth, in our case always 1\n",
        "\n",
        "$\\text{kernel_size}$ - is a touple setting the **height** $x$ **width** of a kernel. In our case the kernel will be of size (n, embedding_size) where 'n' is the number of words or pattern length. \n",
        "\n",
        "\n",
        "### We are going to create three independent conv blocks, stack the output and add a fc layer onto that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9NHJ4bHAVdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, embeddings):\n",
        "    super(Net, self).__init__()\n",
        "    # TODO: Get the required sizes\n",
        "    vocab_size = #?\n",
        "    embedding_size = #?\n",
        "\n",
        "    # Initialize embeddings\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "    # TODO: Load the pretrained weights\n",
        "    self.embeddings.#?\n",
        "\n",
        "    # TODO: Disable training for embeddings\n",
        "    #?\n",
        "\n",
        "    # Set the number of filters\n",
        "    n_filters = 128\n",
        "    # Create 3 different kernel sizes\n",
        "    k1 = #?\n",
        "    k2 = #?\n",
        "    k3 = #?\n",
        "\n",
        "    # TODO: Create three conv layers\n",
        "    self.conv1 = #?\n",
        "    self.conv2 = #?\n",
        "    self.conv3 = #?\n",
        "\n",
        "    # The fully connected\n",
        "    self.fc1 = nn.Linear(3 * n_filters, 2)\n",
        "    # Add some dropout as always\n",
        "    self.d1 = nn.Dropout(0.5)\n",
        "\n",
        "  def conv_block(self, input, conv):\n",
        "    out = conv(input)\n",
        "    out = F.relu(out.squeeze(3))\n",
        "    out = F.max_pool1d(out, out.size()[2]).squeeze(2)\n",
        "    return out\n",
        "\n",
        "  def forward(self, x, lns=0):\n",
        "    # Embed the input: from id -> vec\n",
        "    x = self.embeddings(x) # x.shape = batch_size x sequence_length x emb_size\n",
        "    \n",
        "    # Add a dimension at '1'\n",
        "    x = x.unsqueeze(1) # Because the expected shape = batch_size x channels x sequence_length x emb_size\n",
        "    \n",
        "    # Get the three outputs from conv layers\n",
        "    x1 = self.conv_block(x, self.conv1)\n",
        "    x2 = #? add conv2, all for x as input\n",
        "    x3 = #? add conv3, all for x as input\n",
        "\n",
        "    x_all = torch.cat((x1, x2, x3), 1)\n",
        "    x_all = self.d1(x_all)\n",
        "    logits = self.fc1(x_all)\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laylNHAgCEVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Device\n",
        "device = torch.device(DEVICE)\n",
        "# Create the network and get CE loss\n",
        "net = Net(embeddings)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# We don't want parameters that don't require a grad in the optimizer\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "print(parameters)\n",
        "#TODO: Switch optimizer to Adam and set lr=0.001\n",
        "optimizer = optim.Adam(parameters, lr=0.001)\n",
        "# Move the net to the device\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btkiKjw0CHVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(DEVICE)\n",
        "# Move data to the right device only test, train is in batches\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
        "for epoch in range(200):\n",
        "  net.train()\n",
        "\n",
        "  # Create the running loss array\n",
        "  running_loss = []\n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i+1) * batch_size\n",
        "    \n",
        "    x_train_batch = x_train[start:end] \n",
        "    y_train_batch = y_train[start:end] \n",
        "    l_train_batch = l_train[start:end]\n",
        "\n",
        "    # TODO: Move the batches to the right device\n",
        "    x_train_batch = x_train_batch.to(device)\n",
        "    y_train_batch = y_train_batch.to(device)\n",
        "    l_train_batch = l_train_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(x_train_batch, l_train_batch)\n",
        "    loss = criterion(outputs, y_train_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the loss to the running_loss\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "      net.eval()\n",
        "      outputs = net(x_train_batch, l_train_batch)\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in torch.max(outputs, 1)[1].cpu().detach().numpy()], y_train_batch.cpu().numpy())\n",
        "\n",
        "      outputs_dev = net(x_test, l_test)\n",
        "      acc_dev = sklearn.metrics.accuracy_score(torch.max(outputs_dev, 1)[1].cpu().detach().numpy(), y_test.cpu().numpy())\n",
        "      f1_dev = f1_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      p_dev = precision_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      r_dev = recall_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f} F1 Dev: {:.3f} p Dev: {:.3f} r Dev: {:.3f}\".format(epoch, np.average(running_loss), acc, acc_dev, f1_dev, p_dev, r_dev))\n",
        "      \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8sUYqDAER_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a tweet\n",
        "tweet = \"<your tweet>\"\n",
        "print(tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4fwgt-GJ2KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tkns = # Clean the tokens in the same way you've done earlier\n",
        "print(tkns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB1pCz_aKCfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert tokens to indices \n",
        "inds = #?\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrXegU2fKNOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add padding to match len(inds) == 40\n",
        "inds.extend([word2id['<PAD>']] * (MAX_SEQ_LEN - len(inds)))\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AydPv2LdKdYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move to torch\n",
        "inds = torch.tensor([inds]).to(device)\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN41G8fEK2LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Predict\n",
        "net #? switch to eval mode\n",
        "out = #? \n",
        "out = #? Calculate softmax on the output torch.softmax - don't forget the dimension\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As9wtdjtLzM6",
        "colab_type": "text"
      },
      "source": [
        "# Find where mistakes are made in the test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaHV3C6FOb7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Get all predictions for x_test and apply softmax\n",
        "out = #?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJeJ0Fs7OglU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find a couple of examples where the Net is sure it is correct\n",
        "out = out.detach().cpu().numpy()\n",
        "for i in range(200):\n",
        "  pred = np.argmax(out[i])\n",
        "  if pred != y_test[i] and out[i][pred] > 0.9:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUo4rUkIO_E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the examples\n",
        "ind = 105\n",
        "print(y_test[ind])\n",
        "print(out[ind])\n",
        "print(\" \".join([id2word[i] for i in x_test[ind].cpu().detach().numpy() if id2word[i] != '<PAD>']))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}