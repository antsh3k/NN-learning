{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Understand the model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antsh3k/NN-learning/blob/master/3_Understand_the_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdjVtLYL6sXJ",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_QSfQu7arNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SWITCH TO GPU\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import torch \n",
        "\n",
        "SEED = 15\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4lXNDq6njd",
        "colab_type": "text"
      },
      "source": [
        "# Get the data from github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0g6YMvNcFUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/w-is-h/tmp/master/dataset.csv\", encoding='cp1252')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YALrViGcIQ7",
        "colab_type": "code",
        "outputId": "4f1e1942-9b10-4c25-e429-17b2da6ab5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>first think another Disney movie, might good, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>big fan Stephen King's work, film made even gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment\n",
              "0  first think another Disney movie, might good, ...          1\n",
              "1  Put aside Dr. House repeat missed, Desperate H...          0\n",
              "2  big fan Stephen King's work, film made even gr...          1\n",
              "3  watched horrid thing TV. Needless say one movi...          0\n",
              "4  truly enjoyed film. acting terrific plot. Jeff...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO8wGAHN6uMq",
        "colab_type": "text"
      },
      "source": [
        "# Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqoppNTg6xzu",
        "colab_type": "code",
        "outputId": "f328b827-1856-4ce7-e12d-51c5199c59a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "shape = df.shape\n",
        "cols = list(df.columns) # Must be a list\n",
        "num_pos = np.sum(df['Sentiment'])\n",
        "print(\"The shape of the dataset is: \" + str(shape))\n",
        "print(\"The columns are: \" + str(cols))\n",
        "print(\"Number of positive values: \" + str(num_pos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is: (25000, 2)\n",
            "The columns are: ['SentimentText', 'Sentiment']\n",
            "Number of positive values: 12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YadlC2_cMzc",
        "colab_type": "code",
        "outputId": "b0e6db85-bebc-452d-e791-26e2d814623a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "_ = plt.hist(df['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVcTC417YI8",
        "colab_type": "text"
      },
      "source": [
        "###Get the $x,y$ values from the datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa66fNKWd2lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df['SentimentText'].values\n",
        "y = df['Sentiment'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErAVeYuduCx",
        "colab_type": "text"
      },
      "source": [
        "###Look at x and check do we need to clean anything\n",
        "\n",
        "Cleaning is not always necessary, but the more garbage we leave the more the Neural Network has to learn. So, if the cleaning process is easy - better to do it. Or get a gigantic dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO4wpcINdWMz",
        "colab_type": "code",
        "outputId": "a735a8c8-85e3-4edd-f659-68c22d54aede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(x[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. <br /><br />The characters composed one lame stereo-type another, obvious attempt creating another \"Bad News Bears\" embarrassing say least.<br /><br />I seen prized turkeys time, reason list since \"Numero Uno\".<br /><br />Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I9rNmcu6fpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove mails and https links\n",
        "pat_1 = r\"(?:\\@|https?\\://)\\S+\"\n",
        "# Remove tags\n",
        "pat_2 = r'#\\w+ ?'\n",
        "# Combine into one regex\n",
        "combined_pat = r'|'.join((pat_1, pat_2))\n",
        "# Remove websites\n",
        "www_pat = r'www.[^ ]+'\n",
        "# Remove HTML tags\n",
        "html_tag = r'<[^>]+>'\n",
        "def data_cleaner(text):\n",
        "  cleantags = \"\"\n",
        "  try:\n",
        "    stripped = re.sub(combined_pat, '', text)\n",
        "    stripped = re.sub(www_pat, '', stripped)\n",
        "    cleantags = re.sub(html_tag, '', stripped)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    cleantags = \"None\"\n",
        "  return cleantags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98OEBCta7Os0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# : Run the cleaning function for each sentence in 'x' and save the results into a\n",
        "#new variable x2\n",
        "x2 = []\n",
        "for doc in x:\n",
        "  x2.append(data_cleaner(doc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoQ0ndefe_IF",
        "colab_type": "code",
        "outputId": "dd33efb9-c697-4068-aa64-c897f177d81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x_original = x\n",
        "x = x2\n",
        "print(x[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. The characters composed one lame stereo-type another, obvious attempt creating another \"Bad News Bears\" embarrassing say least.I seen prized turkeys time, reason list since \"Numero Uno\".Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z75yCgk-7wLa",
        "colab_type": "text"
      },
      "source": [
        "# SpaCy\n",
        "\n",
        "We can use spacy to tokenize the text and further clean it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmE8oH7KbonG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.attrs import LOWER\n",
        "# Load the english model for spacy, the disable part is used to make it faster\n",
        "nlp = spacy.load('en', disable=['ner', 'parser'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Jz7Ql5jCKr",
        "colab_type": "text"
      },
      "source": [
        "###The nlp variable is an instance of a class that has the method \\_\\_call__ defined, which means it is a class that is callable\n",
        "\n",
        "It returns an object that is a spacy representation of the input sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccllc0l7zHG",
        "colab_type": "code",
        "outputId": "e8769f63-d255-4567-d6df-bae6e3fed33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Type of the nlp variable is: \" + str(type(nlp)))\n",
        "doc = nlp(\"I was running yesterday.\")\n",
        "print(\"\\nThe 'nlp' function returns: \" + str(type(doc)))\n",
        "print(\"\\nIf we print the 'doc' variable we get the original input sentence: \" + str(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the nlp variable is: <class 'spacy.lang.en.English'>\n",
            "\n",
            "The 'nlp' function returns: <class 'spacy.tokens.doc.Doc'>\n",
            "\n",
            "If we print the 'doc' variable we get the original input sentence: I was running yesterday.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2gPwleAkTXb",
        "colab_type": "text"
      },
      "source": [
        "###To access tokens we can loop over the document - this means the 'doc' class has an \\_\\_iter__ method defined\n",
        "\n",
        "Look [here](https://github.com/explosion/spaCy/blob/9003fd25e5e966bd8d1b67a18f3ebd6010d6f718/spacy/tokens/doc.pyx) for the class definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2shYPjFX75Gq",
        "colab_type": "code",
        "outputId": "774007cd-181f-4377-f235-72f15bf404b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "was\n",
            "running\n",
            "yesterday\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-m0CAgoddy",
        "colab_type": "text"
      },
      "source": [
        "###Note that each token is a spacy object - not a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xicuAf3qoj_K",
        "colab_type": "code",
        "outputId": "4aa34f7e-5f4a-4631-96da-a5641645b02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(doc[0])) # we can also index the doc object and get tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ja7vJ3omkQ",
        "colab_type": "text"
      },
      "source": [
        "###To get strings we use the .text property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9y7IEeoq6W",
        "colab_type": "code",
        "outputId": "7037b195-d800-4b06-f9e7-e6ad332a3087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(doc[0].text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzY4iHv18CVt",
        "colab_type": "code",
        "outputId": "28cac551-fb68-4a25-9f26-1932e5d6ec34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Each token has multiple properties, e.g. lemma_ - note the '_', that means print text\n",
        "for token in doc:\n",
        "  print(token.lemma_)\n",
        "  print(token.is_stop)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PRON-\n",
            "True\n",
            "\n",
            "be\n",
            "True\n",
            "\n",
            "run\n",
            "False\n",
            "\n",
            "yesterday\n",
            "False\n",
            "\n",
            ".\n",
            "False\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXbv-Df64G50",
        "colab_type": "code",
        "outputId": "9de50205-a425-420f-9d01-b4c20176df8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "snt = \"That was a very good movie, John\"\n",
        "# : Tokenize the sentence above and save the lemmatized strings into the \n",
        "#tmp variable\n",
        "\n",
        "tmp = [t.lemma_ for t in nlp(snt)]\n",
        "print(tmp)\n",
        "# The output should be ['that', 'be', 'a', 'very', 'good', 'movie', ',', 'John']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['that', 'be', 'a', 'very', 'good', 'movie', ',', 'John']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUpxGw6vo2su",
        "colab_type": "code",
        "outputId": "f9b4193b-5861-4cb1-b90e-32e5baad69a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# : Tokenize the sentence again and save the lemmatized strings into the \n",
        "#tmp variable, but skip stopwords and punctuation plus lowercase the lemmas, \n",
        "#print the tmp variable\n",
        "tmp = [t.lemma_.lower() for t in nlp(snt) if not t.is_stop and not t.is_punct]\n",
        "print(tmp)\n",
        "# The output should be ['good', 'movie', 'john']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good', 'movie', 'john']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ykAaAne8KVb",
        "colab_type": "text"
      },
      "source": [
        "# Split sentences into tokens and lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81ry5hpe8XGp",
        "colab_type": "code",
        "outputId": "890319e3-81bd-4b55-8e1a-523849291f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYR6ueKeWz6",
        "colab_type": "code",
        "outputId": "e087cecc-96b5-49e4-e679-02fed47c891c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# : Lemmatize and split each sentence in our dataset 'x', save the new\n",
        "#lemmatized strings into the tok_snts variable. Skip punctuation and stopwords plus lowercase lemmas\n",
        "tok_snts = []\n",
        "for snt in x:\n",
        "  tkns = [tkn.lemma_.lower() for tkn in nlp(snt) if not tkn.is_punct]\n",
        "  tok_snts.append(tkns)\n",
        "# Save back\n",
        "x = tok_snts\n",
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'think', 'another', 'disney', 'movie', 'may', 'good', '-pron-', 'be', 'kid', 'movie', 'watch', '-pron-', 'can', 'not', 'help', 'enjoy', '-pron-', 'age', 'love', 'movie', 'first', 'see', 'movie', '10', '8', 'year', 'later', 'still', 'love', '-pron-', 'danny', 'glover', 'superb', 'could', 'play', 'part', 'better', 'christopher', 'lloyd', 'hilarious', 'perfect', 'part', 'tony', 'danza', 'believable', 'mel', 'clark', 'can', 'not', 'help', 'enjoy', 'movie', 'give', '10/10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQc1FRv48n9J",
        "colab_type": "text"
      },
      "source": [
        "# Train word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVSxGpV4cOx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(x, size=300, window=6, min_count=4, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpMdj6WIUvS",
        "colab_type": "text"
      },
      "source": [
        "#Similarity\n",
        "\n",
        "We can now easily calculate word similarity using the `w2v.wv.most_similar()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KyOvtimcsdh",
        "colab_type": "code",
        "outputId": "333672c4-e8fc-4b2d-d0db-0c9a9ddcdc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "w2v.wv.most_similar(\"bad\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrible', 0.7666038274765015),\n",
              " ('awful', 0.760027289390564),\n",
              " ('horrible', 0.73488450050354),\n",
              " ('suck', 0.712145209312439),\n",
              " ('worst', 0.7115403413772583),\n",
              " ('lousy', 0.6652932167053223),\n",
              " ('stupid', 0.6638615131378174),\n",
              " ('crappy', 0.6453804969787598),\n",
              " ('good', 0.6416094303131104),\n",
              " ('dreadful', 0.6223640441894531)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMdm2yTIHmM",
        "colab_type": "text"
      },
      "source": [
        "#Get vectors\n",
        "\n",
        "To get a vector for a word 'w' we can use the `w2v.wv.get_vector(w)` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU_X_Wb48tGW",
        "colab_type": "code",
        "outputId": "c2c93bb9-cd7e-4e2c-ecc9-1399ad3f1f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#To check is a word in our vocab we use:\n",
        "if 'bad' in w2v.wv:\n",
        "  print(w2v.wv.get_vector(\"bad\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.9930603e-01 -7.1830708e-01  4.3662372e-01  3.7881717e-01\n",
            " -5.5523425e-01  3.8440472e-01  1.4767395e+00 -2.6020375e-01\n",
            " -6.6382289e-01  5.6127882e-01 -1.1903995e+00  3.2087913e-01\n",
            " -4.2468601e-01  4.9083146e-01 -2.1801984e-01  5.3348470e-01\n",
            " -5.3254777e-01 -3.9537117e-01 -9.2829889e-01  1.3717705e+00\n",
            " -3.5480502e-01 -1.8207449e-01 -9.9948637e-02 -7.4317539e-01\n",
            " -7.3766977e-01  1.5350636e+00  2.2298485e-01  1.5597947e-01\n",
            "  6.9809717e-01  6.5537530e-01  1.8340760e-01 -2.3912665e-01\n",
            "  6.8455374e-01  7.6289302e-01 -1.4322363e+00 -1.6938481e+00\n",
            " -3.1448212e-01 -4.2879599e-01  1.2794785e+00 -1.0271571e+00\n",
            " -8.2129651e-01  1.2379475e+00  1.5273639e+00  3.4195292e-01\n",
            " -3.8150558e-01  1.6041555e-01  9.5072651e-01  1.0753525e+00\n",
            " -3.3933330e-01  3.7714341e-01 -4.7700089e-01  1.3936623e+00\n",
            " -6.1721593e-01  5.6903267e-01 -7.2206241e-01  4.7973052e-01\n",
            " -9.2810953e-01  1.2970347e+00  1.0000376e+00 -9.4042027e-01\n",
            " -2.3078531e-01 -7.4820411e-01  9.0098399e-01  1.0505563e+00\n",
            " -7.4888968e-01  1.7500877e+00  3.0300677e-01  1.3586330e+00\n",
            "  1.1542845e+00  9.3527979e-01 -1.3949277e+00 -1.5715140e+00\n",
            " -4.9916682e-01 -4.0456510e-01 -6.3242787e-01  3.7950167e-01\n",
            "  9.2641521e-01  1.6500986e-01 -2.5672933e-01 -3.9790455e-01\n",
            " -6.0624713e-01 -1.3985912e+00 -3.6799568e-01 -1.9175550e+00\n",
            "  3.3779597e-01 -3.0489507e-01 -5.7515484e-01 -4.1503999e-01\n",
            " -1.0378215e+00  1.1978846e+00  1.7388030e+00 -1.2840733e-01\n",
            " -1.1609528e+00  5.5978364e-01  6.8774921e-01 -6.2035371e-02\n",
            " -8.8063097e-01 -5.7990444e-01 -8.1610626e-01 -1.0959941e-02\n",
            " -1.5201415e-01  1.5721896e-01  9.5916623e-01  9.0510064e-01\n",
            " -1.2157986e+00  9.0094566e-01 -3.2071596e-01 -8.6261463e-01\n",
            " -9.7673810e-01 -2.3200297e-01 -7.7227527e-01  2.7000016e-01\n",
            "  5.6318039e-01  4.0755546e-01  8.1596851e-01 -1.0691382e+00\n",
            " -9.6464658e-01 -1.4363815e-01 -1.5447076e-01 -2.0918307e-01\n",
            " -6.5925163e-01 -5.2186334e-01  1.0181072e-01  7.8138697e-01\n",
            " -5.2569860e-01  5.2191600e-02 -1.0163764e+00 -4.3477428e-01\n",
            "  2.1048242e-01  8.9315081e-01  3.2466739e-02  6.9908452e-01\n",
            " -4.6070182e-01 -5.1224679e-01  2.1880297e-01 -9.5148903e-01\n",
            " -1.1772943e+00  2.3239019e+00  5.9796739e-01  1.4843736e-02\n",
            "  5.7169485e-01  1.0636177e+00 -2.6244736e-01  4.1957244e-01\n",
            " -8.4522885e-01  8.0274242e-01 -8.3658420e-02 -5.2812189e-01\n",
            " -2.8000271e-01 -1.9463797e+00 -1.1000772e+00 -4.1886801e-01\n",
            " -3.4285206e-01  1.6619974e-01 -1.3930418e+00 -3.3755463e-01\n",
            "  1.0845984e+00  1.1653490e+00  1.7660247e-01  8.1194353e-01\n",
            "  4.5662230e-01 -8.6841851e-01 -3.3090499e-01  2.1251519e-01\n",
            " -1.2050650e+00 -1.1142293e+00  2.5446558e-01 -1.2448559e+00\n",
            " -8.5613900e-01  9.7412193e-01  1.3898738e-01  1.7916485e+00\n",
            "  2.7294576e-01  1.2169011e+00 -1.1712103e+00 -4.9434021e-02\n",
            "  7.8430337e-01  9.5360327e-01 -7.2949225e-01  7.1204597e-01\n",
            "  5.2025265e-01 -1.3441690e+00  1.0134233e-01 -6.7738158e-01\n",
            "  2.7384186e-02  6.7521542e-01  3.4556866e-01 -4.8329949e-01\n",
            " -1.0814029e+00  8.3075875e-01  2.6488561e-02  7.9967171e-01\n",
            " -6.0296152e-02 -4.3825072e-01  2.7111632e-01 -7.6478142e-01\n",
            "  5.3782970e-01 -6.9209397e-01  4.1157719e-01  2.8379822e-01\n",
            "  3.5383341e-01  4.1808686e-01  2.1754318e-01 -4.8081347e-01\n",
            "  7.7589959e-01  1.3465364e-01 -6.0924709e-01  1.8192257e-01\n",
            " -7.8697139e-01 -6.3852018e-01  2.0947029e-01  4.7111133e-01\n",
            " -2.0156282e-01  9.0235049e-01  2.9618743e-01  7.0843619e-01\n",
            " -2.7306765e-01 -4.7350401e-01  7.2184402e-01  6.3019812e-01\n",
            "  9.1182894e-01  3.9977828e-01 -8.3189622e-02 -3.7224922e-01\n",
            " -5.0887346e-02  2.6226327e-01  6.2191978e-02 -1.2312500e+00\n",
            "  9.2900217e-01 -5.9398700e-02 -1.3651395e+00 -2.5642264e-01\n",
            " -1.1848984e+00 -1.6526310e+00  4.0116191e-01  1.3387828e+00\n",
            "  1.1846838e-01 -1.6961005e+00  5.3162688e-01 -4.8490459e-01\n",
            " -2.6479286e-01  5.0322855e-01  7.4172485e-01  1.3328530e+00\n",
            "  5.4168306e-02  4.9344905e-02  7.5715226e-01  4.8645310e-02\n",
            " -7.9430950e-01 -1.7819096e-01  6.5993321e-01 -1.3051276e-01\n",
            " -3.1578973e-01  1.7042232e-01 -2.5817522e-01  1.3274508e+00\n",
            " -6.0462189e-01 -1.2236130e+00  6.3906246e-01  1.7472988e-01\n",
            "  1.2618418e-01  1.7420374e-02  1.7294537e-01 -3.0580026e-01\n",
            "  6.6489011e-01  2.5603014e-01  4.4888610e-01 -1.9773866e-01\n",
            "  8.8656849e-01 -3.9880961e-01 -2.1649064e-01  1.1098825e+00\n",
            " -1.4617340e-01  7.7048892e-01  3.8754022e-01 -1.4284629e+00\n",
            " -1.0515653e+00 -1.1634221e+00 -7.8510112e-01  2.2262242e-03\n",
            " -4.2298540e-01 -9.0668452e-01 -6.6854393e-01  1.5554779e+00\n",
            " -5.5509853e-01 -3.1529641e-01  1.9530024e-01 -2.3627482e-01\n",
            "  7.0277530e-01  4.7048753e-01  1.4088297e-01 -4.9498120e-01\n",
            "  3.9837727e-01  7.6175445e-01  7.7956311e-02  1.9482183e+00\n",
            " -1.7290625e-01 -7.2883189e-01  4.8543647e-01  1.9783267e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZsis4V88i-",
        "colab_type": "text"
      },
      "source": [
        "# Convert each sentence into the average sum of the vector representations of its tokens\n",
        "\n",
        "Save the results into a new variable x_emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTQktTzBdaYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "\n",
        "# x_emb - embedded sentences\n",
        "x_emb = np.zeros((len(x), 300))\n",
        "# Loop over sentences\n",
        "for i_snt, snt in enumerate(x):\n",
        "  cnt = 0\n",
        "  # Loop over the words of a sentence\n",
        "  for i_word, word in enumerate(snt):\n",
        "    if word in w2v.wv:\n",
        "      x_emb[i_snt] += w2v.wv.get_vector(word)\n",
        "      cnt += 1\n",
        "  if cnt > 0:\n",
        "    x_emb[i_snt] = x_emb[i_snt] / cnt\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQJQFjdkUo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get torch stuff\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sklearn.metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATUWE2b9VKd",
        "colab_type": "text"
      },
      "source": [
        "# Split the dataset into train/test/dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kpy4rEGnghM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "inds = np.random.permutation(len(x))\n",
        "inds_train = inds[0:int(0.8*len(x))]\n",
        "inds_test = inds[int(0.8*len(x)):int(0.9*len(x))]\n",
        "inds_dev = inds[int(0.9*len(x)):]\n",
        "\n",
        "# 80% of the dataset\n",
        "x_train = x_emb[inds_train]\n",
        "y_train = y[inds_train]\n",
        "x_train_w = np.array(x)[inds_train]\n",
        "x_train_o = np.array(x_original)[inds_train]\n",
        "\n",
        "# 10% of the dataset\n",
        "x_test = x_emb[inds_test]\n",
        "y_test = y[inds_test]\n",
        "\n",
        "# 10% of the dataset\n",
        "x_dev = x_emb[inds_dev]\n",
        "y_dev = y[inds_dev]\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "x_dev = torch.tensor(x_dev, dtype=torch.float32)\n",
        "y_dev = torch.tensor(y_dev.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH3ssL6_dK4",
        "colab_type": "text"
      },
      "source": [
        "#Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjDb8Wz0m2PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# : Set the device to 'cuda'\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# : Build a network with 4 layers:\n",
        "#1: 150 neurons and ReLU activation (300 inputs)\n",
        "#2: 70 neurons and ReLU activation\n",
        "#3: 25 neurons and ReLU activation\n",
        "#4: 1 neuron and sigmoid activation\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 150)\n",
        "      self.fc2 = nn.Linear(150, 70)\n",
        "      self.fc3 = nn.Linear(70, 25)\n",
        "      self.fc4 = nn.Linear(25, 1)\n",
        "      \n",
        "      self.d1 = nn.Dropout(0.5)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.d1(torch.relu(self.fc1(x)))\n",
        "      x = self.d1(torch.relu(self.fc2(x)))\n",
        "      x = self.d1(torch.relu(self.fc3(x)))\n",
        "      x = torch.sigmoid(self.fc4(x))\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMz8tBisAaGA",
        "colab_type": "code",
        "outputId": "e8f60130-36e9-4c24-edcd-297c12b7ea4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Create the network and get BCE loss\n",
        "net = Net()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# : Make a SGD optimizer with lr=0.002 and momentum=0.99\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.99)\n",
        "# : Move the net to the device\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (fc2): Linear(in_features=150, out_features=70, bias=True)\n",
              "  (fc3): Linear(in_features=70, out_features=25, bias=True)\n",
              "  (fc4): Linear(in_features=25, out_features=1, bias=True)\n",
              "  (d1): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7VRuEb9bj1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtU1lLR_cfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move data to the right device\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "x_dev = x_dev.to(device)\n",
        "y_dev = y_dev.to(device)\n",
        "\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXYUTXUoS-t",
        "colab_type": "code",
        "outputId": "1e04ab42-63c7-4535-f25f-2a3121fef3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "net.train()\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "for epoch in range(5000):  # do 10,000 epochs \n",
        "  # : zero the gradients on the optimizer\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # : Calculate the forward pass \n",
        "  outputs = net(x_train)\n",
        "  # : Calculate the loss\n",
        "  loss = criterion(outputs, y_train)\n",
        "  # : Backward pass\n",
        "  loss.backward()\n",
        "  # : Optimize/Update parameters\n",
        "  optimizer.step()\n",
        "  \n",
        "  # Track the changes - This is normally done using tensorboard or similar\n",
        "  losses.append(loss.item())\n",
        "  accs.append(sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy()))\n",
        "\n",
        "  # print statistics\n",
        "  if epoch % 500 == 0:\n",
        "      net.eval()\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_dev)\n",
        "      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "      accs_dev.append(acc_dev)\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "      net.train()\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.69569 Acc: 0.502 Acc Dev: 0.484\n",
            "Epoch:  500 Loss: 0.47002 Acc: 0.800 Acc Dev: 0.832\n",
            "Epoch: 1000 Loss: 0.39615 Acc: 0.835 Acc Dev: 0.844\n",
            "Epoch: 1500 Loss: 0.37714 Acc: 0.847 Acc Dev: 0.851\n",
            "Epoch: 2000 Loss: 0.36212 Acc: 0.855 Acc Dev: 0.853\n",
            "Epoch: 2500 Loss: 0.34938 Acc: 0.863 Acc Dev: 0.857\n",
            "Epoch: 3000 Loss: 0.34264 Acc: 0.868 Acc Dev: 0.858\n",
            "Epoch: 3500 Loss: 0.32747 Acc: 0.875 Acc Dev: 0.857\n",
            "Epoch: 4000 Loss: 0.31297 Acc: 0.879 Acc Dev: 0.859\n",
            "Epoch: 4500 Loss: 0.29849 Acc: 0.888 Acc Dev: 0.862\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhPQXS_uopWq",
        "colab_type": "code",
        "outputId": "0c9845be-4e56-41b6-9578-d90b315de01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Switch to eval mode \n",
        "net.eval()\n",
        "net.to(device)\n",
        "print(\"Predicted value: \" + str(net(torch.tensor(x_emb[4], dtype=torch.float32).to(device))))\n",
        "print(\"Real value: \" + str(y[4]))\n",
        "print(\"Input sentence: \" + df['SentimentText'].iloc[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted value: tensor([0.8760], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "Real value: 1\n",
            "Input sentence: truly enjoyed film. acting terrific plot. Jeff Combs talent recognized for. part flick would change ending. death creature far gruesome Sci Fi Channel.<br /><br />There interesting religious messages film. Jeff Combs obviously played Messiah figure creature (or shark prefer) represented anti-Chirst. particularly frightening scenes 'end world feel'. noticed third viewing classic creature feature. know many people won't get references Christianity, watch close you'll get it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJsArQIGHz3B",
        "colab_type": "text"
      },
      "source": [
        "#Go back and add dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGjzLNow-T9_",
        "colab_type": "text"
      },
      "source": [
        "#Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2vaQdnM-Wyf",
        "colab_type": "code",
        "outputId": "76e825dc-cbe7-474a-b5c9-7202773a867f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Make a small network with one layer\n",
        "device = torch.device('cuda')\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 1)\n",
        "       \n",
        "    def forward(self, x):\n",
        "      x = torch.sigmoid(self.fc1(x))\n",
        "      return x\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.99)\n",
        "for epoch in range(4000):  # do 10,000 epochs \n",
        "  # zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Forward \n",
        "  outputs = net(x_train)\n",
        "  # Calculate error\n",
        "  loss = criterion(outputs, y_train)\n",
        "  # Backward\n",
        "  loss.backward()\n",
        "  # Optimize/Update parameters\n",
        "  optimizer.step()\n",
        "  \n",
        "  # Track the changes - This is normally done using tensorboard or similar\n",
        "  losses.append(loss.item())\n",
        "\n",
        "  # print statistics\n",
        "  if epoch % 500 == 0:\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_dev)\n",
        "      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.70144 Acc: 0.409 Acc Dev: 0.414\n",
            "Epoch:  500 Loss: 0.45068 Acc: 0.807 Acc Dev: 0.816\n",
            "Epoch: 1000 Loss: 0.41426 Acc: 0.825 Acc Dev: 0.834\n",
            "Epoch: 1500 Loss: 0.39950 Acc: 0.831 Acc Dev: 0.835\n",
            "Epoch: 2000 Loss: 0.39118 Acc: 0.836 Acc Dev: 0.836\n",
            "Epoch: 2500 Loss: 0.38579 Acc: 0.839 Acc Dev: 0.836\n",
            "Epoch: 3000 Loss: 0.38200 Acc: 0.840 Acc Dev: 0.838\n",
            "Epoch: 3500 Loss: 0.37918 Acc: 0.841 Acc Dev: 0.840\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVx2mD7b_YZq",
        "colab_type": "code",
        "outputId": "eaf5b899-ea2a-419e-b5fe-4536bf96d80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Print the prediction of the first 30 sentences\n",
        "for i in range(30):\n",
        "  out = net(x_train[i])\n",
        "  print(i, out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([0.2452], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "1 tensor([0.2832], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "2 tensor([0.1570], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "3 tensor([0.0777], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "4 tensor([0.0066], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "5 tensor([0.1787], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "6 tensor([0.7117], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "7 tensor([0.0027], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "8 tensor([0.0440], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "9 tensor([0.0037], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "10 tensor([0.6430], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "11 tensor([0.2967], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "12 tensor([0.9493], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "13 tensor([0.8238], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "14 tensor([0.0053], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "15 tensor([0.2821], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "16 tensor([0.0018], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "17 tensor([0.1051], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "18 tensor([0.9171], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "19 tensor([0.0940], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "20 tensor([0.2955], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "21 tensor([0.2442], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "22 tensor([0.9874], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "23 tensor([0.3668], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "24 tensor([0.2381], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "25 tensor([0.9407], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "26 tensor([0.9850], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "27 tensor([0.0168], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "28 tensor([0.9675], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "29 tensor([0.9526], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCVkmeCE_iKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the 'i_snt' to high prob\n",
        "i_snt = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeAWnVvA-r4h",
        "colab_type": "code",
        "outputId": "2cd4109b-1495-40a3-9526-14f667b0f8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "prod = []\n",
        "# Take the weights of our layer\n",
        "we = net.fc1.weight.detach().cpu().numpy()[0]\n",
        "# For each word in the dataset check how much did it contribute to the \n",
        "#final classification\n",
        "for ind, word in enumerate(x_train_w[i_snt]):\n",
        "  if word in w2v:\n",
        "    prod.append(np.dot(we, w2v.wv.get_vector(word)))\n",
        "  else:\n",
        "    prod.append(0)\n",
        "    \n",
        "# Sort the contributions\n",
        "srt = np.argsort(prod)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mewfla3Q_Gfi",
        "colab_type": "code",
        "outputId": "a4eea916-cee0-4962-b306-e342bda4c925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "print(x_train_o[i_snt].replace(\"<br /><br />\", \"\\n\"))\n",
        "print()\n",
        "# print the top 10 words that contributed the most\n",
        "for i in range(1, 30):\n",
        "  print(x_train_w[i_snt][srt[-i]], prod[srt[-i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1954 Marlon Brando hot actor performances Streetcar Named Desire Waterfront. Frank Sinatra yet re-invent silver screen. Sinatra's portrayal erstwhile Nathan Detroit, helped re-establish Sinatra fans.\n",
            "It great screen version great play choices leads support players terrific. Imagine movie Brando sings? one singing role portrayed Sky Masterson. addition female leads, Jean Simmons Vivian Blaine(replaying stage role Nathan's long suffering girlfriend Adelade), put superlative efforts. Special mention goes great Stubby Kaye(as Nicely Nicely), due respect Eric Clapton, one's version Rockin' Boat even comes close Stubby's. Sheldon Leonard, would go fame TV producer shows Danny Thomas Show Dick Van Dyke Show \"Harry Horse\" wonders, B.S.Pulley excellent harsh mannered rough talking \"Big Julie\", even Regis Toomey offers excellence \"Brother Arvide\".\n",
            "It one fun musicals see, good comedy, get Sinatra Brando. Soooooo \"Luck Lady Tonight\" brother...\"it's dice\"\n",
            "\n",
            "great 36.186264\n",
            "great 36.186264\n",
            "great 36.186264\n",
            "excellent 34.763885\n",
            "fan 25.432453\n",
            "fun 22.782785\n",
            "musical 18.208477\n",
            "support 16.187204\n",
            "good 15.732631\n",
            "terrific 15.426791\n",
            "frank 13.855747\n",
            "comedy 13.403849\n",
            "see 13.290284\n",
            "portrayal 11.860675\n",
            "sing 11.164317\n",
            "performance 10.937294\n",
            "sky 10.837616\n",
            "sinatra 10.384501\n",
            "sinatra 10.384501\n",
            "sinatra 10.384501\n",
            "sinatra 10.384501\n",
            "yet 9.391551\n",
            "show 9.386928\n",
            "show 9.386928\n",
            "show 9.386928\n",
            "nicely 8.019801\n",
            "nicely 8.019801\n",
            "close 7.6193643\n",
            "danny 7.615398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE-ACAsa9Bkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review1 = \"That was not a good movie and my friend was there\"\n",
        "review2 = \"That was a good movie and my friend was not there\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoAXnTuhAenx",
        "colab_type": "code",
        "outputId": "4f2999fd-f183-4401-dc61-d64f9a2bf55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(review1)\n",
        "# Tokenize the review using spacy - same as we have done above\n",
        "tkns = [tkn.lemma_.lower() for tkn in nlp(review1) if not tkn.is_punct]\n",
        "# Convert the tokens to vectors and save into an array\n",
        "vecs = [w2v.wv.get_vector(t) for t in tkns]\n",
        "emb = torch.tensor(np.average(vecs, axis=0)).to(device)\n",
        "net(emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That was not a good movie and my friend was there\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8837], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5kDLBH9sGf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fycnlnhK7ReI",
        "colab_type": "code",
        "outputId": "0a348753-e7ab-44e4-9859-383201515381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(review2)\n",
        "# Tokenize the review using spacy - same as we have done above\n",
        "tkns = [tkn.lemma_.lower() for tkn in nlp(review2) if not tkn.is_punct]\n",
        "# Convert the tokens to vectors and save into an array\n",
        "vecs = [w2v.wv.get_vector(t) for t in tkns]\n",
        "emb = torch.tensor(np.average(vecs, axis=0)).to(device)\n",
        "net(emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That was a good movie and my friend was not there\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8837], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gYgqG2h7hPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: print 3 examples where the model made a mistake\n",
        "#x_train_o - contains the original text\n",
        "#x_train - contains the embeding for each review in x_train_o\n",
        "#y_train - contains the ground truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0TOd4iIsuIR",
        "colab_type": "code",
        "outputId": "3aaa6079-fd9e-4691-f555-8c5a00b2594c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "for i in range(len(x_train_o)):\n",
        "  text = x_train_o[i]\n",
        "  emb = x_train[i]\n",
        "  ground_truth = y_train[i]\n",
        "  \n",
        "  prediction = net(emb)\n",
        "  prediction = prediction.detach().cpu().numpy()[0]\n",
        "  if prediction < 0.01 and ground_truth == 1: # This is done to remove the extreme examples from the training or testing dataset\n",
        "    print(text)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actually liked movie very, much. it`s plot, acting, jokes, no. liked it, it`s one worse movies ever created. It`s lame, bad, becomes terribly funny. jokes actually cool, rest makes pray unemployment scriptwriter. \"Men white\" dumb stupid, two things. Turn TV roll floor laughing (beer helps lot:). chose second option.\n",
            "\n",
            "low rating? really don't get it... bad acting? bad dialogue? Well, cares things cheesy low-budget horror movies? Seriously, acting dialogue isn't important movies. People hate movies bad acting bad dialogue shouldn't allowed rate cheesy low-budget movies. movies shouldn't taken seriously. Period.<br /><br />Anyway, time talk movie, right? Well, loved it! bought expected gorefest, it's gorefest gore pretty bad (most time it's animal guts placed body actors that's lame), didn't really care movie hilarious! characters hilarious, acting hilarious (bad acting GOOD thing cheesy low-budget horror movies), dialogue hilarious (bad dialogue GOOD thing cheesy low-budget horror movies), zombie rapist huge dick hilarious, flying demon baby hilarious could go on, don't want say much... mention there's scene girl masturbates sex doll like it's alive lol! Oh zombie rapist falls love sex doll lol!<br /><br />Best lines movie:<br /><br />Detective Manners: *sniffs coke* Detective Sloane: *beep* doing, Manners? hell snort? hell that? Detective Manners: It's nothing man, it's... Ehh... Cold medicine...<br /><br />Detective Manners: *injects heroin arm* Detective Sloane: *beep* doing, Manners? *beep* insane? Detective Manners: It's cold medicine.<br /><br />Detective Manners: *repeatedly kicks random guy face* Detective Sloane: hell's going on, Manners? doing? Detective Manners: maniac rambling demons started smashing head rock! started smashing head rock! think he's PCP something!<br /><br />LOL!\n",
            "\n",
            "creators film made attempt introducing reality plot, would one waste time, money, creative effort. Fortunately, throwing pretense reality winds, created comedic marvel. could pass film alien pilot spends entire film acting like Jack Nicholson, complete Lakers T-shirt. dismiss film trash.\n",
            "\n",
            "felt necessary respond comments posted front page film's page slightly misinformative.<br /><br />Originally posted quotes original poster, wasn't sure proper given \"comments\" index message board (though used use 'em way back IMDb added film message boards) edit make unnecessary.<br /><br />Well, first may aware this, Gene Kelly first became famous playing \"Pal Joey\" Broadway original production. Vincente Minnelli decided make Gershwin \"panorama\" film, wanted Kelly's character sophisticated \"goody two shoes\" roles playing films (with exception \"For Gal\"). Alan Jay Lerner instructed construct new story set Paris based story \"Pal Joey\". gave Kelly chance play famous role Broadway even though Warners outbid MGM rights \"Pal Joey.\" opinion, WB film \"Pal Joey\" wreck, though Sinatra suitable role, problems sunk film (script changes poor direction. ===================================================<br /><br />You complain Kelly's pictures well done, even citing art education prove point. miss fact Kelly's bad art clearly designed bad, necessary story/characters. pictures bad, audience knows Kelly isn't ready exhibition. Even knows it, though Milo sort sugared point almost believes her. it's important audience sitting saying \"but, he's great artist, chance!\". want audience fully aware deficiencies.<br /><br />Then complain sabotages interest show; understanding structure story. refuses doesn't want feel like gigolo, knows he's really ready exhibition. enthusiasm exhibition certainly great \"Joey's\" enthusiasm \"start nightclub\". serves function plot. Remember, it's essential \"Pal Joey\" (the play) Joey gives nightclub realizes doesn't deserve it. art show. Kelly's paintings actually good, would undermine whole point. ===================================================<br /><br />Then complain Caron Kelly \"chemistry\". guess it's eye beholder. agree, chemistry strong be, fine. Compare even worse \"forced\" romances like one Cary Grant Sophia Loren \"The Pride Passion\". ====================================================<br /><br />When say big dance finale nothing anything else film, shows haven't dug beneath surface film symbolism. Many elements dance sequence relate story characters, dance plot resolved images symbolism. It's finding love, enjoying love, losing love (he looks around love gone). movements symphony constructed part dance scene mirrors separate phase Parisian Art also separate phase relationships. didn't' see that, it's movie's fault. It's certainly \"load crap\". ==================================================\n",
            "\n",
            "amazing film. little dialogue, whole story told glances body language. involving almost voyeuristic. gripe released video Australia therefore available TV. waste.\n",
            "\n",
            "flick sterling example state erotic B-movies: bad porn movies without hardcore sex. plot one isn't bad things go; involves female lawyer trying prove lover innocent killing wife. rest movie, however, leaves something desired. Bad acting, bad direction, bad looking woman, bad sets, bad cinematography, bad sound bad sex scenes. filmmakers learn difference raunchy erotic. don't even common sense Gabriella Hall naked love scene.<br /><br /> dumb that?\n",
            "\n",
            "simply funniest movie I've seen long time. bad acting, bad script, bad scenery, bad costumes, bad camera work bad special effects stupid find reeling laughter.<br /><br />So it's gonna win Oscar you've got beer friends round can't go wrong.\n",
            "\n",
            "Stupid, Stupid, Stupid. think Angelina Jolie probably one talented actress' today, movie like isn't worth time. deserves better, everyone else movie. Talent wasted. Sorry, don't feel like writing review this.<br /><br />I give stars *****.\n",
            "\n",
            "movie everything makes bad movie worth watching - sloppy editing, little continuity, insane dialog, bad (you might even say non-existent) acting, pointless story lines, shots go FAR long...and it's perfect MST3K-style riffing, mention \"Corpse Eaters Drinking Game\": Scribble forms...take shot - Sign name...take shot - Catch bad Foley edit...take many, many shots.<br /><br />The reason didn't rate higher 8 there's enough gratuitous nudity despite insane badness, it's hour long - hell, movie like least 20-30 minutes longer!\n",
            "\n",
            "definitive movie version Hamlet. Branagh cuts nothing, wasted moments.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKHaQHYmYitZ",
        "colab_type": "text"
      },
      "source": [
        "# Let's switch to batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xQdx8LmXcxH",
        "colab_type": "code",
        "outputId": "4e45f8cb-2d5c-4b3a-91cf-73ff008cc754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This will batch the dataset through the model. Stochastic gradient decent benifits for batch processing\n",
        "\n",
        "device = torch.device('cuda')\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 1)\n",
        "       \n",
        "    def forward(self, x):\n",
        "      x = torch.sigmoid(self.fc1(x))\n",
        "      return x\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.99)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 1000\n",
        "num_batches = int(np.ceil(len(x_train)/batch_size)) #TODO: Calculate the number of batches based on x_train size and batch_size\n",
        "for epoch in range(4000):  # do 10,000 epochs \n",
        "  for i in range(num_batches): #loop over all the batches\n",
        "    start = i*batch_size\n",
        "    end = (i+1) * batch_size\n",
        "    x_train_batch =  x_train[start:end] #? Get batch_size examples from x_train\n",
        "    y_train_batch = y_train[start:end] #? Get batch_size examples from y_train\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward \n",
        "    outputs = net(x_train_batch)\n",
        "    # Calculate error\n",
        "    loss = criterion(outputs, y_train_batch)\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    # Optimize/Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track the changes - This is normally done using tensorboard or similar\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # print statistics\n",
        "    if epoch % 500 == 0:\n",
        "        outputs = net(x_train)\n",
        "        acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "\n",
        "        outputs_dev = net(x_dev)\n",
        "        acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "\n",
        "        print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.70349 Acc: 0.495 Acc Dev: 0.513\n",
            "Epoch:    0 Loss: 0.70379 Acc: 0.495 Acc Dev: 0.513\n",
            "Epoch:    0 Loss: 0.70582 Acc: 0.495 Acc Dev: 0.513\n",
            "Epoch:    0 Loss: 0.70130 Acc: 0.495 Acc Dev: 0.513\n",
            "Epoch:    0 Loss: 0.70239 Acc: 0.495 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.70465 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.70213 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.69870 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.69715 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.69801 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.70235 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.70094 Acc: 0.494 Acc Dev: 0.512\n",
            "Epoch:    0 Loss: 0.69891 Acc: 0.494 Acc Dev: 0.511\n",
            "Epoch:    0 Loss: 0.69810 Acc: 0.494 Acc Dev: 0.510\n",
            "Epoch:    0 Loss: 0.69266 Acc: 0.494 Acc Dev: 0.511\n",
            "Epoch:    0 Loss: 0.70213 Acc: 0.494 Acc Dev: 0.511\n",
            "Epoch:    0 Loss: 0.69584 Acc: 0.495 Acc Dev: 0.510\n",
            "Epoch:    0 Loss: 0.70137 Acc: 0.495 Acc Dev: 0.510\n",
            "Epoch:    0 Loss: 0.69775 Acc: 0.495 Acc Dev: 0.510\n",
            "Epoch:    0 Loss: 0.69838 Acc: 0.495 Acc Dev: 0.510\n",
            "Epoch:  500 Loss: 0.39167 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.38917 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.35469 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.33729 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.40747 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.38193 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.37308 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.34393 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.34470 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.36824 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.35387 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.36874 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.37943 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.35274 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.34958 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.36681 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.37450 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.38947 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.33922 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch:  500 Loss: 0.36875 Acc: 0.843 Acc Dev: 0.844\n",
            "Epoch: 1000 Loss: 0.38874 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.38718 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.35062 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.33285 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.40273 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.37596 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.36976 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.33976 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.33883 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.36388 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.34904 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.36430 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.37429 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.34791 Acc: 0.846 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.34385 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.36350 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.36795 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.38620 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.33461 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1000 Loss: 0.36376 Acc: 0.847 Acc Dev: 0.845\n",
            "Epoch: 1500 Loss: 0.38730 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.38551 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.34937 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.33090 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.39993 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.37286 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.36768 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.33821 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.33656 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.36175 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.34704 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.36206 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.37174 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.34552 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.34074 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.36218 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.36429 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.38467 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.33273 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 1500 Loss: 0.36126 Acc: 0.847 Acc Dev: 0.847\n",
            "Epoch: 2000 Loss: 0.38622 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.38410 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.34862 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.32972 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.39786 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.37079 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.36600 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.33721 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.33533 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.36037 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.34588 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.36046 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.36999 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.34388 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.33849 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.36144 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.36179 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.38367 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.33158 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2000 Loss: 0.35964 Acc: 0.848 Acc Dev: 0.849\n",
            "Epoch: 2500 Loss: 0.38526 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.38291 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.34798 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.32891 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.39626 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.36927 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.36459 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.33643 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.33455 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.35936 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.34511 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.35921 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.36864 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.34264 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.33670 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.36094 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.35996 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.38290 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.33073 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 2500 Loss: 0.35846 Acc: 0.849 Acc Dev: 0.848\n",
            "Epoch: 3000 Loss: 0.38437 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.38190 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.34739 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.32829 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.39498 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.36808 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.36339 Acc: 0.850 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.33579 Acc: 0.850 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.33401 Acc: 0.850 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.35855 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.34456 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.35817 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.36754 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.34165 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.33522 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.36057 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.35856 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.38226 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.33004 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3000 Loss: 0.35754 Acc: 0.849 Acc Dev: 0.850\n",
            "Epoch: 3500 Loss: 0.38354 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.38102 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.34684 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.32780 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.39394 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.36711 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.36235 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.33523 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.33361 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.35788 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.34414 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.35730 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.36662 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.34083 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.33394 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.36028 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.35745 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.38171 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.32944 Acc: 0.850 Acc Dev: 0.849\n",
            "Epoch: 3500 Loss: 0.35679 Acc: 0.850 Acc Dev: 0.849\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}